--- python/src/weight_patterns.py	2026-01-23 05:45:21.261688739 +0000
+++ temp_tools/weight_patterns.py	2026-01-23 05:45:21.253688720 +0000
@@ -4,7 +4,28 @@
     'model.embed_tokens.weight',
     'embed_tokens.weight',
     'embeddings.weight',
-    'transformer.wte.weight'
+    'transformer.wte.weight',
+    'model.decoder.embed_tokens.weight',
+    'decoder.embed_tokens.weight'
+]
+
+MOONSHINE_GLOBAL_WEIGHTS = [
+    ('model.encoder.conv1.weight', 'encoder_conv1_weight.weights'),
+    ('model.encoder.conv2.weight', 'encoder_conv2_weight.weights'),
+    ('model.encoder.conv2.bias', 'encoder_conv2_bias.bias'),
+    ('model.encoder.conv3.weight', 'encoder_conv3_weight.weights'),
+    ('model.encoder.conv3.bias', 'encoder_conv3_bias.bias'),
+    ('model.encoder.groupnorm.weight', 'encoder_norm_weight.weights'),
+    ('model.encoder.groupnorm.bias', 'encoder_norm_bias.bias'),
+    ('model.encoder.layer_norm.weight', 'encoder_layer_norm_weight.weights'),
+    ('model.encoder.layer_norm.bias', 'encoder_layer_norm_bias.bias'),
+    ('model.decoder.norm.weight', 'output_norm.weights'),
+    ('model.decoder.norm.bias', 'output_norm.bias'),
+    ('decoder.norm.weight', 'output_norm.weights'),
+    ('proj_out.weight', 'output_weight.weights'),
+    ('model.proj_out.weight', 'output_weight.weights'),
+    ('model.decoder.embed_tokens.weight', 'token_embeddings.weights'),
+    ('model.decoder.embed_positions.weight', 'decoder_position_embeddings.weights'),
 ]
 
 OUTPUT_NAMES = [
@@ -21,7 +42,8 @@
     'transformer.ln_f.weight',
     'model.embedding_norm.weight',
     'model.language_model.embedding_norm.weight',
-    'model.text_model.norm.weight'
+    'model.text_model.norm.weight',
+    'decoder.norm.weight'
 ]
 
 LAYER_PREFIXES = [
@@ -92,10 +114,10 @@
         (['input_layernorm.weight', 'ln_1.weight', 'operator_norm.weight'], precision, f'layer_{i}_input_norm.weights', False),
         (['self_attn.q_norm.weight', 'self_attn.q_layernorm.weight'], precision, f'layer_{i}_attn_q_norm.weights', False),
         (['self_attn.k_norm.weight', 'self_attn.k_layernorm.weight'], precision, f'layer_{i}_attn_k_norm.weights', False),
-        (['mlp.gate_proj.weight', 'mlp.c_fc.weight', 'feed_forward.w1.weight'], precision, f'layer_{i}_ffn_gate.weights', False),
-        (['mlp.up_proj.weight', 'feed_forward.w3.weight'], precision, f'layer_{i}_ffn_up.weights', False),
-        (['mlp.down_proj.weight', 'mlp.c_proj.weight', 'feed_forward.w2.weight'], precision, f'layer_{i}_ffn_down.weights', False),
-        (['post_attention_layernorm.weight', 'ln_2.weight', 'ffn_norm.weight'], precision, f'layer_{i}_post_attn_norm.weights', False),
+        (['mlp.gate_proj.weight', 'mlp.c_fc.weight', 'feed_forward.w1.weight', 'ff.ff_proj.weight'], precision, f'layer_{i}_ffn_gate.weights', False),
+        (['mlp.up_proj.weight', 'feed_forward.w3.weight', 'ff.ff_noact.weight'], precision, f'layer_{i}_ffn_up.weights', False),
+        (['mlp.down_proj.weight', 'mlp.c_proj.weight', 'feed_forward.w2.weight', 'ff.ff_out.weight'], precision, f'layer_{i}_ffn_down.weights', False),
+        (['post_attention_layernorm.weight', 'ln_2.weight', 'ffn_norm.weight', 'norm2.weight'], precision, f'layer_{i}_post_attn_norm.weights', False),
         (['pre_feedforward_layernorm.weight'], precision, f'layer_{i}_pre_ffn_norm.weights', False),
         (['post_feedforward_layernorm.weight'], precision, f'layer_{i}_post_ffn_norm.weights', False),
         (['conv.in_proj.weight'], precision, f'layer_{i}_conv_in_proj.weights', False),
@@ -103,12 +125,12 @@
         (['conv.conv.weight'], precision, f'layer_{i}_conv_depthwise.weights', False),
         (['attn.Wqkv.bias'], precision, f'layer_{i}_attn_{{channel}}.bias', False),
         (['attn.Wqkv.weight'], precision, f'layer_{i}_attn_{{channel}}.weights', False),
-        (['attn.out_proj.bias'], precision, f'layer_{i}_attn_output.bias', False),
-        (['attn.out_proj.weight'], precision, f'layer_{i}_attn_output.weights', False),
-        (['mlp.fc1.bias'], precision, f'layer_{i}_mlp_fc1.bias', False),
-        (['mlp.fc1.weight'], precision, f'layer_{i}_mlp_fc1.weights', False),
-        (['mlp.fc2.bias'], precision, f'layer_{i}_mlp_fc2.bias', False),
-        (['mlp.fc2.weight'], precision, f'layer_{i}_mlp_fc2.weights', False),
+        (['attn.out_proj.bias', 'attention.to_out.bias'], precision, f'layer_{i}_attn_output.bias', False),
+        (['attn.out_proj.weight', 'attention.to_out.weight'], precision, f'layer_{i}_attn_output.weights', False),
+        (['mlp.fc1.bias', 'ff.ff.0.bias'], precision, f'layer_{i}_mlp_fc1.bias', False),
+        (['mlp.fc1.weight', 'ff.ff.0.weight'], precision, f'layer_{i}_mlp_fc1.weights', False),
+        (['mlp.fc2.bias', 'ff.ff.2.bias'], precision, f'layer_{i}_mlp_fc2.bias', False),
+        (['mlp.fc2.weight', 'ff.ff.2.weight'], precision, f'layer_{i}_mlp_fc2.weights', False),
         (['norm1.bias'], precision, f'layer_{i}_norm1.bias', False),
         (['norm1.weight'], precision, f'layer_{i}_norm1.weights', False),
         (['norm2.bias'], precision, f'layer_{i}_norm2.bias', False),
@@ -117,21 +139,21 @@
         (['mlp.experts.mlp.w1'], precision, f'layer_{i}_mlp_expert_{{channel}}.mlp1.weights', False),
         (['mlp.experts.mlp.w2'], precision, f'layer_{i}_mlp_expert_{{channel}}.mlp2.weights', True),
         (['mlp.router.layer.weight'], precision, f'layer_{i}_mlp_router.layer.weights', False),
-        (['encoder_attn.q_proj.weight'], precision, f'layer_{i}_encoder_attn_q.weights', False),
-        (['encoder_attn.k_proj.weight'], precision, f'layer_{i}_encoder_attn_k.weights', False),
-        (['encoder_attn.v_proj.weight'], precision, f'layer_{i}_encoder_attn_v.weights', False),
-        (['encoder_attn.out_proj.weight'], precision, f'layer_{i}_encoder_attn_output.weights', False),
+        (['encoder_attn.q_proj.weight', 'attention.to_q.weight'], precision, f'layer_{i}_encoder_attn_q.weights', False),
+        (['encoder_attn.k_proj.weight', 'attention.to_k.weight'], precision, f'layer_{i}_encoder_attn_k.weights', False),
+        (['encoder_attn.v_proj.weight', 'attention.to_v.weight'], precision, f'layer_{i}_encoder_attn_v.weights', False),
+        (['encoder_attn.out_proj.weight', 'encoder_attn.o_proj.weight'], precision, f'layer_{i}_encoder_attn_output.weights', False),
         (['encoder_attn.q_proj.bias'], precision, f'layer_{i}_encoder_attn_q.bias', False),
         (['encoder_attn.v_proj.bias'], precision, f'layer_{i}_encoder_attn_v.bias', False),
-        (['encoder_attn.out_proj.bias'], precision, f'layer_{i}_encoder_attn_output.bias', False),
+        (['encoder_attn.out_proj.bias', 'encoder_attn.o_proj.bias'], precision, f'layer_{i}_encoder_attn_output.bias', False),
         (['encoder_attn_layer_norm.weight'], precision, f'layer_{i}_encoder_attn_norm.weights', False),
         (['encoder_attn_layer_norm.bias'], precision, f'layer_{i}_encoder_attn_norm.bias', False),
         (['fc1.weight'], precision, f'layer_{i}_mlp_fc1.weights', False),
         (['fc1.bias'], precision, f'layer_{i}_mlp_fc1.bias', False),
         (['fc2.weight'], precision, f'layer_{i}_mlp_fc2.weights', False),
         (['fc2.bias'], precision, f'layer_{i}_mlp_fc2.bias', False),
-        (['final_layer_norm.weight'], precision, f'layer_{i}_final_norm.weights', False),
-        (['final_layer_norm.bias'], precision, f'layer_{i}_final_norm.bias', False),
+        (['final_layer_norm.weight', 'final_layernorm.weight'], precision, f'layer_{i}_final_norm.weights', False),
+        (['final_layer_norm.bias', 'final_layernorm.bias'], precision, f'layer_{i}_final_norm.bias', False),
         
         # Whisper-only: separate self_attn_* outputs (non-Whisper uses attn_* above)
         (['self_attn.q_proj.weight'], precision, f'layer_{i}_self_attn_q.weights', False) if is_whisper else None,
