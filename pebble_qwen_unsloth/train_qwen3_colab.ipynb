{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# ğŸŒµ Qwen3 Tool Calling Training with Unsloth\n",
        "\n",
        "This notebook trains Qwen 3 models for tool calling using Unsloth, a fast and memory-efficient LoRA fine-tuning library.\n",
        "\n",
        "**Features:**\n",
        "- 2x faster training than standard HuggingFace Trainer\n",
        "- 80% less memory with 4-bit quantization\n",
        "- Sparse checkout: downloads only needed folders (~10MB)\n",
        "- Uses Unsloth default hyperparameters\n",
        "\n",
        "**Requirements:**\n",
        "- GPU runtime (T4 or better recommended)\n",
        "- ~6GB VRAM for Qwen3-0.6B\n",
        "- ~15-30 minutes training time\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_header"
      },
      "source": [
        "## ğŸ“‹ Step 1: Check GPU & Install Unsloth\n",
        "\n",
        "First verify we have a GPU, then install Unsloth using the official installation method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_unsloth"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Check GPU\n",
        "!nvidia-smi\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Installing Unsloth (this may take 2-3 minutes)...\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2\n",
        "\n",
        "print(\"\\nâœ… Unsloth installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clone_header"
      },
      "source": [
        "## ğŸ“¥ Step 2: Clone Repository (Sparse Checkout)\n",
        "\n",
        "Use sparse checkout to download only the folders we need:\n",
        "- `pebble_qwen_unsloth/` - Training code\n",
        "- `pebble_qwen/data/` - Dataset and tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Clean up any existing clone\n",
        "if os.path.exists('cactus'):\n",
        "    !rm -rf cactus\n",
        "\n",
        "print(\"Cloning repository with sparse checkout...\\n\")\n",
        "\n",
        "# Clone with sparse checkout\n",
        "!git clone \\\n",
        "    --depth 1 \\\n",
        "    --filter=blob:none \\\n",
        "    --sparse \\\n",
        "    --branch noah/pebble-qwen-unsloth \\\n",
        "    https://github.com/BruinAI/cactus.git\n",
        "\n",
        "# Change to repo directory\n",
        "%cd cactus\n",
        "\n",
        "# Set sparse checkout to only include needed folders\n",
        "!git sparse-checkout set pebble_qwen_unsloth pebble_qwen/data\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Repository Structure:\")\n",
        "print(\"=\"*60)\n",
        "!ls -R pebble_qwen_unsloth\n",
        "!ls -R pebble_qwen/data\n",
        "\n",
        "print(\"\\nâœ… Repository cloned successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config_header"
      },
      "source": [
        "## âš™ï¸ Step 3: Configuration & Setup\n",
        "\n",
        "Set hyperparameters and configuration. **Modify these values as needed!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION - Modify these values as needed!\n",
        "# ============================================================================\n",
        "\n",
        "# Model configuration\n",
        "MODEL_ID = \"unsloth/Qwen3-0.6B\"  # Options: Qwen3-0.6B, Qwen3-1.7B, Qwen3-8B\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "LOAD_IN_4BIT = True\n",
        "\n",
        "# Training hyperparameters (Unsloth defaults)\n",
        "LEARNING_RATE = 2e-4\n",
        "NUM_EPOCHS = 1\n",
        "BATCH_SIZE = 32\n",
        "GRADIENT_ACCUMULATION_STEPS = 1  # Effective batch size: 8\n",
        "\n",
        "# LoRA hyperparameters (Unsloth defaults)\n",
        "LORA_RANK = 16\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0\n",
        "\n",
        "# Data configuration\n",
        "TRAIN_TEST_SPLIT = 0.1  # 10% validation split (set to 0 to disable)\n",
        "ENABLE_EVAL = True  # Set to False to skip evaluation and save VRAM\n",
        "\n",
        "# Output configuration\n",
        "OUTPUT_DIR = \"outputs\"\n",
        "FINAL_MODEL_DIR = \"qwen3_tool_calling_trained\"\n",
        "\n",
        "# Optional: Mount Google Drive for persistent storage\n",
        "USE_GOOGLE_DRIVE = False  # Set to True to save models to Drive\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Configuration:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model: {MODEL_ID}\")\n",
        "print(f\"Max sequence length: {MAX_SEQ_LENGTH}\")\n",
        "print(f\"Quantization: {'4-bit' if LOAD_IN_4BIT else '16-bit (full precision)'}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Batch size: {BATCH_SIZE} (effective: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS})\")\n",
        "print(f\"LoRA rank: {LORA_RANK}, alpha: {LORA_ALPHA}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [],
      "source": [
        "# Optional: Mount Google Drive\n",
        "if USE_GOOGLE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    FINAL_MODEL_DIR = f\"/content/drive/MyDrive/{FINAL_MODEL_DIR}\"\n",
        "    print(f\"âœ… Google Drive mounted! Models will be saved to: {FINAL_MODEL_DIR}\")\n",
        "else:\n",
        "    print(\"â„¹ï¸ Not using Google Drive. Models will be saved to local Colab storage.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_header"
      },
      "source": [
        "## ğŸ¤– Step 4: Load Model & Tokenizer\n",
        "\n",
        "Load the model with the configured quantization setting (see Configuration above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "print(\"Loading model and tokenizer...\\n\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_ID,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=None,  # Auto-detect\n",
        "    load_in_4bit=LOAD_IN_4BIT,  # Use configured quantization setting\n",
        ")\n",
        "\n",
        "print(f\"âœ… Model loaded: {MODEL_ID}\")\n",
        "print(f\"âœ… Quantization: {'4-bit' if LOAD_IN_4BIT else '16-bit (full precision)'}\")\n",
        "print(f\"âœ… Tokenizer loaded with vocab size: {len(tokenizer)}\")\n",
        "print(f\"âœ… Device: {next(model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lora_header"
      },
      "source": [
        "## ğŸ¯ Step 5: Apply LoRA\n",
        "\n",
        "Apply Low-Rank Adaptation for efficient fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apply_lora"
      },
      "outputs": [],
      "source": [
        "print(\"Applying LoRA...\\n\")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=LORA_RANK,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",  # 30% memory savings\n",
        "    random_state=3407,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        ")\n",
        "\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f\"âœ… LoRA applied: rank={LORA_RANK}, alpha={LORA_ALPHA}\")\n",
        "print(f\"âœ… Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_header"
      },
      "source": [
        "## ğŸ“Š Step 6: Load & Prepare Dataset\n",
        "\n",
        "Load the tool calling dataset and format it for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_data"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "# Add pebble_qwen_unsloth to path\n",
        "sys.path.insert(0, '/content/cactus')\n",
        "\n",
        "from pebble_qwen_unsloth.data.format_dataset import (\n",
        "    format_qwen3_dataset,\n",
        "    load_tools,\n",
        "    load_dataset\n",
        ")\n",
        "\n",
        "print(\"Loading dataset and tools...\\n\")\n",
        "\n",
        "# Load tools and dataset\n",
        "tools = load_tools('pebble_qwen/data/tools.json')\n",
        "dataset_samples = load_dataset('pebble_qwen/data/synthetic_finetune_dataset.json')\n",
        "\n",
        "print(f\"âœ… Loaded {len(tools)} tools\")\n",
        "print(f\"âœ… Loaded {len(dataset_samples)} dataset samples\")\n",
        "\n",
        "# Format dataset\n",
        "print(\"\\nFormatting examples for Qwen 3 tool calling...\")\n",
        "formatted_data = []\n",
        "\n",
        "for sample in dataset_samples:\n",
        "    formatted = format_qwen3_dataset(sample, tools, tokenizer)\n",
        "    if formatted:\n",
        "        # Combine all role messages into a single text string\n",
        "        full_text = \"\".join(msg['text'] for msg in formatted)\n",
        "        formatted_data.append({'text': full_text})\n",
        "\n",
        "full_dataset = Dataset.from_list(formatted_data)\n",
        "\n",
        "# Split dataset\n",
        "if ENABLE_EVAL and TRAIN_TEST_SPLIT > 0:\n",
        "    split = full_dataset.train_test_split(test_size=TRAIN_TEST_SPLIT, seed=42)\n",
        "    train_dataset = split['train']\n",
        "    eval_dataset = split['test']\n",
        "else:\n",
        "    train_dataset = full_dataset\n",
        "    eval_dataset = None\n",
        "\n",
        "print(f\"\\nâœ… Formatted {len(formatted_data)} samples\")\n",
        "print(f\"âœ… Training split: {len(train_dataset)} samples\")\n",
        "if eval_dataset:\n",
        "    print(f\"âœ… Validation split: {len(eval_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preview_header"
      },
      "source": [
        "## ğŸ‘€ Step 7: Preview Training Examples\n",
        "\n",
        "Let's look at some formatted training examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preview_data"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Sample Training Examples (2 random samples)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "indices = random.sample(range(len(train_dataset)), min(2, len(train_dataset)))\n",
        "\n",
        "for i, idx in enumerate(indices, 1):\n",
        "    example = train_dataset[idx]\n",
        "    text = example['text']\n",
        "    \n",
        "    print(f\"\\n{'â”€'*60}\")\n",
        "    print(f\"Example {i} (Index: {idx})\")\n",
        "    print(f\"{'â”€'*60}\")\n",
        "    \n",
        "    # Truncate long text for display\n",
        "    if len(text) > 800:\n",
        "        print(text[:800] + \"\\n...[truncated]\")\n",
        "    else:\n",
        "        print(text)\n",
        "\n",
        "print(f\"\\n{'='*60}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trainer_header"
      },
      "source": [
        "## ğŸ“ Step 8: Setup Trainer\n",
        "\n",
        "Configure the SFTTrainer with our hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_trainer"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "print(\"Setting up trainer...\\n\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=max(BATCH_SIZE // 4, 1),\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    warmup_steps=10,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\" if eval_dataset else \"no\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    args=training_args,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "total_steps = len(train_dataset) * NUM_EPOCHS // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)\n",
        "\n",
        "print(\"âœ… Trainer initialized\")\n",
        "print(f\"âœ… Total training steps: {total_steps}\")\n",
        "print(f\"âœ… Estimated time: ~{total_steps * 2 // 60}-{total_steps * 3 // 60} minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_header"
      },
      "source": [
        "## ğŸš€ Step 9: Train!\n",
        "\n",
        "Start the training process. This will take 15-30 minutes depending on your GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"Starting training\")\n",
        "print(\"=\"*60)\n",
        "print(\"This may take 15-30 minutes...\\n\")\n",
        "\n",
        "# Train!\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… Training complete!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_header"
      },
      "source": [
        "## ğŸ§ª Step 10: Test Trained Model\n",
        "\n",
        "Test the trained model with some examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_model"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"Testing Trained Model\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Prepare model for inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Test cases\n",
        "test_cases = [\n",
        "    {'input': 'Remember to buy groceries tomorrow',\n",
        "     'output': {'function_call': {'name': 'create_note', 'arguments': {}}}},\n",
        "    {'input': 'Set an alarm for 7:30 AM',\n",
        "     'output': {'function_call': {'name': 'set_alarm', 'arguments': {}}}},\n",
        "    {'input': 'Set a timer for 10 minutes',\n",
        "     'output': {'function_call': {'name': 'set_timer', 'arguments': {}}}}\n",
        "]\n",
        "\n",
        "for i, sample in enumerate(test_cases, 1):\n",
        "    # Format the example\n",
        "    formatted = format_qwen3_dataset(sample, tools, tokenizer)\n",
        "    if not formatted:\n",
        "        continue\n",
        "    \n",
        "    # Extract prompt (system + user)\n",
        "    prompt = \"\".join(msg['text'] for msg in formatted if msg['role'] in ['system', 'user'])\n",
        "    prompt += \"<think>\\n\\n</think>\\n\\n\"\n",
        "    \n",
        "    # Generate\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128,\n",
        "            temperature=0.0,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    generated = response[len(prompt):]\n",
        "    \n",
        "    print(f\"\\n{'â”€'*60}\")\n",
        "    print(f\"Test {i}: {sample['input']}\")\n",
        "    print(f\"{'â”€'*60}\")\n",
        "    print(\"Generated:\")\n",
        "    print(generated.strip())\n",
        "\n",
        "print(f\"\\n{'='*60}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_header"
      },
      "source": [
        "## ğŸ’¾ Step 11: Save Model\n",
        "\n",
        "Save the trained model in HuggingFace safetensors format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_model"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Saving model in HuggingFace safetensors format\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Output directory: {FINAL_MODEL_DIR}\\n\")\n",
        "\n",
        "# Save merged model (LoRA merged with base weights)\n",
        "model.save_pretrained_merged(\n",
        "    FINAL_MODEL_DIR,\n",
        "    tokenizer,\n",
        "    save_method=\"merged_16bit\"\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… Model saved to: {FINAL_MODEL_DIR}\")\n",
        "print(\"\\nSaved files:\")\n",
        "for f in os.listdir(FINAL_MODEL_DIR):\n",
        "    if os.path.isfile(os.path.join(FINAL_MODEL_DIR, f)):\n",
        "        size = os.path.getsize(os.path.join(FINAL_MODEL_DIR, f)) / (1024 * 1024)\n",
        "        print(f\"  {f:<30} {size:>10.2f} MB\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… Training and saving complete!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_header"
      },
      "source": [
        "## ğŸ“¥ Step 13 (Optional): Download Model\n",
        "\n",
        "Download the trained model to your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_model"
      },
      "outputs": [],
      "source": [
        "# Zip the model directory\n",
        "!zip -r qwen3_trained_model.zip {FINAL_MODEL_DIR}\n",
        "\n",
        "# Download\n",
        "from google.colab import files\n",
        "files.download('qwen3_trained_model.zip')\n",
        "\n",
        "print(\"âœ… Model zip file will be downloaded to your computer.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usage_header"
      },
      "source": [
        "## ğŸ“ Step 13: Usage Instructions\n",
        "\n",
        "How to load and use your trained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usage"
      },
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "ğŸ‰ Training Complete! Here's how to use your model:\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "Option 1: Load in Python\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"{FINAL_MODEL_DIR}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{FINAL_MODEL_DIR}\")\n",
        "\n",
        "Option 2: Continue training with Unsloth\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"{FINAL_MODEL_DIR}\",\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "Option 3: Push to HuggingFace Hub\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "model.push_to_hub_merged(\n",
        "    \"your-username/qwen3-tool-calling\",\n",
        "    tokenizer,\n",
        "    save_method=\"merged_16bit\",\n",
        "    token=\"hf_...\"\n",
        ")\n",
        "\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "resources"
      },
      "source": [
        "---\n",
        "\n",
        "## ğŸ”— Resources\n",
        "\n",
        "- **GitHub Repository**: [github.com/cactus-compute/cactus](https://github.com/cactus-compute/cactus)\n",
        "- **Unsloth Documentation**: [docs.unsloth.ai](https://docs.unsloth.ai/)\n",
        "- **Unsloth GitHub**: [github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)\n",
        "\n",
        "---\n",
        "\n",
        "**Created by**: Noah Cylich  \n",
        "**License**: Apache 2.0  \n",
        "**Model**: Qwen3 (Alibaba Cloud)  \n",
        "**Framework**: Unsloth + HuggingFace Transformers"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
