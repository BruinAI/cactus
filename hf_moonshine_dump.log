=== HuggingFace Moonshine Named-Module Intermediate Outputs ===
Model: UsefulSensors/moonshine-tiny
Audio: /home/karen/Documents/cactus/tests/assets/test.wav
Device: cpu
Captured records: 278
Capture inputs: False
Transcription: Hello hello hello just um quickly testing out creating a wave file through voice memos um the goal is to use this wave file to test out whisper hopefully this will transcribe properly that's all I can hope for alright here we go

============================================================

------------------------------------------------------------
Node 0 (call 1) - model.encoder.conv1 :: Conv1d :: output
  Shape: [1,288,5007]  Precision: FLOAT16
  Stats: min=-0.426514 max=0.748047 mean=0.002018 std=0.065729
  Note: stats computed on first 1000 of 1442016 values
  Preview: [0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, ...]
------------------------------------------------------------
Node 1 (call 2) - model.encoder.groupnorm :: GroupNorm :: output
  Shape: [1,288,5007]  Precision: FLOAT16
  Stats: min=-0.082642 max=0.130737 mean=0.000591 std=0.013159
  Note: stats computed on first 1000 of 1442016 values
  Preview: [0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, ...]
------------------------------------------------------------
Node 2 (call 3) - model.encoder.conv2 :: Conv1d :: output
  Shape: [1,576,1667]  Precision: FLOAT16
  Stats: min=-21.484375 max=17.468750 mean=-0.051234 std=1.891287
  Note: stats computed on first 1000 of 960192 values
  Preview: [-0.055756, -0.055756, -0.055756, -0.055756, -0.055756, -0.055756, -0.055756, -0.055756, -0.055756, -0.055847, -0.053528, -0.061096, -0.056030, -0.033142, -0.099915, -0.041046, ...]
------------------------------------------------------------
Node 3 (call 4) - model.encoder.conv3 :: Conv1d :: output
  Shape: [1,288,833]  Precision: FLOAT16
  Stats: min=-58.906250 max=7.015625 mean=1.327610 std=5.011489
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.367188, 1.367188, 1.367188, 1.367188, 1.367188, 1.367188, 1.367188, 1.368164, 1.367188, 1.367188, 1.368164, 1.368164, 1.364258, 1.394531, 1.476562, 1.365234, ...]
------------------------------------------------------------
Node 4 (call 5) - model.encoder.rotary_emb :: MoonshineRotaryEmbedding :: output[0]
  Shape: [1,833,32]  Precision: FLOAT16
  Stats: min=-1.000000 max=1.000000 mean=0.635670 std=0.615844
  Note: stats computed on first 1000 of 26656 values
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, ...]
------------------------------------------------------------
Node 5 (call 5) - model.encoder.rotary_emb :: MoonshineRotaryEmbedding :: output[1]
  Shape: [1,833,32]  Precision: FLOAT16
  Stats: min=-1.000000 max=1.000000 mean=0.165587 std=0.435017
  Note: stats computed on first 1000 of 26656 values
  Preview: [0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, ...]
------------------------------------------------------------
Node 6 (call 6) - model.encoder.layers.0.input_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-0.862793 max=5.535156 mean=-0.029225 std=0.411599
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.209961, 0.173584, 0.036835, -0.091125, -0.017914, -0.085693, -0.388184, 0.109863, -0.158691, 0.129150, -0.162720, 0.166260, 0.048035, 0.180664, -0.476318, 0.006313, ...]
------------------------------------------------------------
Node 7 (call 7) - model.encoder.layers.0.self_attn.q_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-4.410156 max=4.296875 mean=0.054759 std=1.431199
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.344727, -2.351562, -2.277344, 0.107239, -1.559570, 0.676270, -1.372070, 0.880371, 1.597656, -0.175171, 1.618164, -0.281982, 0.174194, -0.928711, 0.652832, 1.446289, ...]
------------------------------------------------------------
Node 8 (call 8) - model.encoder.layers.0.self_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-7.515625 max=9.218750 mean=0.017540 std=2.524132
  Note: stats computed on first 1000 of 239904 values
  Preview: [8.484375, 2.367188, -3.865234, -3.007812, -3.996094, 1.043945, -3.599609, 1.208984, 3.330078, -0.571289, 2.646484, -0.632812, 1.396484, -1.508789, 1.691406, 3.117188, ...]
------------------------------------------------------------
Node 9 (call 9) - model.encoder.layers.0.self_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-1.547852 max=1.720703 mean=0.013619 std=0.444901
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.223145, -0.339600, 0.089783, 0.519043, 0.094238, 0.401855, -0.260498, -0.084534, 0.056793, -0.170410, -0.503906, 0.244019, -0.164062, -0.312256, 0.337402, -0.021790, ...]
------------------------------------------------------------
Node 10 (call 10) - model.encoder.layers.0.self_attn.o_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-12.890625 max=11.164062 mean=-0.085917 std=1.158316
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.562012, -0.627930, -0.169312, 0.283447, -0.257568, 0.382812, 0.279785, -0.858398, 1.064453, -0.245972, 0.046906, -0.583008, 0.326172, -0.783691, 0.857910, -0.561523, ...]
------------------------------------------------------------
Node 11 (call 11) - model.encoder.layers.0.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-12.890625 max=11.164062 mean=-0.085917 std=1.158316
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.562012, -0.627930, -0.169312, 0.283447, -0.257568, 0.382812, 0.279785, -0.858398, 1.064453, -0.245972, 0.046906, -0.583008, 0.326172, -0.783691, 0.857910, -0.561523, ...]
------------------------------------------------------------
Node 12 (call 11) - model.encoder.layers.0.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,833,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.838379 mean=0.001930 std=0.027487
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.207886, 0.001838, 0.000144, 0.000424, 0.006985, 0.023499, 0.003481, 0.000117, 0.000023, 0.000144, 0.006039, 0.049042, 0.007927, 0.000047, 0.000001, 0.000002, ...]
------------------------------------------------------------
Node 13 (call 12) - model.encoder.layers.0.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-2.093750 max=3.406250 mean=-0.045539 std=0.491233
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.017944, -0.129272, -0.011192, 0.122070, -0.152344, 0.205200, -0.230835, -0.573242, 0.329102, 0.180908, -0.219727, -0.112183, 0.290527, -0.238892, 0.043274, -0.524414, ...]
------------------------------------------------------------
Node 14 (call 13) - model.encoder.layers.0.mlp.fc1 :: Linear :: output
  Shape: [1,833,1152]  Precision: FLOAT16
  Stats: min=-6.574219 max=1.964844 mean=-1.079310 std=1.101502
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.447998, -0.128662, -0.597656, -2.250000, -1.012695, -0.358887, -0.678223, -0.028091, -2.361328, -0.735352, -1.626953, -1.447266, -1.014648, -2.296875, -2.164062, -2.919922, ...]
------------------------------------------------------------
Node 15 (call 14) - model.encoder.layers.0.mlp.activation_fn :: GELUActivation :: output
  Shape: [1,833,1152]  Precision: FLOAT16
  Stats: min=-0.169922 max=1.916016 mean=-0.035271 std=0.206474
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.146484, -0.057739, -0.164429, -0.027512, -0.157593, -0.129150, -0.168701, -0.013733, -0.021500, -0.169922, -0.084412, -0.106995, -0.157349, -0.024841, -0.032959, -0.005112, ...]
------------------------------------------------------------
Node 16 (call 15) - model.encoder.layers.0.mlp.fc2 :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-12.664062 max=32.468750 mean=0.022141 std=2.744824
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.238281, -0.423096, -0.821289, -0.535645, -0.875488, -2.103516, -0.287598, -1.491211, 1.960938, -1.682617, -1.465820, -0.507324, -1.052734, -0.706543, 0.275146, -0.638184, ...]
------------------------------------------------------------
Node 17 (call 16) - model.encoder.layers.0.mlp :: MoonshineEncoderMLP :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-12.664062 max=32.468750 mean=0.022141 std=2.744824
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.238281, -0.423096, -0.821289, -0.535645, -0.875488, -2.103516, -0.287598, -1.491211, 1.960938, -1.682617, -1.465820, -0.507324, -1.052734, -0.706543, 0.275146, -0.638184, ...]
------------------------------------------------------------
Node 18 (call 17) - model.encoder.layers.0 :: MoonshineEncoderLayer :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-17.218750 max=44.656250 mean=0.672370 std=3.329599
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.925781, 0.047607, -0.174805, 0.297363, -0.421143, -1.183594, 0.032959, -1.406250, 3.177734, -0.785156, -1.071289, 0.006348, 0.154297, -0.375977, 0.962891, -0.447754, ...]
------------------------------------------------------------
Node 19 (call 18) - model.encoder.layers.1.input_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-3.035156 max=7.449219 mean=-0.000077 std=0.574103
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.193237, -0.124573, -0.160522, -0.078430, -0.249390, -0.361328, -0.105713, -0.515137, 0.395508, -0.296387, -0.277344, -0.134277, -0.094482, -0.185303, 0.079529, -0.329834, ...]
------------------------------------------------------------
Node 20 (call 19) - model.encoder.layers.1.self_attn.q_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-7.429688 max=4.429688 mean=-0.178056 std=1.799484
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.481445, -0.398926, 1.824219, -0.107178, -2.099609, 1.085938, -1.405273, 2.859375, -0.513672, 2.195312, -0.423828, -2.904297, 2.605469, -0.969727, 1.062500, -2.666016, ...]
------------------------------------------------------------
Node 21 (call 20) - model.encoder.layers.1.self_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-7.199219 max=6.195312 mean=-0.111198 std=1.972856
  Note: stats computed on first 1000 of 239904 values
  Preview: [2.365234, 0.575684, 0.192871, 0.438477, -1.872070, 1.035156, -1.195312, 1.029297, 0.274414, 1.941406, -1.251953, -2.550781, 1.057617, -1.407227, -0.240601, -2.185547, ...]
------------------------------------------------------------
Node 22 (call 21) - model.encoder.layers.1.self_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-3.671875 max=3.582031 mean=-0.028801 std=1.117045
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.855469, -1.341797, -1.286133, 1.372070, 1.002930, -0.939453, 0.299561, -0.442627, -0.256104, 0.325928, -0.669434, -0.921387, -1.063477, -0.591797, 0.325439, -0.236206, ...]
------------------------------------------------------------
Node 23 (call 22) - model.encoder.layers.1.self_attn.o_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-17.156250 max=31.781250 mean=-0.014957 std=2.799118
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.977051, -2.119141, -1.054688, 1.359375, 0.217041, -0.222290, -0.820312, 1.205078, -3.578125, 0.931152, 1.896484, 0.585938, 1.572266, -1.010742, -1.099609, 0.518555, ...]
------------------------------------------------------------
Node 24 (call 23) - model.encoder.layers.1.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-17.156250 max=31.781250 mean=-0.014957 std=2.799118
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.977051, -2.119141, -1.054688, 1.359375, 0.217041, -0.222290, -0.820312, 1.205078, -3.578125, 0.931152, 1.896484, 0.585938, 1.572266, -1.010742, -1.099609, 0.518555, ...]
------------------------------------------------------------
Node 25 (call 23) - model.encoder.layers.1.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,833,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.087097 mean=0.001840 std=0.006808
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.087097, 0.065247, 0.044128, 0.034241, 0.029633, 0.024948, 0.017685, 0.009903, 0.006008, 0.004162, 0.003933, 0.004059, 0.002859, 0.001321, 0.007290, 0.001943, ...]
------------------------------------------------------------
Node 26 (call 24) - model.encoder.layers.1.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-4.527344 max=4.957031 mean=-0.017678 std=0.505829
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.034180, -0.336914, -0.264648, 0.121704, -0.142456, -0.275146, -0.168091, -0.149292, -0.129639, -0.086670, 0.017380, -0.010796, 0.132568, -0.281250, -0.164429, -0.110901, ...]
------------------------------------------------------------
Node 27 (call 25) - model.encoder.layers.1.mlp.fc1 :: Linear :: output
  Shape: [1,833,1152]  Precision: FLOAT16
  Stats: min=-6.433594 max=3.027344 mean=-0.998897 std=1.122285
  Note: stats computed on first 1000 of 959616 values
  Preview: [-1.809570, 1.357422, -2.205078, -0.375244, -0.571289, -2.968750, -1.000000, -2.072266, -2.994141, -0.021561, -1.611328, -0.696777, -2.794922, -2.267578, -1.933594, -1.324219, ...]
------------------------------------------------------------
Node 28 (call 26) - model.encoder.layers.1.mlp.activation_fn :: GELUActivation :: output
  Shape: [1,833,1152]  Precision: FLOAT16
  Stats: min=-0.169922 max=3.023438 mean=-0.006167 std=0.310369
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.063660, 1.239258, -0.030258, -0.132690, -0.162231, -0.004440, -0.158691, -0.039612, -0.004120, -0.010597, -0.086304, -0.169312, -0.007256, -0.026474, -0.051392, -0.122803, ...]
------------------------------------------------------------
Node 29 (call 27) - model.encoder.layers.1.mlp.fc2 :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-18.218750 max=45.500000 mean=-0.033683 std=4.752737
  Note: stats computed on first 1000 of 239904 values
  Preview: [2.996094, 2.251953, -3.328125, 4.914062, -9.445312, -1.018555, 0.967773, -4.031250, -0.272217, 1.128906, -2.101562, 2.992188, 3.843750, -6.402344, 0.399902, -1.692383, ...]
------------------------------------------------------------
Node 30 (call 28) - model.encoder.layers.1.mlp :: MoonshineEncoderMLP :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-18.218750 max=45.500000 mean=-0.033683 std=4.752737
  Note: stats computed on first 1000 of 239904 values
  Preview: [2.996094, 2.251953, -3.328125, 4.914062, -9.445312, -1.018555, 0.967773, -4.031250, -0.272217, 1.128906, -2.101562, 2.992188, 3.843750, -6.402344, 0.399902, -1.692383, ...]
------------------------------------------------------------
Node 31 (call 29) - model.encoder.layers.1 :: MoonshineEncoderLayer :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-52.468750 max=121.625000 mean=0.623737 std=8.783505
  Note: stats computed on first 1000 of 239904 values
  Preview: [3.945312, 0.179688, -4.558594, 6.570312, -9.648438, -2.425781, 0.180664, -4.234375, -0.672852, 1.275391, -1.276367, 3.583984, 5.570312, -7.789062, 0.263184, -1.621094, ...]
------------------------------------------------------------
Node 32 (call 30) - model.encoder.layers.2.input_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-2.931641 max=6.386719 mean=-0.000571 std=0.561051
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.323730, -0.043396, -0.432861, 0.543945, -0.965820, -0.259277, -0.036743, -0.505859, -0.117554, 0.073364, -0.160034, 0.224731, 0.394775, -0.816406, -0.039673, -0.244873, ...]
------------------------------------------------------------
Node 33 (call 31) - model.encoder.layers.2.self_attn.q_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-4.007812 max=4.382812 mean=0.092743 std=1.188617
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.862793, 0.093506, 0.435059, 0.282471, -0.707520, -0.403320, -0.507812, -0.482422, 0.648926, 0.674805, 0.865234, 1.446289, 1.173828, -1.064453, 2.291016, -1.055664, ...]
------------------------------------------------------------
Node 34 (call 32) - model.encoder.layers.2.self_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-10.812500 max=7.753906 mean=0.000559 std=2.443291
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.027145, -0.170776, -1.674805, -1.056641, -0.675781, 1.028320, -0.723633, 2.191406, 2.242188, -0.684570, 0.815430, 0.671387, -0.108765, -4.281250, 3.857422, -3.468750, ...]
------------------------------------------------------------
Node 35 (call 33) - model.encoder.layers.2.self_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-5.621094 max=6.742188 mean=0.018822 std=1.672568
  Note: stats computed on first 1000 of 239904 values
  Preview: [-4.484375, 2.361328, -0.878418, -1.852539, 0.250244, 4.718750, -0.120361, 0.634766, -2.388672, 0.683594, 0.583008, 2.025391, 0.942383, 1.311523, -0.948730, -1.562500, ...]
------------------------------------------------------------
Node 36 (call 34) - model.encoder.layers.2.self_attn.o_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-18.578125 max=43.000000 mean=0.035173 std=3.506482
  Note: stats computed on first 1000 of 239904 values
  Preview: [3.587891, 0.982422, 2.544922, -0.565430, -1.100586, 1.845703, -1.165039, 3.435547, 1.039062, -1.441406, -2.994141, 0.267090, -1.409180, -1.318359, -1.505859, 1.274414, ...]
------------------------------------------------------------
Node 37 (call 35) - model.encoder.layers.2.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-18.578125 max=43.000000 mean=0.035173 std=3.506482
  Note: stats computed on first 1000 of 239904 values
  Preview: [3.587891, 0.982422, 2.544922, -0.565430, -1.100586, 1.845703, -1.165039, 3.435547, 1.039062, -1.441406, -2.994141, 0.267090, -1.409180, -1.318359, -1.505859, 1.274414, ...]
------------------------------------------------------------
Node 38 (call 35) - model.encoder.layers.2.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,833,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.040527 mean=0.001987 std=0.005576
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.017563, 0.021698, 0.026993, 0.031799, 0.036194, 0.039734, 0.040527, 0.036316, 0.028961, 0.022385, 0.019745, 0.023270, 0.027420, 0.013893, 0.022644, 0.018127, ...]
------------------------------------------------------------
Node 39 (call 36) - model.encoder.layers.2.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-4.050781 max=5.988281 mean=-0.011607 std=0.537970
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.404785, 0.031769, -0.170410, 0.330322, -0.774902, -0.080627, -0.098816, -0.124146, -0.018448, -0.054932, -0.301758, 0.216675, 0.232178, -0.604004, -0.161133, -0.077148, ...]
------------------------------------------------------------
Node 40 (call 37) - model.encoder.layers.2.mlp.fc1 :: Linear :: output
  Shape: [1,833,1152]  Precision: FLOAT16
  Stats: min=-5.523438 max=5.980469 mean=-0.884580 std=1.075169
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.317383, -1.018555, -1.303711, -1.221680, -0.092407, -1.521484, -1.013672, 0.592773, -0.634277, -3.214844, -1.196289, -1.703125, -0.571777, -0.787109, -0.527832, -0.604492, ...]
------------------------------------------------------------
Node 41 (call 38) - model.encoder.layers.2.mlp.activation_fn :: GELUActivation :: output
  Shape: [1,833,1152]  Precision: FLOAT16
  Stats: min=-0.169922 max=5.980469 mean=0.000592 std=0.359191
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.119141, -0.157104, -0.125366, -0.135498, -0.042816, -0.097473, -0.157471, 0.428711, -0.166748, -0.002098, -0.138550, -0.075378, -0.162231, -0.169678, -0.157715, -0.164917, ...]
------------------------------------------------------------
Node 42 (call 39) - model.encoder.layers.2.mlp.fc2 :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-25.031250 max=41.750000 mean=-0.026637 std=4.805869
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.673828, -0.793457, -2.486328, 7.449219, -9.750000, 2.419922, -1.047852, 1.966797, 2.835938, -0.477295, 3.195312, -3.033203, 1.197266, -7.335938, -0.715332, 3.267578, ...]
------------------------------------------------------------
Node 43 (call 40) - model.encoder.layers.2.mlp :: MoonshineEncoderMLP :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-25.031250 max=41.750000 mean=-0.026637 std=4.805869
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.673828, -0.793457, -2.486328, 7.449219, -9.750000, 2.419922, -1.047852, 1.966797, 2.835938, -0.477295, 3.195312, -3.033203, 1.197266, -7.335938, -0.715332, 3.267578, ...]
------------------------------------------------------------
Node 44 (call 41) - model.encoder.layers.2 :: MoonshineEncoderLayer :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-91.000000 max=206.250000 mean=0.632176 std=14.623592
  Note: stats computed on first 1000 of 239904 values
  Preview: [8.203125, 0.368652, -4.500000, 13.453125, -20.500000, 1.839844, -2.031250, 1.167969, 3.203125, -0.643555, -1.074219, 0.818359, 5.359375, -16.437500, -1.957031, 2.921875, ...]
------------------------------------------------------------
Node 45 (call 42) - model.encoder.layers.3.input_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-2.341797 max=4.878906 mean=0.000058 std=0.456516
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.331299, -0.009354, -0.261719, 0.659180, -1.133789, 0.063904, -0.106384, 0.032928, 0.143921, -0.064514, -0.072388, 0.013145, 0.224731, -0.824219, -0.131226, 0.152222, ...]
------------------------------------------------------------
Node 46 (call 43) - model.encoder.layers.3.self_attn.q_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-9.531250 max=4.683594 mean=0.017590 std=1.524787
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.830566, 0.765137, 0.464844, 1.240234, 1.044922, 0.991699, -0.474365, 1.178711, 1.298828, -0.457520, 0.752441, -1.575195, -1.510742, -0.500000, 1.006836, -3.035156, ...]
------------------------------------------------------------
Node 47 (call 44) - model.encoder.layers.3.self_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-10.210938 max=9.492188 mean=0.023117 std=2.305910
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.458008, 0.475586, 2.128906, -0.957520, 1.607422, -0.548340, 1.136719, 0.903809, 0.240601, -2.330078, 0.270996, -2.072266, -3.658203, 0.798828, 0.253174, -2.255859, ...]
------------------------------------------------------------
Node 48 (call 45) - model.encoder.layers.3.self_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-5.261719 max=5.125000 mean=0.025375 std=1.613185
  Note: stats computed on first 1000 of 239904 values
  Preview: [-1.778320, -0.056335, -0.314209, 0.373047, 0.532715, -0.885254, 0.080750, -0.009529, -0.222290, -1.708008, -0.993652, 2.595703, -1.694336, 0.661621, 3.185547, -0.467041, ...]
------------------------------------------------------------
Node 49 (call 46) - model.encoder.layers.3.self_attn.o_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-23.671875 max=12.265625 mean=0.007770 std=4.273449
  Note: stats computed on first 1000 of 239904 values
  Preview: [-2.076172, 3.044922, 5.898438, 10.296875, -0.817871, -1.622070, -0.767578, 1.553711, -0.721191, -2.404297, 2.193359, -1.059570, 2.037109, -2.884766, 3.955078, 2.322266, ...]
------------------------------------------------------------
Node 50 (call 47) - model.encoder.layers.3.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-23.671875 max=12.265625 mean=0.007770 std=4.273449
  Note: stats computed on first 1000 of 239904 values
  Preview: [-2.076172, 3.044922, 5.898438, 10.296875, -0.817871, -1.622070, -0.767578, 1.553711, -0.721191, -2.404297, 2.193359, -1.059570, 2.037109, -2.884766, 3.955078, 2.322266, ...]
------------------------------------------------------------
Node 51 (call 47) - model.encoder.layers.3.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,833,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.084167 mean=0.001963 std=0.007322
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.023926, 0.051056, 0.083557, 0.084167, 0.058319, 0.038239, 0.030014, 0.027435, 0.025085, 0.019226, 0.012611, 0.008369, 0.006699, 0.002874, 0.004826, 0.013214, ...]
------------------------------------------------------------
Node 52 (call 48) - model.encoder.layers.3.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-4.792969 max=6.421875 mean=-0.008571 std=0.657898
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.254639, 0.128296, 0.039642, 1.088867, -1.040039, -0.017059, -0.149902, 0.111572, 0.100464, -0.183350, 0.024261, -0.038177, 0.317383, -0.971191, 0.084106, 0.276611, ...]
------------------------------------------------------------
Node 53 (call 49) - model.encoder.layers.3.mlp.fc1 :: Linear :: output
  Shape: [1,833,1152]  Precision: FLOAT16
  Stats: min=-7.023438 max=5.593750 mean=-1.036559 std=1.265121
  Note: stats computed on first 1000 of 959616 values
  Preview: [-1.384766, -3.712891, -1.048828, 0.204346, -1.823242, -1.646484, -2.191406, -1.727539, -1.559570, 0.531738, -1.429688, -1.267578, -1.613281, -1.132812, -1.518555, -2.333984, ...]
------------------------------------------------------------
Node 54 (call 50) - model.encoder.layers.3.mlp.activation_fn :: GELUActivation :: output
  Shape: [1,833,1152]  Precision: FLOAT16
  Stats: min=-0.169922 max=5.593750 mean=0.031963 std=0.402440
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.115051, -0.000381, -0.154297, 0.118713, -0.062225, -0.082031, -0.031143, -0.072632, -0.092712, 0.373535, -0.109253, -0.129883, -0.086060, -0.145752, -0.097839, -0.022873, ...]
------------------------------------------------------------
Node 55 (call 51) - model.encoder.layers.3.mlp.fc2 :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-32.500000 max=26.875000 mean=0.011466 std=5.781262
  Note: stats computed on first 1000 of 239904 values
  Preview: [-3.410156, -3.332031, -3.000000, -0.204834, -3.724609, 2.771484, -8.234375, 1.633789, -0.666016, 1.161133, 4.792969, 5.941406, -3.332031, -2.609375, -5.332031, 6.660156, ...]
------------------------------------------------------------
Node 56 (call 52) - model.encoder.layers.3.mlp :: MoonshineEncoderMLP :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-32.500000 max=26.875000 mean=0.011466 std=5.781262
  Note: stats computed on first 1000 of 239904 values
  Preview: [-3.410156, -3.332031, -3.000000, -0.204834, -3.724609, 2.771484, -8.234375, 1.633789, -0.666016, 1.161133, 4.792969, 5.941406, -3.332031, -2.609375, -5.332031, 6.660156, ...]
------------------------------------------------------------
Node 57 (call 53) - model.encoder.layers.3 :: MoonshineEncoderLayer :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-119.812500 max=237.750000 mean=0.651533 std=19.011370
  Note: stats computed on first 1000 of 239904 values
  Preview: [2.714844, 0.082031, -1.601562, 23.546875, -25.031250, 2.988281, -11.031250, 4.355469, 1.816406, -1.885742, 5.914062, 5.699219, 4.066406, -21.937500, -3.333984, 11.906250, ...]
------------------------------------------------------------
Node 58 (call 54) - model.encoder.layers.4.input_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-3.003906 max=7.433594 mean=0.006722 std=0.682461
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.102722, -0.019287, -0.100098, 1.124023, -1.182617, 0.116089, -0.488037, 0.200684, 0.068176, -0.133423, 0.181152, 0.246094, 0.180908, -1.063477, -0.176880, 0.632812, ...]
------------------------------------------------------------
Node 59 (call 55) - model.encoder.layers.4.self_attn.q_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-5.523438 max=5.871094 mean=0.066721 std=1.569523
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.105591, 0.295410, -1.334961, 0.390381, -0.087341, 0.762695, 0.163086, 0.531250, 0.035645, -0.035553, 0.234863, 0.525391, -0.951172, 0.175903, 0.581055, -0.523926, ...]
------------------------------------------------------------
Node 60 (call 56) - model.encoder.layers.4.self_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-8.937500 max=9.531250 mean=-0.145912 std=2.650248
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.187622, 0.058746, 1.112305, -1.840820, -1.344727, -4.015625, -1.871094, -0.914551, 3.035156, 1.150391, 0.340576, -1.843750, -0.670410, -2.822266, 2.345703, 0.969727, ...]
------------------------------------------------------------
Node 61 (call 57) - model.encoder.layers.4.self_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-9.125000 max=6.949219 mean=-0.278422 std=2.751364
  Note: stats computed on first 1000 of 239904 values
  Preview: [-2.123047, 4.867188, -4.285156, -1.643555, -0.012924, 1.583008, 0.805664, -0.684570, -1.535156, -0.798340, 2.001953, 0.875977, 0.035278, 2.816406, -3.167969, 1.245117, ...]
------------------------------------------------------------
Node 62 (call 58) - model.encoder.layers.4.self_attn.o_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-45.718750 max=29.203125 mean=-0.257695 std=7.398980
  Note: stats computed on first 1000 of 239904 values
  Preview: [5.894531, -4.191406, 8.148438, -2.173828, -11.484375, 9.453125, 2.753906, 4.199219, 5.140625, 7.257812, -12.968750, -7.000000, -0.643555, 6.738281, 8.187500, -1.325195, ...]
------------------------------------------------------------
Node 63 (call 59) - model.encoder.layers.4.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-45.718750 max=29.203125 mean=-0.257695 std=7.398980
  Note: stats computed on first 1000 of 239904 values
  Preview: [5.894531, -4.191406, 8.148438, -2.173828, -11.484375, 9.453125, 2.753906, 4.199219, 5.140625, 7.257812, -12.968750, -7.000000, -0.643555, 6.738281, 8.187500, -1.325195, ...]
------------------------------------------------------------
Node 64 (call 59) - model.encoder.layers.4.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,833,833]  Precision: FLOAT16
  Stats: min=0.000004 max=0.029572 mean=0.001199 std=0.002609
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.021179, 0.016052, 0.014000, 0.013206, 0.012848, 0.013046, 0.014053, 0.014557, 0.013206, 0.009323, 0.005089, 0.002312, 0.001400, 0.002817, 0.004269, 0.001370, ...]
------------------------------------------------------------
Node 65 (call 60) - model.encoder.layers.4.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-5.253906 max=7.636719 mean=0.004927 std=0.762871
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.288330, -0.155640, 0.242432, 0.827148, -1.380859, 0.484131, -0.317627, 0.338379, 0.225464, 0.199707, -0.234497, -0.059570, 0.121521, -0.555664, 0.219849, 0.419922, ...]
------------------------------------------------------------
Node 66 (call 61) - model.encoder.layers.4.mlp.fc1 :: Linear :: output
  Shape: [1,833,1152]  Precision: FLOAT16
  Stats: min=-11.304688 max=3.333984 mean=-1.424298 std=1.708255
  Note: stats computed on first 1000 of 959616 values
  Preview: [-4.089844, 2.005859, -0.344482, 0.582520, -0.578125, -0.607422, -0.039917, -1.284180, 0.302979, -3.410156, -2.474609, -1.312500, -0.577637, -1.550781, -1.794922, -2.306641, ...]
------------------------------------------------------------
Node 67 (call 62) - model.encoder.layers.4.mlp.activation_fn :: GELUActivation :: output
  Shape: [1,833,1152]  Precision: FLOAT16
  Stats: min=-0.169922 max=3.332031 mean=0.054772 std=0.390649
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.000088, 1.960938, -0.125854, 0.419434, -0.162842, -0.165039, -0.019318, -0.127808, 0.187500, -0.001107, -0.016510, -0.124268, -0.162720, -0.093811, -0.065186, -0.024307, ...]
------------------------------------------------------------
Node 68 (call 63) - model.encoder.layers.4.mlp.fc2 :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-23.140625 max=14.914062 mean=-0.013375 std=4.648750
  Note: stats computed on first 1000 of 239904 values
  Preview: [-1.299805, 0.207031, -3.183594, -4.492188, -4.835938, 1.037109, 1.330078, -4.140625, -2.958984, 1.378906, -0.123108, 3.855469, -2.919922, 3.263672, -4.753906, -3.677734, ...]
------------------------------------------------------------
Node 69 (call 64) - model.encoder.layers.4.mlp :: MoonshineEncoderMLP :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-23.140625 max=14.914062 mean=-0.013375 std=4.648750
  Note: stats computed on first 1000 of 239904 values
  Preview: [-1.299805, 0.207031, -3.183594, -4.492188, -4.835938, 1.037109, 1.330078, -4.140625, -2.958984, 1.378906, -0.123108, 3.855469, -2.919922, 3.263672, -4.753906, -3.677734, ...]
------------------------------------------------------------
Node 70 (call 65) - model.encoder.layers.4 :: MoonshineEncoderLayer :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-177.000000 max=279.750000 mean=0.380527 std=25.223326
  Note: stats computed on first 1000 of 239904 values
  Preview: [7.308594, -3.902344, 3.363281, 16.875000, -41.343750, 13.476562, -6.953125, 4.414062, 3.998047, 6.750000, -7.179688, 2.554688, 0.501953, -11.937500, 0.097656, 6.898438, ...]
------------------------------------------------------------
Node 71 (call 66) - model.encoder.layers.5.input_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-3.763672 max=7.164062 mean=0.023178 std=0.723998
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.242432, -0.147461, 0.146851, 0.625000, -1.550781, 0.500977, -0.307617, 0.168945, 0.137329, 0.308594, -0.241821, 0.119202, 0.007683, -0.451660, -0.006866, 0.271240, ...]
------------------------------------------------------------
Node 72 (call 67) - model.encoder.layers.5.self_attn.q_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-5.062500 max=5.285156 mean=0.076667 std=1.390527
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.179932, 0.179688, -0.666016, -0.871582, -0.476074, 0.109741, 0.291748, -0.034576, -0.508789, -0.594238, 0.214478, 0.321045, 0.011452, 0.170288, -0.467529, 0.309082, ...]
------------------------------------------------------------
Node 73 (call 68) - model.encoder.layers.5.self_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-6.699219 max=8.859375 mean=0.314053 std=2.277726
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.252441, -0.073242, 0.205933, 0.454834, -0.378418, 0.178589, 0.035187, -0.182373, -0.168579, 0.344971, 0.113770, -0.315186, -0.109375, 1.304688, 1.960938, -0.076538, ...]
------------------------------------------------------------
Node 74 (call 69) - model.encoder.layers.5.self_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-15.039062 max=11.367188 mean=0.129280 std=4.041480
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.599609, -2.986328, -4.515625, -5.546875, 7.027344, -8.562500, -0.468262, -1.382812, 8.554688, -2.302734, 4.097656, -4.964844, 2.210938, 3.164062, -3.800781, -1.273438, ...]
------------------------------------------------------------
Node 75 (call 70) - model.encoder.layers.5.self_attn.o_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-128.750000 max=120.187500 mean=-0.256545 std=20.299664
  Note: stats computed on first 1000 of 239904 values
  Preview: [2.636719, 1.739258, 2.208984, 5.578125, -49.406250, 9.085938, 5.746094, -4.953125, -7.214844, 14.273438, 14.039062, -6.230469, -0.470215, -4.320312, -0.165649, -1.033203, ...]
------------------------------------------------------------
Node 76 (call 71) - model.encoder.layers.5.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-128.750000 max=120.187500 mean=-0.256545 std=20.299664
  Note: stats computed on first 1000 of 239904 values
  Preview: [2.636719, 1.739258, 2.208984, 5.578125, -49.406250, 9.085938, 5.746094, -4.953125, -7.214844, 14.273438, 14.039062, -6.230469, -0.470215, -4.320312, -0.165649, -1.033203, ...]
------------------------------------------------------------
Node 77 (call 71) - model.encoder.layers.5.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,833,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.035858 mean=0.001003 std=0.004185
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.000105, 0.000106, 0.000108, 0.000107, 0.000102, 0.000093, 0.000082, 0.000077, 0.000072, 0.000069, 0.000058, 0.000046, 0.000014, 0.000001, 0.000001, 0.000031, ...]
------------------------------------------------------------
Node 78 (call 72) - model.encoder.layers.5.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-5.234375 max=4.312500 mean=0.026833 std=0.811513
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.313232, -0.066345, 0.206787, 0.752930, -2.642578, 0.783203, -0.041138, -0.020966, -0.105774, 0.764160, 0.216675, -0.126709, 0.001838, -0.563477, -0.002041, 0.204590, ...]
------------------------------------------------------------
Node 79 (call 73) - model.encoder.layers.5.mlp.fc1 :: Linear :: output
  Shape: [1,833,1152]  Precision: FLOAT16
  Stats: min=-27.218750 max=11.117188 mean=-1.910189 std=3.195758
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.979004, 1.414062, -2.841797, -0.743652, -0.270996, -2.244141, 1.489258, -5.253906, -2.203125, -1.550781, -1.707031, -1.272461, -5.859375, -0.392578, -1.685547, -0.345703, ...]
------------------------------------------------------------
Node 80 (call 74) - model.encoder.layers.5.mlp.activation_fn :: GELUActivation :: output
  Shape: [1,833,1152]  Precision: FLOAT16
  Stats: min=-0.169922 max=11.117188 mean=0.319734 std=1.133779
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.160400, 1.302734, -0.006374, -0.169922, -0.106567, -0.027847, 1.387695, -0.000000, -0.030380, -0.093811, -0.074951, -0.129272, -0.000000, -0.136353, -0.077454, -0.126099, ...]
------------------------------------------------------------
Node 81 (call 75) - model.encoder.layers.5.mlp.fc2 :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-306.750000 max=306.250000 mean=-0.105355 std=41.392708
  Note: stats computed on first 1000 of 239904 values
  Preview: [5.601562, -14.546875, 6.917969, -8.367188, -86.875000, 8.890625, -0.791504, 1.916016, -1.263672, 6.828125, -8.093750, -2.962891, 10.875000, 1.966797, 7.871094, -2.357422, ...]
------------------------------------------------------------
Node 82 (call 76) - model.encoder.layers.5.mlp :: MoonshineEncoderMLP :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-306.750000 max=306.250000 mean=-0.105355 std=41.392708
  Note: stats computed on first 1000 of 239904 values
  Preview: [5.601562, -14.546875, 6.917969, -8.367188, -86.875000, 8.890625, -0.791504, 1.916016, -1.263672, 6.828125, -8.093750, -2.962891, 10.875000, 1.966797, 7.871094, -2.357422, ...]
------------------------------------------------------------
Node 83 (call 77) - model.encoder.layers.5 :: MoonshineEncoderLayer :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-560.500000 max=477.500000 mean=0.018149 std=70.754967
  Note: stats computed on first 1000 of 239904 values
  Preview: [15.546875, -16.718750, 12.484375, 14.085938, -177.625000, 31.453125, -1.998047, 1.376953, -4.480469, 27.859375, -1.234375, -6.640625, 10.906250, -14.281250, 7.804688, 3.509766, ...]
------------------------------------------------------------
Node 84 (call 78) - model.encoder.layer_norm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-3.701172 max=4.226562 mean=0.029271 std=0.487593
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.180664, -0.195435, 0.162720, 0.162354, -1.183594, 0.366211, -0.022263, 0.021271, -0.057770, 0.387207, -0.011963, -0.082275, 0.152466, -0.176147, 0.085754, 0.049591, ...]
------------------------------------------------------------
Node 85 (call 79) - model.encoder :: MoonshineEncoder :: output.last_hidden_state
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-3.701172 max=4.226562 mean=0.029271 std=0.487593
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.180664, -0.195435, 0.162720, 0.162354, -1.183594, 0.366211, -0.022263, 0.021271, -0.057770, 0.387207, -0.011963, -0.082275, 0.152466, -0.176147, 0.085754, 0.049591, ...]
------------------------------------------------------------
Node 86 (call 79) - model.encoder :: MoonshineEncoder :: output.hidden_states[0]
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-0.169922 max=17.937500 mean=0.736143 std=1.061226
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.250000, 1.098633, 0.815918, 0.549316, 0.711914, 0.537109, 0.040771, 0.942871, 0.152344, 1.143555, 0.347900, 1.096680, 0.880859, 1.114258, -0.169922, 0.751953, ...]
------------------------------------------------------------
Node 87 (call 79) - model.encoder :: MoonshineEncoder :: output.hidden_states[1]
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-17.218750 max=44.656250 mean=0.672370 std=3.329599
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.925781, 0.047607, -0.174805, 0.297363, -0.421143, -1.183594, 0.032959, -1.406250, 3.177734, -0.785156, -1.071289, 0.006348, 0.154297, -0.375977, 0.962891, -0.447754, ...]
------------------------------------------------------------
Node 88 (call 79) - model.encoder :: MoonshineEncoder :: output.hidden_states[2]
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-52.468750 max=121.625000 mean=0.623737 std=8.783505
  Note: stats computed on first 1000 of 239904 values
  Preview: [3.945312, 0.179688, -4.558594, 6.570312, -9.648438, -2.425781, 0.180664, -4.234375, -0.672852, 1.275391, -1.276367, 3.583984, 5.570312, -7.789062, 0.263184, -1.621094, ...]
------------------------------------------------------------
Node 89 (call 79) - model.encoder :: MoonshineEncoder :: output.hidden_states[3]
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-91.000000 max=206.250000 mean=0.632176 std=14.623592
  Note: stats computed on first 1000 of 239904 values
  Preview: [8.203125, 0.368652, -4.500000, 13.453125, -20.500000, 1.839844, -2.031250, 1.167969, 3.203125, -0.643555, -1.074219, 0.818359, 5.359375, -16.437500, -1.957031, 2.921875, ...]
------------------------------------------------------------
Node 90 (call 79) - model.encoder :: MoonshineEncoder :: output.hidden_states[4]
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-119.812500 max=237.750000 mean=0.651533 std=19.011370
  Note: stats computed on first 1000 of 239904 values
  Preview: [2.714844, 0.082031, -1.601562, 23.546875, -25.031250, 2.988281, -11.031250, 4.355469, 1.816406, -1.885742, 5.914062, 5.699219, 4.066406, -21.937500, -3.333984, 11.906250, ...]
------------------------------------------------------------
Node 91 (call 79) - model.encoder :: MoonshineEncoder :: output.hidden_states[5]
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-177.000000 max=279.750000 mean=0.380527 std=25.223326
  Note: stats computed on first 1000 of 239904 values
  Preview: [7.308594, -3.902344, 3.363281, 16.875000, -41.343750, 13.476562, -6.953125, 4.414062, 3.998047, 6.750000, -7.179688, 2.554688, 0.501953, -11.937500, 0.097656, 6.898438, ...]
------------------------------------------------------------
Node 92 (call 79) - model.encoder :: MoonshineEncoder :: output.hidden_states[6]
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-3.701172 max=4.226562 mean=0.029271 std=0.487593
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.180664, -0.195435, 0.162720, 0.162354, -1.183594, 0.366211, -0.022263, 0.021271, -0.057770, 0.387207, -0.011963, -0.082275, 0.152466, -0.176147, 0.085754, 0.049591, ...]
------------------------------------------------------------
Node 93 (call 79) - model.encoder :: MoonshineEncoder :: output.attentions[0]
  Shape: [1,8,833,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.838379 mean=0.001930 std=0.027487
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.207886, 0.001838, 0.000144, 0.000424, 0.006985, 0.023499, 0.003481, 0.000117, 0.000023, 0.000144, 0.006039, 0.049042, 0.007927, 0.000047, 0.000001, 0.000002, ...]
------------------------------------------------------------
Node 94 (call 79) - model.encoder :: MoonshineEncoder :: output.attentions[1]
  Shape: [1,8,833,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.087097 mean=0.001840 std=0.006808
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.087097, 0.065247, 0.044128, 0.034241, 0.029633, 0.024948, 0.017685, 0.009903, 0.006008, 0.004162, 0.003933, 0.004059, 0.002859, 0.001321, 0.007290, 0.001943, ...]
------------------------------------------------------------
Node 95 (call 79) - model.encoder :: MoonshineEncoder :: output.attentions[2]
  Shape: [1,8,833,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.040527 mean=0.001987 std=0.005576
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.017563, 0.021698, 0.026993, 0.031799, 0.036194, 0.039734, 0.040527, 0.036316, 0.028961, 0.022385, 0.019745, 0.023270, 0.027420, 0.013893, 0.022644, 0.018127, ...]
------------------------------------------------------------
Node 96 (call 79) - model.encoder :: MoonshineEncoder :: output.attentions[3]
  Shape: [1,8,833,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.084167 mean=0.001963 std=0.007322
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.023926, 0.051056, 0.083557, 0.084167, 0.058319, 0.038239, 0.030014, 0.027435, 0.025085, 0.019226, 0.012611, 0.008369, 0.006699, 0.002874, 0.004826, 0.013214, ...]
------------------------------------------------------------
Node 97 (call 79) - model.encoder :: MoonshineEncoder :: output.attentions[4]
  Shape: [1,8,833,833]  Precision: FLOAT16
  Stats: min=0.000004 max=0.029572 mean=0.001199 std=0.002609
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.021179, 0.016052, 0.014000, 0.013206, 0.012848, 0.013046, 0.014053, 0.014557, 0.013206, 0.009323, 0.005089, 0.002312, 0.001400, 0.002817, 0.004269, 0.001370, ...]
------------------------------------------------------------
Node 98 (call 79) - model.encoder :: MoonshineEncoder :: output.attentions[5]
  Shape: [1,8,833,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.035858 mean=0.001003 std=0.004185
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.000105, 0.000106, 0.000108, 0.000107, 0.000102, 0.000093, 0.000082, 0.000077, 0.000072, 0.000069, 0.000058, 0.000046, 0.000014, 0.000001, 0.000001, 0.000031, ...]
------------------------------------------------------------
Node 99 (call 80) - model.decoder.embed_tokens :: Embedding :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-1.484375 max=0.408447 mean=0.000947 std=0.120418
  Preview: [0.004871, 0.019821, 0.064758, 0.019211, 0.024994, 0.024811, 0.000693, 0.011948, 0.028442, 0.014214, 0.005535, -0.566895, 0.000872, 0.000878, 0.007687, 0.006084, ...]
------------------------------------------------------------
Node 100 (call 81) - model.decoder.rotary_emb :: MoonshineRotaryEmbedding :: output[0]
  Shape: [1,1,32]  Precision: FLOAT16
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, ...]
------------------------------------------------------------
Node 101 (call 81) - model.decoder.rotary_emb :: MoonshineRotaryEmbedding :: output[1]
  Shape: [1,1,32]  Precision: FLOAT16
  Stats: min=0.000000 max=0.000000 mean=0.000000 std=0.000000
  Preview: [0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, ...]
------------------------------------------------------------
Node 102 (call 82) - model.decoder.layers.0.input_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-3.136719 max=1.396484 mean=0.024512 std=0.321486
  Preview: [0.019867, 0.078857, 0.228516, 0.078247, 0.091797, 0.099854, -0.001255, 0.050842, 0.109863, 0.069824, 0.024200, -1.707031, -0.000343, -0.000377, 0.034973, 0.034790, ...]
------------------------------------------------------------
Node 103 (call 83) - model.decoder.layers.0.self_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-3.070312 max=3.324219 mean=0.024186 std=1.216666
  Preview: [-0.443848, 1.873047, 1.289062, 2.082031, 0.737305, 1.024414, -0.963867, 0.127319, 0.081177, -0.211548, 1.059570, -2.347656, 1.960938, 0.454102, -1.346680, 0.164795, ...]
------------------------------------------------------------
Node 104 (call 84) - model.decoder.layers.0.self_attn.k_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-5.875000 max=6.582031 mean=-0.105741 std=1.466487
  Preview: [-0.044098, 0.039185, -0.036621, 0.041229, -0.006641, -0.011696, 0.010063, -0.049530, 0.015549, 0.019119, -0.044281, 0.012268, 0.009743, 0.111877, 0.035431, 0.193604, ...]
------------------------------------------------------------
Node 105 (call 85) - model.decoder.layers.0.self_attn.v_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-0.075562 max=0.048615 mean=0.000327 std=0.015944
  Preview: [0.006432, -0.021454, 0.012810, -0.000482, -0.010223, -0.023422, 0.005070, 0.039307, 0.030319, 0.006889, 0.012039, -0.017227, -0.005188, -0.025116, -0.003599, -0.006905, ...]
------------------------------------------------------------
Node 106 (call 86) - model.decoder.layers.0.self_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-0.336914 max=0.117249 mean=0.000003 std=0.025438
  Preview: [0.000620, 0.016327, 0.016739, 0.004532, -0.015884, -0.011429, 0.015617, -0.010391, 0.010361, 0.011879, -0.002430, 0.017609, 0.015274, 0.016281, -0.015297, 0.002497, ...]
------------------------------------------------------------
Node 107 (call 87) - model.decoder.layers.0.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-0.336914 max=0.117249 mean=0.000003 std=0.025438
  Preview: [0.000620, 0.016327, 0.016739, 0.004532, -0.015884, -0.011429, 0.015617, -0.010391, 0.010361, 0.011879, -0.002430, 0.017609, 0.015274, 0.016281, -0.015297, 0.002497, ...]
------------------------------------------------------------
Node 108 (call 87) - model.decoder.layers.0.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,1]  Precision: FLOAT16
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 109 (call 88) - model.decoder.layers.0.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-8.210938 max=2.642578 mean=0.003068 std=0.693069
  Preview: [0.025375, 0.199463, 0.408936, 0.147217, 0.049652, 0.057892, 0.115540, 0.003149, 0.223022, 0.164673, 0.014397, -2.750000, 0.087158, 0.116333, -0.051971, 0.052155, ...]
------------------------------------------------------------
Node 110 (call 89) - model.decoder.layers.0.encoder_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-4.386719 max=5.882812 mean=0.072416 std=1.066635
  Preview: [0.067017, 0.286377, 0.117188, -1.007812, 0.082458, -0.960938, 1.292969, -0.458252, 0.229004, -0.520020, -0.348389, -0.121338, 0.317383, 0.331787, 0.336670, -0.050140, ...]
------------------------------------------------------------
Node 111 (call 90) - model.decoder.layers.0.encoder_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-3.529297 max=2.480469 mean=0.114334 std=0.822928
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.207275, 0.266113, 0.167969, 0.916016, -0.828613, 0.148682, 0.072998, -0.815430, 0.545898, 1.774414, 1.439453, 0.280029, 0.889648, 0.051392, -0.157471, 0.429443, ...]
------------------------------------------------------------
Node 112 (call 91) - model.decoder.layers.0.encoder_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-1.061523 max=0.982910 mean=-0.035126 std=0.315865
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.133911, -0.276123, -0.257080, 0.125610, -0.354248, -0.328613, -0.352051, -0.237915, 0.107483, -0.410889, -0.334473, -0.708008, 0.059326, -0.231689, 0.102905, 0.044922, ...]
------------------------------------------------------------
Node 113 (call 92) - model.decoder.layers.0.encoder_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-5.355469 max=0.558594 mean=-0.017868 std=0.359367
  Preview: [0.164795, -0.017288, 0.068970, -0.056396, -0.013489, 0.029755, -0.002481, 0.034180, -0.044525, -0.069336, -0.003174, 0.359131, 0.134033, -0.090515, 0.047791, 0.387451, ...]
------------------------------------------------------------
Node 114 (call 93) - model.decoder.layers.0.encoder_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-5.355469 max=0.558594 mean=-0.017868 std=0.359367
  Preview: [0.164795, -0.017288, 0.068970, -0.056396, -0.013489, 0.029755, -0.002481, 0.034180, -0.044525, -0.069336, -0.003174, 0.359131, 0.134033, -0.090515, 0.047791, 0.387451, ...]
------------------------------------------------------------
Node 115 (call 93) - model.decoder.layers.0.encoder_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,833]  Precision: FLOAT16
  Stats: min=0.000018 max=0.011856 mean=0.001284 std=0.001561
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.001385, 0.001405, 0.001395, 0.001336, 0.001243, 0.001089, 0.000926, 0.000795, 0.000710, 0.000628, 0.000574, 0.000698, 0.000813, 0.001390, 0.001546, 0.000906, ...]
------------------------------------------------------------
Node 116 (call 94) - model.decoder.layers.0.final_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-1.445312 max=0.919434 mean=0.041505 std=0.305933
  Preview: [0.346436, 0.065857, 0.321533, -0.032898, 0.026871, 0.122925, 0.076111, 0.113586, 0.023788, -0.054962, 0.038055, -0.363281, 0.319580, -0.119507, 0.118042, 0.734375, ...]
------------------------------------------------------------
Node 117 (call 95) - model.decoder.layers.0.mlp.fc1 :: Linear :: output
  Shape: [1,1,2304]  Precision: FLOAT16
  Stats: min=-2.898438 max=2.177734 mean=0.006075 std=0.428618
  Note: stats computed on first 1000 of 2304 values
  Preview: [-0.010262, -0.238403, 0.163574, -0.956543, -0.781250, -0.791504, 0.032837, 0.569336, 0.245117, 0.003489, -0.128052, -0.011826, 0.169922, -0.005478, 1.128906, -0.424072, ...]
------------------------------------------------------------
Node 118 (call 96) - model.decoder.layers.0.mlp.activation_fn :: SiLUActivation :: output
  Shape: [1,1,1152]  Precision: FLOAT16
  Stats: min=-0.278564 max=1.627930 mean=-0.075878 std=0.198160
  Note: stats computed on first 1000 of 1152 values
  Preview: [-0.192627, -0.244507, -0.262939, 0.983398, -0.114258, -0.060120, -0.133545, -0.251221, -0.255127, -0.169312, -0.008133, -0.060150, -0.217896, -0.053802, -0.009071, 0.005684, ...]
------------------------------------------------------------
Node 119 (call 97) - model.decoder.layers.0.mlp.fc2 :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-20.171875 max=4.710938 mean=0.007119 std=1.529810
  Preview: [-0.056458, 0.075256, 0.160889, 0.109009, -0.137939, 0.301514, -0.437256, -0.580078, -0.421143, 0.594727, 0.279053, 0.434326, -0.073792, -1.000977, 0.277100, 0.197388, ...]
------------------------------------------------------------
Node 120 (call 98) - model.decoder.layers.0.mlp :: MoonshineDecoderMLP :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-20.171875 max=4.710938 mean=0.007119 std=1.529810
  Preview: [-0.056458, 0.075256, 0.160889, 0.109009, -0.137939, 0.301514, -0.437256, -0.580078, -0.421143, 0.594727, 0.279053, 0.434326, -0.073792, -1.000977, 0.277100, 0.197388, ...]
------------------------------------------------------------
Node 121 (call 99) - model.decoder.layers.0 :: MoonshineDecoderLayer :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-25.843750 max=5.222656 mean=-0.009816 std=1.833451
  Preview: [0.113831, 0.094116, 0.311279, 0.076355, -0.142334, 0.344727, -0.423340, -0.544434, -0.426758, 0.551270, 0.279053, 0.244141, 0.076355, -1.074219, 0.317383, 0.593262, ...]
------------------------------------------------------------
Node 122 (call 100) - model.decoder.layers.1.input_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-7.035156 max=1.295898 mean=0.000226 std=0.492364
  Preview: [0.035400, 0.026718, 0.079468, 0.025650, -0.034637, 0.100647, -0.120728, -0.154175, -0.113831, 0.158447, 0.082764, 0.077332, 0.029968, -0.254150, 0.122925, 0.163940, ...]
------------------------------------------------------------
Node 123 (call 101) - model.decoder.layers.1.self_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-2.148438 max=3.416016 mean=-0.029643 std=0.807441
  Preview: [-0.904297, -0.310059, -0.758789, 0.320801, -1.273438, -0.694824, -0.451904, -1.073242, 1.337891, -1.162109, -0.028488, 0.671875, -0.354736, 0.885742, 0.620605, 2.439453, ...]
------------------------------------------------------------
Node 124 (call 102) - model.decoder.layers.1.self_attn.k_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-9.117188 max=5.113281 mean=-0.325472 std=1.967128
  Preview: [0.178345, 0.117493, 0.157104, -0.207764, 0.345459, -0.037292, 0.284424, 0.075378, 0.408447, -0.115540, -0.496338, 0.666992, -0.146606, 0.427490, 1.965820, 2.699219, ...]
------------------------------------------------------------
Node 125 (call 103) - model.decoder.layers.1.self_attn.v_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-0.563965 max=0.624023 mean=0.009227 std=0.204591
  Preview: [0.286865, 0.069397, 0.330078, -0.091858, -0.003742, -0.068726, -0.013557, 0.140381, 0.076233, -0.098450, -0.225464, -0.087158, 0.015083, 0.178101, -0.095032, -0.374268, ...]
------------------------------------------------------------
Node 126 (call 104) - model.decoder.layers.1.self_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-3.681641 max=4.375000 mean=0.003652 std=0.538989
  Preview: [-0.286377, -0.011139, -0.280029, -0.217041, 0.463135, 0.522461, -0.666992, -0.106567, -0.008339, 0.357666, 0.009995, 0.923828, -0.431152, -0.763184, -0.517578, 0.231079, ...]
------------------------------------------------------------
Node 127 (call 105) - model.decoder.layers.1.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-3.681641 max=4.375000 mean=0.003652 std=0.538989
  Preview: [-0.286377, -0.011139, -0.280029, -0.217041, 0.463135, 0.522461, -0.666992, -0.106567, -0.008339, 0.357666, 0.009995, 0.923828, -0.431152, -0.763184, -0.517578, 0.231079, ...]
------------------------------------------------------------
Node 128 (call 105) - model.decoder.layers.1.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,1]  Precision: FLOAT16
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 129 (call 106) - model.decoder.layers.1.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-10.937500 max=1.293945 mean=-0.002225 std=0.740799
  Preview: [-0.066162, 0.031677, 0.013351, -0.053436, 0.131470, 0.396729, -0.332031, -0.269531, -0.176514, 0.346191, 0.093323, 0.378662, -0.145630, -0.689453, -0.073181, 0.316650, ...]
------------------------------------------------------------
Node 130 (call 107) - model.decoder.layers.1.encoder_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-3.146484 max=4.296875 mean=-0.033335 std=1.001897
  Preview: [-3.001953, -0.444824, 1.463867, 0.236572, -1.775391, 0.101990, 0.682129, -0.308350, 0.152100, -0.163940, -1.510742, -1.444336, -0.838379, -0.875000, 1.141602, -1.597656, ...]
------------------------------------------------------------
Node 131 (call 108) - model.decoder.layers.1.encoder_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-3.023438 max=5.468750 mean=-0.003848 std=1.154356
  Note: stats computed on first 1000 of 239904 values
  Preview: [-1.046875, -0.370117, -2.345703, -1.199219, -1.105469, -0.706055, 2.656250, 0.642090, 1.599609, -0.883301, -0.494385, -0.772461, 0.299072, -0.288330, 0.652832, 0.568359, ...]
------------------------------------------------------------
Node 132 (call 109) - model.decoder.layers.1.encoder_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-2.267578 max=2.359375 mean=-0.035916 std=0.552141
  Note: stats computed on first 1000 of 239904 values
  Preview: [-1.513672, -0.123779, 0.863281, 0.015823, -0.316406, -0.294189, 0.105042, 0.332275, -0.130493, 0.678711, -0.670410, -0.073364, -0.202515, 0.145508, 0.366943, -0.213989, ...]
------------------------------------------------------------
Node 133 (call 110) - model.decoder.layers.1.encoder_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-2.140625 max=3.140625 mean=0.006320 std=0.462671
  Preview: [-0.192993, 0.131714, -0.622070, 0.103088, 0.236450, 0.067932, 0.147949, 0.245972, 0.340576, -0.693359, -0.368896, -0.108459, 0.154907, 0.332031, -0.240845, 0.247681, ...]
------------------------------------------------------------
Node 134 (call 111) - model.decoder.layers.1.encoder_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-2.140625 max=3.140625 mean=0.006320 std=0.462671
  Preview: [-0.192993, 0.131714, -0.622070, 0.103088, 0.236450, 0.067932, 0.147949, 0.245972, 0.340576, -0.693359, -0.368896, -0.108459, 0.154907, 0.332031, -0.240845, 0.247681, ...]
------------------------------------------------------------
Node 135 (call 111) - model.decoder.layers.1.encoder_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,833]  Precision: FLOAT16
  Stats: min=0.000006 max=0.017487 mean=0.001057 std=0.001490
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.006748, 0.006649, 0.006420, 0.005737, 0.004841, 0.004017, 0.003323, 0.002907, 0.002655, 0.002171, 0.001599, 0.000990, 0.002533, 0.007042, 0.007133, 0.000679, ...]
------------------------------------------------------------
Node 136 (call 112) - model.decoder.layers.1.final_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-2.205078 max=0.905762 mean=0.015595 std=0.257750
  Preview: [-0.080017, 0.044495, -0.134521, -0.008888, 0.134033, 0.192261, -0.189331, -0.095825, -0.022308, 0.046722, -0.018433, 0.246338, -0.045135, -0.313477, -0.108521, 0.208374, ...]
------------------------------------------------------------
Node 137 (call 113) - model.decoder.layers.1.mlp.fc1 :: Linear :: output
  Shape: [1,1,2304]  Precision: FLOAT16
  Stats: min=-4.804688 max=4.945312 mean=-0.010372 std=0.535152
  Note: stats computed on first 1000 of 2304 values
  Preview: [-0.031921, 0.096008, -0.408936, -0.268311, -0.005451, -0.130249, -0.035126, -0.146484, 0.485840, -0.214844, -0.296387, 0.223022, -0.119690, -2.816406, -0.241821, 0.323975, ...]
------------------------------------------------------------
Node 138 (call 114) - model.decoder.layers.1.mlp.activation_fn :: SiLUActivation :: output
  Shape: [1,1,1152]  Precision: FLOAT16
  Stats: min=-0.278564 max=5.835938 mean=0.036778 std=0.465104
  Note: stats computed on first 1000 of 1152 values
  Preview: [-0.029755, -0.217896, 0.566406, -0.158081, 0.038574, -0.150146, -0.266846, -0.270508, 0.097839, 0.006771, -0.184937, -0.061035, -0.086731, 0.245728, -0.211914, -0.006344, ...]
------------------------------------------------------------
Node 139 (call 115) - model.decoder.layers.1.mlp.fc2 :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-217.500000 max=30.031250 mean=-0.124012 std=13.864226
  Preview: [-2.669922, 1.277344, 0.770020, -0.142334, 1.785156, 0.065979, 2.066406, 0.885742, 0.319092, 1.312500, 1.799805, 1.889648, -0.424072, -9.265625, -0.142578, -2.031250, ...]
------------------------------------------------------------
Node 140 (call 116) - model.decoder.layers.1.mlp :: MoonshineDecoderMLP :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-217.500000 max=30.031250 mean=-0.124012 std=13.864226
  Preview: [-2.669922, 1.277344, 0.770020, -0.142334, 1.785156, 0.065979, 2.066406, 0.885742, 0.319092, 1.312500, 1.799805, 1.889648, -0.424072, -9.265625, -0.142578, -2.031250, ...]
------------------------------------------------------------
Node 141 (call 117) - model.decoder.layers.1 :: MoonshineDecoderLayer :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-249.125000 max=32.437500 mean=-0.123784 std=15.895808
  Preview: [-3.035156, 1.492188, 0.179199, -0.179932, 2.341797, 1.000977, 1.125000, 0.480957, 0.224609, 1.528320, 1.719727, 2.949219, -0.624023, -10.773438, -0.583496, -0.958984, ...]
------------------------------------------------------------
Node 142 (call 118) - model.decoder.layers.2.input_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-7.757812 max=1.170898 mean=0.008530 std=0.501417
  Preview: [-0.123352, 0.052826, 0.011520, -0.002619, 0.107788, 0.045654, 0.042847, 0.022629, 0.013145, 0.068115, 0.077209, 0.125977, -0.021133, -0.417725, -0.022675, -0.029434, ...]
------------------------------------------------------------
Node 143 (call 119) - model.decoder.layers.2.self_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-2.298828 max=2.925781 mean=0.025433 std=0.771644
  Preview: [0.562012, -0.736328, 0.184814, 0.283691, -0.189331, -0.134644, -0.175049, 1.740234, 0.364990, 0.604980, 0.462158, -0.291504, -0.641113, 0.444580, 0.672363, 0.621094, ...]
------------------------------------------------------------
Node 144 (call 120) - model.decoder.layers.2.self_attn.k_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-4.347656 max=11.039062 mean=0.103101 std=1.588786
  Preview: [0.011292, -0.075256, -0.086487, 0.002508, 0.084106, -0.023666, -0.490723, 0.128784, -0.245850, -0.077820, -0.147095, -0.907715, -0.215088, 2.796875, 1.833008, 0.277344, ...]
------------------------------------------------------------
Node 145 (call 121) - model.decoder.layers.2.self_attn.v_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-0.500000 max=0.604004 mean=-0.006414 std=0.130250
  Preview: [-0.043030, 0.002546, -0.022461, -0.103394, -0.087646, -0.233521, -0.129639, -0.165894, -0.067444, -0.002996, -0.176514, -0.085938, 0.013535, 0.090271, -0.013412, -0.292236, ...]
------------------------------------------------------------
Node 146 (call 122) - model.decoder.layers.2.self_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-3.212891 max=2.552734 mean=0.002753 std=0.386768
  Preview: [0.059814, 0.312256, 0.068237, -0.172485, 0.168213, 0.521484, -0.513184, -0.446289, -0.197388, 0.665039, 0.081421, 0.551758, -0.167969, -0.270752, -0.517578, 0.023972, ...]
------------------------------------------------------------
Node 147 (call 123) - model.decoder.layers.2.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-3.212891 max=2.552734 mean=0.002753 std=0.386768
  Preview: [0.059814, 0.312256, 0.068237, -0.172485, 0.168213, 0.521484, -0.513184, -0.446289, -0.197388, 0.665039, 0.081421, 0.551758, -0.167969, -0.270752, -0.517578, 0.023972, ...]
------------------------------------------------------------
Node 148 (call 123) - model.decoder.layers.2.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,1]  Precision: FLOAT16
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 149 (call 124) - model.decoder.layers.2.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-10.335938 max=1.648438 mean=0.014933 std=0.669405
  Preview: [-0.146118, 0.102600, 0.018997, -0.013359, 0.153809, 0.108765, 0.040741, 0.009033, 0.009644, 0.145508, 0.109436, 0.182251, -0.038269, -0.615234, -0.060638, -0.042572, ...]
------------------------------------------------------------
Node 150 (call 125) - model.decoder.layers.2.encoder_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-3.552734 max=2.314453 mean=0.049352 std=0.956013
  Preview: [0.239990, -0.673340, 0.443848, 0.159790, 1.152344, -1.285156, -1.260742, 2.314453, -0.081604, 1.823242, 0.673340, -1.041016, -0.031372, 1.493164, 1.504883, 0.485840, ...]
------------------------------------------------------------
Node 151 (call 126) - model.decoder.layers.2.encoder_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-9.390625 max=7.558594 mean=-0.098313 std=2.354832
  Note: stats computed on first 1000 of 239904 values
  Preview: [-9.320312, 2.189453, -0.274658, -1.383789, -1.147461, -1.968750, -1.589844, 3.185547, -0.357178, -0.499512, 5.980469, -2.531250, -4.597656, -4.628906, 2.634766, -3.638672, ...]
------------------------------------------------------------
Node 152 (call 127) - model.decoder.layers.2.encoder_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-1.433594 max=1.840820 mean=-0.036838 std=0.588441
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.521973, -0.224731, -0.033905, -0.014183, -0.461670, -0.591797, -0.676758, -0.009102, 0.670898, 0.250977, 0.168945, 0.402344, -0.059082, 0.550293, -0.827637, -0.414551, ...]
------------------------------------------------------------
Node 153 (call 128) - model.decoder.layers.2.encoder_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-7.417969 max=7.082031 mean=0.028176 std=0.944094
  Preview: [-0.560547, 0.013878, -0.205933, -0.937500, 0.763184, -0.722168, -0.871094, 0.387207, 0.104187, -0.715332, 0.039673, 0.047150, 0.403076, -2.335938, -1.029297, 1.193359, ...]
------------------------------------------------------------
Node 154 (call 129) - model.decoder.layers.2.encoder_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-7.417969 max=7.082031 mean=0.028176 std=0.944094
  Preview: [-0.560547, 0.013878, -0.205933, -0.937500, 0.763184, -0.722168, -0.871094, 0.387207, 0.104187, -0.715332, 0.039673, 0.047150, 0.403076, -2.335938, -1.029297, 1.193359, ...]
------------------------------------------------------------
Node 155 (call 129) - model.decoder.layers.2.encoder_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,833]  Precision: FLOAT16
  Stats: min=0.000006 max=0.031952 mean=0.001368 std=0.003781
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.022751, 0.022308, 0.021530, 0.020554, 0.019073, 0.017166, 0.015091, 0.013451, 0.012054, 0.010117, 0.008614, 0.008270, 0.013397, 0.019684, 0.018127, 0.008209, ...]
------------------------------------------------------------
Node 156 (call 130) - model.decoder.layers.2.final_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-3.271484 max=0.879395 mean=0.013631 std=0.253862
  Preview: [-0.100586, 0.054077, 0.003748, -0.036621, 0.103699, 0.021927, -0.004749, 0.016647, 0.006756, 0.044769, 0.051941, 0.103943, -0.009384, -0.366211, -0.060211, 0.007786, ...]
------------------------------------------------------------
Node 157 (call 131) - model.decoder.layers.2.mlp.fc1 :: Linear :: output
  Shape: [1,1,2304]  Precision: FLOAT16
  Stats: min=-1.953125 max=13.218750 mean=0.001948 std=0.559336
  Note: stats computed on first 1000 of 2304 values
  Preview: [0.031525, -0.330078, -0.021713, 0.298584, 0.858887, 0.163452, -1.042969, 0.020325, -1.198242, -0.307373, 0.028168, 0.099426, -0.001631, 0.039978, -0.033112, 0.115295, ...]
------------------------------------------------------------
Node 158 (call 132) - model.decoder.layers.2.mlp.activation_fn :: SiLUActivation :: output
  Shape: [1,1,1152]  Precision: FLOAT16
  Stats: min=-0.278564 max=18.796875 mean=0.024473 std=0.663309
  Note: stats computed on first 1000 of 1152 values
  Preview: [-0.273193, -0.103943, 0.042389, 0.054291, -0.000433, -0.231201, -0.035583, -0.134277, 0.005981, 0.151855, -0.071472, -0.090698, -0.085327, -0.148193, -0.083252, -0.239624, ...]
------------------------------------------------------------
Node 159 (call 133) - model.decoder.layers.2.mlp.fc2 :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-17.484375 max=3.031250 mean=-0.014543 std=1.150429
  Preview: [0.322998, 0.404785, 0.237549, 0.030243, 0.108032, 0.146729, 0.239014, -0.169800, 0.054291, 0.161987, 0.503906, 0.161499, -0.345215, -0.949707, 0.086670, 0.309570, ...]
------------------------------------------------------------
Node 160 (call 134) - model.decoder.layers.2.mlp :: MoonshineDecoderMLP :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-17.484375 max=3.031250 mean=-0.014543 std=1.150429
  Preview: [0.322998, 0.404785, 0.237549, 0.030243, 0.108032, 0.146729, 0.239014, -0.169800, 0.054291, 0.161987, 0.503906, 0.161499, -0.345215, -0.949707, 0.086670, 0.309570, ...]
------------------------------------------------------------
Node 161 (call 135) - model.decoder.layers.2 :: MoonshineDecoderLayer :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-277.250000 max=37.343750 mean=-0.107258 std=17.732203
  Preview: [-3.212891, 2.222656, 0.279053, -1.259766, 3.380859, 0.947266, -0.020264, 0.251953, 0.185669, 1.640625, 2.343750, 3.708984, -0.734375, -14.335938, -2.044922, 0.567871, ...]
------------------------------------------------------------
Node 162 (call 136) - model.decoder.layers.3.input_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-8.023438 max=1.436523 mean=0.018225 std=0.538703
  Preview: [-0.143188, 0.114380, 0.016449, -0.053223, 0.174072, 0.050751, 0.004128, 0.016464, 0.012642, 0.083130, 0.105713, 0.174805, -0.031052, -0.669922, -0.096436, 0.025772, ...]
------------------------------------------------------------
Node 163 (call 137) - model.decoder.layers.3.self_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-1.973633 max=2.427734 mean=0.006520 std=0.740134
  Preview: [-0.441406, 0.562988, -0.496582, 0.766113, -0.284668, -0.877441, -0.052734, -0.268799, 0.609863, 1.019531, 0.499512, -1.253906, 0.158447, -1.450195, 1.049805, 0.504395, ...]
------------------------------------------------------------
Node 164 (call 138) - model.decoder.layers.3.self_attn.k_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-3.912109 max=5.417969 mean=0.026324 std=1.183600
  Preview: [-0.066528, -0.093689, -0.015350, 0.036591, 0.199097, -0.163574, 0.075684, 0.049255, 0.185181, 0.052521, -0.115112, -0.016815, -0.041229, -0.029846, 0.239258, 0.210327, ...]
------------------------------------------------------------
Node 165 (call 139) - model.decoder.layers.3.self_attn.v_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-0.648926 max=0.698242 mean=0.011407 std=0.168435
  Preview: [0.083374, -0.102051, -0.154907, 0.009590, 0.027710, 0.134521, 0.069824, 0.020676, -0.107483, -0.148071, 0.179932, 0.168701, 0.043213, -0.006153, -0.005802, 0.016647, ...]
------------------------------------------------------------
Node 166 (call 140) - model.decoder.layers.3.self_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-4.093750 max=5.968750 mean=0.006916 std=0.603667
  Preview: [-0.343750, 0.405273, -0.780762, 0.483887, -0.204346, 0.428467, -0.482910, -0.433838, 0.279053, 0.878906, -0.189453, 1.053711, -0.123840, -0.556152, -0.775879, -0.118347, ...]
------------------------------------------------------------
Node 167 (call 141) - model.decoder.layers.3.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-4.093750 max=5.968750 mean=0.006916 std=0.603667
  Preview: [-0.343750, 0.405273, -0.780762, 0.483887, -0.204346, 0.428467, -0.482910, -0.433838, 0.279053, 0.878906, -0.189453, 1.053711, -0.123840, -0.556152, -0.775879, -0.118347, ...]
------------------------------------------------------------
Node 168 (call 141) - model.decoder.layers.3.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,1]  Precision: FLOAT16
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 169 (call 142) - model.decoder.layers.3.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-10.898438 max=2.054688 mean=0.013759 std=0.712543
  Preview: [-0.177612, 0.128662, -0.021866, -0.035858, 0.193604, 0.089478, -0.023117, -0.004642, 0.033783, 0.146973, 0.127319, 0.278564, -0.039581, -0.722168, -0.152222, 0.023849, ...]
------------------------------------------------------------
Node 170 (call 143) - model.decoder.layers.3.encoder_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-2.859375 max=3.771484 mean=0.099520 std=1.060956
  Preview: [0.824219, -1.733398, 0.099365, 0.059448, 0.444336, -2.560547, 0.044983, -0.641113, 0.895996, 1.369141, -0.669922, -0.620605, -0.859375, 0.082642, -0.242065, -0.079102, ...]
------------------------------------------------------------
Node 171 (call 144) - model.decoder.layers.3.encoder_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-11.023438 max=11.593750 mean=-0.034763 std=4.112491
  Note: stats computed on first 1000 of 239904 values
  Preview: [7.843750, -9.890625, 2.373047, -3.152344, -0.913086, -7.906250, -4.929688, -3.863281, 2.529297, 6.097656, -4.953125, -10.960938, -1.401367, 2.927734, -5.628906, 0.881836, ...]
------------------------------------------------------------
Node 172 (call 145) - model.decoder.layers.3.encoder_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-2.228516 max=1.794922 mean=-0.039137 std=0.692196
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.627930, -0.103882, -0.491455, -0.187012, -0.561035, 1.338867, 0.051971, -0.224976, 0.597168, 0.012184, -0.482666, -0.506836, -0.986816, 0.601562, -0.779297, -0.792480, ...]
------------------------------------------------------------
Node 173 (call 146) - model.decoder.layers.3.encoder_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-4.058594 max=21.093750 mean=0.049018 std=1.686319
  Preview: [-0.692383, -0.859863, 0.672852, 0.050446, 1.850586, 0.097961, -0.639160, -0.383301, 1.614258, -0.268066, -1.134766, -0.637207, -1.076172, -0.154419, -0.729004, 2.218750, ...]
------------------------------------------------------------
Node 174 (call 147) - model.decoder.layers.3.encoder_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-4.058594 max=21.093750 mean=0.049018 std=1.686319
  Preview: [-0.692383, -0.859863, 0.672852, 0.050446, 1.850586, 0.097961, -0.639160, -0.383301, 1.614258, -0.268066, -1.134766, -0.637207, -1.076172, -0.154419, -0.729004, 2.218750, ...]
------------------------------------------------------------
Node 175 (call 147) - model.decoder.layers.3.encoder_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.040985 mean=0.001100 std=0.004784
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.012894, 0.012115, 0.011833, 0.011833, 0.012207, 0.012207, 0.011467, 0.010689, 0.009811, 0.008797, 0.008133, 0.009148, 0.010361, 0.004089, 0.002520, 0.012398, ...]
------------------------------------------------------------
Node 176 (call 148) - model.decoder.layers.3.final_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-3.328125 max=0.938965 mean=0.014133 std=0.275236
  Preview: [-0.128906, 0.049347, 0.006191, -0.017776, 0.147461, 0.038361, -0.030060, -0.015274, 0.058197, 0.065796, 0.029037, 0.110413, -0.049957, -0.447998, -0.090332, 0.066101, ...]
------------------------------------------------------------
Node 177 (call 149) - model.decoder.layers.3.mlp.fc1 :: Linear :: output
  Shape: [1,1,2304]  Precision: FLOAT16
  Stats: min=-5.753906 max=2.681641 mean=0.003739 std=0.627463
  Note: stats computed on first 1000 of 2304 values
  Preview: [0.163818, -0.000737, 0.119019, 0.177979, 0.589355, -0.435303, -0.581055, 0.024216, -0.207764, 1.017578, -0.088257, 0.186768, 0.768555, 0.294434, -1.112305, 1.503906, ...]
------------------------------------------------------------
Node 178 (call 150) - model.decoder.layers.3.mlp.activation_fn :: SiLUActivation :: output
  Shape: [1,1,1152]  Precision: FLOAT16
  Stats: min=-0.278564 max=5.535156 mean=0.082396 std=0.467249
  Note: stats computed on first 1000 of 1152 values
  Preview: [-0.201172, -0.044769, -0.203125, 0.132202, -0.071350, 0.403564, -0.021408, -0.096130, 0.023926, 0.027496, -0.139771, -0.250977, -0.012207, -0.216064, -0.024765, 0.028214, ...]
------------------------------------------------------------
Node 179 (call 151) - model.decoder.layers.3.mlp.fc2 :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-1.676758 max=1.754883 mean=0.002515 std=0.592935
  Preview: [0.737793, -0.076599, -0.066650, 0.095154, -0.270996, 0.488525, -0.053894, 0.008835, -0.442139, 0.250977, 1.126953, 0.344238, 0.277588, -0.869141, -0.276367, -0.185425, ...]
------------------------------------------------------------
Node 180 (call 152) - model.decoder.layers.3.mlp :: MoonshineDecoderMLP :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-1.676758 max=1.754883 mean=0.002515 std=0.592935
  Preview: [0.737793, -0.076599, -0.066650, 0.095154, -0.270996, 0.488525, -0.053894, 0.008835, -0.442139, 0.250977, 1.126953, 0.344238, 0.277588, -0.869141, -0.276367, -0.185425, ...]
------------------------------------------------------------
Node 181 (call 153) - model.decoder.layers.3 :: MoonshineDecoderLayer :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-285.000000 max=52.437500 mean=-0.048628 std=18.472553
  Preview: [-3.511719, 1.693359, 0.104248, -0.630371, 4.757812, 1.961914, -1.196289, -0.556641, 1.635742, 2.503906, 2.146484, 4.468750, -1.657227, -15.914062, -3.824219, 2.482422, ...]
------------------------------------------------------------
Node 182 (call 154) - model.decoder.layers.4.input_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-8.289062 max=1.658203 mean=0.012535 std=0.559155
  Preview: [-0.163086, 0.072571, 0.005577, -0.024734, 0.198608, 0.080139, -0.051117, -0.020538, 0.069214, 0.092407, 0.099487, 0.168579, -0.066956, -0.670898, -0.171387, 0.089294, ...]
------------------------------------------------------------
Node 183 (call 155) - model.decoder.layers.4.self_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-2.632812 max=3.072266 mean=0.010715 std=0.831574
  Preview: [0.150635, -0.723145, 0.385742, 0.961426, 0.050568, -0.837402, -0.056976, 0.531738, 0.376221, -0.273438, 0.269775, 0.513184, -0.051819, 0.324707, -1.711914, -0.304932, ...]
------------------------------------------------------------
Node 184 (call 156) - model.decoder.layers.4.self_attn.k_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-3.843750 max=3.992188 mean=-0.037357 std=1.148820
  Preview: [-0.113525, -0.019958, 0.046478, -0.048035, -0.047485, -0.036438, 0.186646, 0.033997, -0.026688, -0.236572, 0.066711, -0.036316, 0.210205, 0.049286, -0.120361, -0.249756, ...]
------------------------------------------------------------
Node 185 (call 157) - model.decoder.layers.4.self_attn.v_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-0.863281 max=0.619141 mean=-0.015653 std=0.210944
  Preview: [-0.031311, 0.079651, 0.254639, 0.239380, 0.151733, -0.060516, -0.118469, 0.121887, 0.240723, -0.269531, -0.028305, -0.226685, 0.079590, 0.221313, 0.376221, -0.001535, ...]
------------------------------------------------------------
Node 186 (call 158) - model.decoder.layers.4.self_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-4.332031 max=14.937500 mean=0.007699 std=1.127712
  Preview: [-0.295410, 0.724121, -1.035156, 0.898438, -0.676270, 1.089844, -0.807129, 0.108704, -0.147949, 0.990234, 0.104675, 0.559082, -0.049103, -0.489258, -0.972656, 0.193848, ...]
------------------------------------------------------------
Node 187 (call 159) - model.decoder.layers.4.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-4.332031 max=14.937500 mean=0.007699 std=1.127712
  Preview: [-0.295410, 0.724121, -1.035156, 0.898438, -0.676270, 1.089844, -0.807129, 0.108704, -0.147949, 0.990234, 0.104675, 0.559082, -0.049103, -0.489258, -0.972656, 0.193848, ...]
------------------------------------------------------------
Node 188 (call 159) - model.decoder.layers.4.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,1]  Precision: FLOAT16
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 189 (call 160) - model.decoder.layers.4.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-9.757812 max=2.595703 mean=0.018357 std=0.677933
  Preview: [-0.167847, 0.115234, -0.049042, 0.016495, 0.227661, 0.169556, -0.100281, -0.018616, 0.082092, 0.205811, 0.122375, 0.262695, -0.080811, -0.741211, -0.237671, 0.133301, ...]
------------------------------------------------------------
Node 190 (call 161) - model.decoder.layers.4.encoder_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-3.378906 max=3.527344 mean=-0.013059 std=1.154314
  Preview: [-0.422852, -1.063477, -0.021652, -0.522461, -1.336914, -0.394775, -0.540527, -2.099609, -0.678223, -1.911133, -0.576172, -0.831543, -0.803711, -0.924316, -1.039062, -0.787109, ...]
------------------------------------------------------------
Node 191 (call 162) - model.decoder.layers.4.encoder_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-11.390625 max=12.578125 mean=-0.563249 std=4.797074
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.506836, -2.169922, 1.261719, -8.281250, -0.945801, -3.312500, -2.423828, -5.671875, -0.399658, -5.757812, -8.656250, 0.348877, 3.199219, 4.613281, 4.371094, 6.332031, ...]
------------------------------------------------------------
Node 192 (call 163) - model.decoder.layers.4.encoder_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-3.417969 max=2.695312 mean=0.072432 std=0.780810
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.082336, 0.013649, 0.275391, -0.869141, 0.073547, -0.509766, -1.805664, -1.005859, 0.032867, 0.470459, -0.145386, 0.616211, -0.292480, -0.221558, -0.375977, -0.246582, ...]
------------------------------------------------------------
Node 193 (call 164) - model.decoder.layers.4.encoder_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-10.335938 max=66.875000 mean=0.166882 std=5.461771
  Preview: [2.166016, 0.496582, 3.876953, 6.371094, 3.226562, 7.085938, 0.949219, 7.726562, -5.445312, -5.355469, 4.066406, -2.458984, -0.844727, 6.179688, 0.116516, 2.531250, ...]
------------------------------------------------------------
Node 194 (call 165) - model.decoder.layers.4.encoder_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-10.335938 max=66.875000 mean=0.166882 std=5.461771
  Preview: [2.166016, 0.496582, 3.876953, 6.371094, 3.226562, 7.085938, 0.949219, 7.726562, -5.445312, -5.355469, 4.066406, -2.458984, -0.844727, 6.179688, 0.116516, 2.531250, ...]
------------------------------------------------------------
Node 195 (call 165) - model.decoder.layers.4.encoder_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.131104 mean=0.001964 std=0.009368
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.026382, 0.025772, 0.025558, 0.024780, 0.023270, 0.020706, 0.017990, 0.016373, 0.015388, 0.014450, 0.014793, 0.017715, 0.017990, 0.021194, 0.020218, 0.026169, ...]
------------------------------------------------------------
Node 196 (call 166) - model.decoder.layers.4.final_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-4.300781 max=1.951172 mean=0.003089 std=0.368488
  Preview: [-0.053131, 0.084534, 0.083374, 0.178955, 0.200195, 0.245483, -0.037231, 0.228760, -0.120544, -0.061401, 0.155640, 0.070740, -0.079285, -0.315674, -0.121460, 0.138306, ...]
------------------------------------------------------------
Node 197 (call 167) - model.decoder.layers.4.mlp.fc1 :: Linear :: output
  Shape: [1,1,2304]  Precision: FLOAT16
  Stats: min=-2.964844 max=3.048828 mean=-0.002620 std=0.789597
  Note: stats computed on first 1000 of 2304 values
  Preview: [0.392334, -0.627441, -0.257324, 0.523438, 0.614258, 0.285889, -1.647461, -0.540527, -0.039551, 0.916016, -0.163940, 0.647461, 0.381104, 0.146851, -0.578613, -0.056274, ...]
------------------------------------------------------------
Node 198 (call 168) - model.decoder.layers.4.mlp.activation_fn :: SiLUActivation :: output
  Shape: [1,1,1152]  Precision: FLOAT16
  Stats: min=-0.278564 max=8.359375 mean=0.187067 std=0.566916
  Note: stats computed on first 1000 of 1152 values
  Preview: [0.866211, -0.169678, -0.085083, -0.032074, 0.391357, -0.194946, -0.004295, -0.061310, 0.159790, -0.223267, 0.203857, 0.402832, -0.132812, 0.120056, -0.091980, 0.325684, ...]
------------------------------------------------------------
Node 199 (call 169) - model.decoder.layers.4.mlp.fc2 :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-22.703125 max=11.320312 mean=-0.025981 std=2.220714
  Preview: [1.439453, -2.117188, -0.902344, -1.095703, -0.134644, 2.154297, 3.380859, 0.523438, -1.135742, -1.296875, 1.045898, 0.900879, -0.854004, 0.255371, -0.241089, -0.520508, ...]
------------------------------------------------------------
Node 200 (call 170) - model.decoder.layers.4.mlp :: MoonshineDecoderMLP :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-22.703125 max=11.320312 mean=-0.025981 std=2.220714
  Preview: [1.439453, -2.117188, -0.902344, -1.095703, -0.134644, 2.154297, 3.380859, 0.523438, -1.135742, -1.296875, 1.045898, 0.900879, -0.854004, 0.255371, -0.241089, -0.520508, ...]
------------------------------------------------------------
Node 201 (call 171) - model.decoder.layers.4 :: MoonshineDecoderLayer :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-292.750000 max=145.625000 mean=0.100037 std=21.192844
  Preview: [-0.201172, 0.796875, 2.042969, 5.546875, 7.175781, 12.296875, 2.326172, 7.800781, -5.093750, -3.158203, 7.367188, 3.468750, -3.404297, -9.968750, -4.921875, 4.687500, ...]
------------------------------------------------------------
Node 202 (call 172) - model.decoder.layers.5.input_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-7.015625 max=3.802734 mean=0.007222 std=0.552975
  Preview: [-0.011353, 0.025589, 0.068970, 0.208130, 0.227539, 0.347900, 0.091064, 0.252197, -0.193848, -0.106628, 0.245850, 0.101501, -0.121216, -0.393555, -0.188965, 0.157959, ...]
------------------------------------------------------------
Node 203 (call 173) - model.decoder.layers.5.self_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-4.777344 max=4.613281 mean=-0.005954 std=1.091545
  Preview: [-0.834473, 0.166626, -0.046082, -1.085938, -0.803711, 0.710449, -1.195312, -0.355957, -0.129272, -0.660645, -0.338135, 0.717285, -0.495361, -0.293945, -0.972168, 0.751465, ...]
------------------------------------------------------------
Node 204 (call 174) - model.decoder.layers.5.self_attn.k_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-5.820312 max=5.042969 mean=0.090623 std=1.598379
  Preview: [-0.047119, -0.118713, 0.295410, 0.165894, 0.022842, -0.280273, 0.133301, -0.225098, 0.036194, -0.171875, -0.152954, 0.159424, 0.667969, -0.070374, -0.960449, -0.390869, ...]
------------------------------------------------------------
Node 205 (call 175) - model.decoder.layers.5.self_attn.v_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-2.296875 max=1.570312 mean=0.032476 std=0.483664
  Preview: [-0.068359, 0.069641, 0.208496, 0.242798, -0.270020, 0.025406, 0.086182, 0.097717, -0.145386, 0.061615, 0.126831, -0.027283, -0.212769, -0.063843, -0.166382, -0.013153, ...]
------------------------------------------------------------
Node 206 (call 176) - model.decoder.layers.5.self_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-9.210938 max=58.656250 mean=0.097323 std=4.067997
  Preview: [1.564453, -0.963867, -1.523438, 0.109192, -0.174316, 0.447754, 1.549805, 0.967285, -1.865234, -1.030273, 0.665527, -0.610840, -1.583984, 2.773438, -0.886719, 1.314453, ...]
------------------------------------------------------------
Node 207 (call 177) - model.decoder.layers.5.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-9.210938 max=58.656250 mean=0.097323 std=4.067997
  Preview: [1.564453, -0.963867, -1.523438, 0.109192, -0.174316, 0.447754, 1.549805, 0.967285, -1.865234, -1.030273, 0.665527, -0.610840, -1.583984, 2.773438, -0.886719, 1.314453, ...]
------------------------------------------------------------
Node 208 (call 177) - model.decoder.layers.5.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,1]  Precision: FLOAT16
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 209 (call 178) - model.decoder.layers.5.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-6.332031 max=5.937500 mean=0.012918 std=0.618642
  Preview: [0.040680, -0.015015, 0.015366, 0.238403, 0.387939, 0.535645, 0.160889, 0.343018, -0.327148, -0.245239, 0.427002, 0.095581, -0.237183, -0.322266, -0.224121, 0.251709, ...]
------------------------------------------------------------
Node 210 (call 179) - model.decoder.layers.5.encoder_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-3.687500 max=2.769531 mean=-0.039181 std=1.147060
  Preview: [1.045898, -0.031616, -2.199219, 0.120178, -0.991699, 1.285156, -0.117859, 0.213501, 0.470459, 1.722656, 0.068237, -0.895996, -3.687500, 1.472656, 2.302734, 0.196533, ...]
------------------------------------------------------------
Node 211 (call 180) - model.decoder.layers.5.encoder_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-13.828125 max=11.882812 mean=0.277598 std=4.596105
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.743652, 2.453125, 0.279785, 3.523438, -3.800781, -0.570801, -7.953125, 1.197266, 10.289062, 7.675781, -0.391602, 1.127930, -0.785645, 3.496094, 0.496826, 1.904297, ...]
------------------------------------------------------------
Node 212 (call 181) - model.decoder.layers.5.encoder_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-2.851562 max=2.843750 mean=-0.060739 std=0.904537
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.980957, -0.591797, -0.954590, -0.411865, -2.197266, -0.805176, -0.544434, -0.278564, -0.073364, -0.044861, 0.566895, 0.006958, -0.735352, 0.397461, -1.073242, -0.669922, ...]
------------------------------------------------------------
Node 213 (call 182) - model.decoder.layers.5.encoder_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-25.625000 max=108.312500 mean=0.258842 std=11.336498
  Preview: [1.171875, 9.570312, -5.144531, 12.367188, 4.968750, 6.652344, -5.687500, 5.250000, -2.802734, 8.804688, 1.990234, -11.218750, 9.367188, 4.097656, -7.609375, -0.398682, ...]
------------------------------------------------------------
Node 214 (call 183) - model.decoder.layers.5.encoder_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-25.625000 max=108.312500 mean=0.258842 std=11.336498
  Preview: [1.171875, 9.570312, -5.144531, 12.367188, 4.968750, 6.652344, -5.687500, 5.250000, -2.802734, 8.804688, 1.990234, -11.218750, 9.367188, 4.097656, -7.609375, -0.398682, ...]
------------------------------------------------------------
Node 215 (call 183) - model.decoder.layers.5.encoder_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.119751 mean=0.001948 std=0.009545
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.005981, 0.005936, 0.005890, 0.005936, 0.005890, 0.005711, 0.005363, 0.005058, 0.004787, 0.004623, 0.004498, 0.003910, 0.004734, 0.005157, 0.004429, 0.003504, ...]
------------------------------------------------------------
Node 216 (call 184) - model.decoder.layers.5.final_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-2.212891 max=4.765625 mean=-0.007125 std=0.457689
  Preview: [0.061737, 0.296631, -0.162476, 0.533203, 0.350586, 0.586426, -0.076416, 0.408691, -0.316650, 0.131470, 0.270752, -0.234985, 0.111938, -0.112671, -0.397949, 0.145752, ...]
------------------------------------------------------------
Node 217 (call 185) - model.decoder.layers.5.mlp.fc1 :: Linear :: output
  Shape: [1,1,2304]  Precision: FLOAT16
  Stats: min=-4.238281 max=19.578125 mean=0.017584 std=1.494255
  Note: stats computed on first 1000 of 2304 values
  Preview: [2.140625, -0.573242, -4.238281, -1.444336, -1.786133, -0.359131, 0.301270, 1.635742, -1.362305, 0.643555, 0.553711, -1.886719, 0.718750, -1.181641, -1.186523, 1.404297, ...]
------------------------------------------------------------
Node 218 (call 186) - model.decoder.layers.5.mlp.activation_fn :: SiLUActivation :: output
  Shape: [1,1,1152]  Precision: FLOAT16
  Stats: min=-0.278564 max=4.054688 mean=0.144955 std=0.528207
  Note: stats computed on first 1000 of 1152 values
  Preview: [-0.110718, -0.108643, -0.119629, 0.466797, 0.031616, -0.203613, -0.087769, -0.109802, -0.243652, 0.152954, -0.132935, 0.034424, -0.256104, 0.082336, -0.195190, 0.366699, ...]
------------------------------------------------------------
Node 219 (call 187) - model.decoder.layers.5.mlp.fc2 :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-13.750000 max=28.796875 mean=0.142584 std=5.117561
  Preview: [-4.589844, 5.089844, -2.857422, -6.324219, -7.933594, 3.791016, 3.173828, -4.800781, -5.585938, -1.191406, 2.810547, 0.519043, -1.101562, -0.521484, 1.771484, 15.523438, ...]
------------------------------------------------------------
Node 220 (call 188) - model.decoder.layers.5.mlp :: MoonshineDecoderMLP :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-13.750000 max=28.796875 mean=0.142584 std=5.117561
  Preview: [-4.589844, 5.089844, -2.857422, -6.324219, -7.933594, 3.791016, 3.173828, -4.800781, -5.585938, -1.191406, 2.810547, 0.519043, -1.101562, -0.521484, 1.771484, 15.523438, ...]
------------------------------------------------------------
Node 221 (call 189) - model.decoder.layers.5 :: MoonshineDecoderLayer :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-97.000000 max=281.250000 mean=0.598897 std=21.091698
  Preview: [-2.054688, 14.500000, -7.484375, 11.703125, 4.035156, 23.187500, 1.361328, 9.218750, -15.351562, 3.425781, 12.835938, -7.839844, 3.277344, -3.619141, -11.648438, 21.125000, ...]
------------------------------------------------------------
Node 222 (call 190) - model.decoder.norm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-4.738281 max=2.316406 mean=-0.058492 std=0.892553
  Preview: [-0.203735, 1.079102, -0.595215, 0.834961, 0.242188, 1.560547, 0.070129, 0.668457, -1.244141, 0.206909, 0.895996, -0.621094, 0.212158, -0.293213, -0.872070, 1.045898, ...]
------------------------------------------------------------
Node 223 (call 191) - model.decoder :: MoonshineDecoder :: output.last_hidden_state
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-4.738281 max=2.316406 mean=-0.058492 std=0.892553
  Preview: [-0.203735, 1.079102, -0.595215, 0.834961, 0.242188, 1.560547, 0.070129, 0.668457, -1.244141, 0.206909, 0.895996, -0.621094, 0.212158, -0.293213, -0.872070, 1.045898, ...]
------------------------------------------------------------
Node 224 (call 191) - model.decoder :: MoonshineDecoder :: output.hidden_states[0]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-1.484375 max=0.408447 mean=0.000947 std=0.120418
  Preview: [0.004871, 0.019821, 0.064758, 0.019211, 0.024994, 0.024811, 0.000693, 0.011948, 0.028442, 0.014214, 0.005535, -0.566895, 0.000872, 0.000878, 0.007687, 0.006084, ...]
------------------------------------------------------------
Node 225 (call 191) - model.decoder :: MoonshineDecoder :: output.hidden_states[1]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-25.843750 max=5.222656 mean=-0.009816 std=1.833451
  Preview: [0.113831, 0.094116, 0.311279, 0.076355, -0.142334, 0.344727, -0.423340, -0.544434, -0.426758, 0.551270, 0.279053, 0.244141, 0.076355, -1.074219, 0.317383, 0.593262, ...]
------------------------------------------------------------
Node 226 (call 191) - model.decoder :: MoonshineDecoder :: output.hidden_states[2]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-249.125000 max=32.437500 mean=-0.123784 std=15.895808
  Preview: [-3.035156, 1.492188, 0.179199, -0.179932, 2.341797, 1.000977, 1.125000, 0.480957, 0.224609, 1.528320, 1.719727, 2.949219, -0.624023, -10.773438, -0.583496, -0.958984, ...]
------------------------------------------------------------
Node 227 (call 191) - model.decoder :: MoonshineDecoder :: output.hidden_states[3]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-277.250000 max=37.343750 mean=-0.107258 std=17.732203
  Preview: [-3.212891, 2.222656, 0.279053, -1.259766, 3.380859, 0.947266, -0.020264, 0.251953, 0.185669, 1.640625, 2.343750, 3.708984, -0.734375, -14.335938, -2.044922, 0.567871, ...]
------------------------------------------------------------
Node 228 (call 191) - model.decoder :: MoonshineDecoder :: output.hidden_states[4]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-285.000000 max=52.437500 mean=-0.048628 std=18.472553
  Preview: [-3.511719, 1.693359, 0.104248, -0.630371, 4.757812, 1.961914, -1.196289, -0.556641, 1.635742, 2.503906, 2.146484, 4.468750, -1.657227, -15.914062, -3.824219, 2.482422, ...]
------------------------------------------------------------
Node 229 (call 191) - model.decoder :: MoonshineDecoder :: output.hidden_states[5]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-292.750000 max=145.625000 mean=0.100037 std=21.192844
  Preview: [-0.201172, 0.796875, 2.042969, 5.546875, 7.175781, 12.296875, 2.326172, 7.800781, -5.093750, -3.158203, 7.367188, 3.468750, -3.404297, -9.968750, -4.921875, 4.687500, ...]
------------------------------------------------------------
Node 230 (call 191) - model.decoder :: MoonshineDecoder :: output.hidden_states[6]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-4.738281 max=2.316406 mean=-0.058492 std=0.892553
  Preview: [-0.203735, 1.079102, -0.595215, 0.834961, 0.242188, 1.560547, 0.070129, 0.668457, -1.244141, 0.206909, 0.895996, -0.621094, 0.212158, -0.293213, -0.872070, 1.045898, ...]
------------------------------------------------------------
Node 231 (call 191) - model.decoder :: MoonshineDecoder :: output.attentions[0]
  Shape: [1,8,1,1]  Precision: FLOAT16
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 232 (call 191) - model.decoder :: MoonshineDecoder :: output.attentions[1]
  Shape: [1,8,1,1]  Precision: FLOAT16
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 233 (call 191) - model.decoder :: MoonshineDecoder :: output.attentions[2]
  Shape: [1,8,1,1]  Precision: FLOAT16
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 234 (call 191) - model.decoder :: MoonshineDecoder :: output.attentions[3]
  Shape: [1,8,1,1]  Precision: FLOAT16
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 235 (call 191) - model.decoder :: MoonshineDecoder :: output.attentions[4]
  Shape: [1,8,1,1]  Precision: FLOAT16
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 236 (call 191) - model.decoder :: MoonshineDecoder :: output.attentions[5]
  Shape: [1,8,1,1]  Precision: FLOAT16
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 237 (call 191) - model.decoder :: MoonshineDecoder :: output.cross_attentions[0]
  Shape: [1,8,1,833]  Precision: FLOAT16
  Stats: min=0.000018 max=0.011856 mean=0.001284 std=0.001561
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.001385, 0.001405, 0.001395, 0.001336, 0.001243, 0.001089, 0.000926, 0.000795, 0.000710, 0.000628, 0.000574, 0.000698, 0.000813, 0.001390, 0.001546, 0.000906, ...]
------------------------------------------------------------
Node 238 (call 191) - model.decoder :: MoonshineDecoder :: output.cross_attentions[1]
  Shape: [1,8,1,833]  Precision: FLOAT16
  Stats: min=0.000006 max=0.017487 mean=0.001057 std=0.001490
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.006748, 0.006649, 0.006420, 0.005737, 0.004841, 0.004017, 0.003323, 0.002907, 0.002655, 0.002171, 0.001599, 0.000990, 0.002533, 0.007042, 0.007133, 0.000679, ...]
------------------------------------------------------------
Node 239 (call 191) - model.decoder :: MoonshineDecoder :: output.cross_attentions[2]
  Shape: [1,8,1,833]  Precision: FLOAT16
  Stats: min=0.000006 max=0.031952 mean=0.001368 std=0.003781
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.022751, 0.022308, 0.021530, 0.020554, 0.019073, 0.017166, 0.015091, 0.013451, 0.012054, 0.010117, 0.008614, 0.008270, 0.013397, 0.019684, 0.018127, 0.008209, ...]
------------------------------------------------------------
Node 240 (call 191) - model.decoder :: MoonshineDecoder :: output.cross_attentions[3]
  Shape: [1,8,1,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.040985 mean=0.001100 std=0.004784
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.012894, 0.012115, 0.011833, 0.011833, 0.012207, 0.012207, 0.011467, 0.010689, 0.009811, 0.008797, 0.008133, 0.009148, 0.010361, 0.004089, 0.002520, 0.012398, ...]
------------------------------------------------------------
Node 241 (call 191) - model.decoder :: MoonshineDecoder :: output.cross_attentions[4]
  Shape: [1,8,1,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.131104 mean=0.001964 std=0.009368
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.026382, 0.025772, 0.025558, 0.024780, 0.023270, 0.020706, 0.017990, 0.016373, 0.015388, 0.014450, 0.014793, 0.017715, 0.017990, 0.021194, 0.020218, 0.026169, ...]
------------------------------------------------------------
Node 242 (call 191) - model.decoder :: MoonshineDecoder :: output.cross_attentions[5]
  Shape: [1,8,1,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.119751 mean=0.001948 std=0.009545
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.005981, 0.005936, 0.005890, 0.005936, 0.005890, 0.005711, 0.005363, 0.005058, 0.004787, 0.004623, 0.004498, 0.003910, 0.004734, 0.005157, 0.004429, 0.003504, ...]
------------------------------------------------------------
Node 243 (call 192) - model :: MoonshineModel :: output.last_hidden_state
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-4.738281 max=2.316406 mean=-0.058492 std=0.892553
  Preview: [-0.203735, 1.079102, -0.595215, 0.834961, 0.242188, 1.560547, 0.070129, 0.668457, -1.244141, 0.206909, 0.895996, -0.621094, 0.212158, -0.293213, -0.872070, 1.045898, ...]
------------------------------------------------------------
Node 244 (call 192) - model :: MoonshineModel :: output.decoder_hidden_states[0]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-1.484375 max=0.408447 mean=0.000947 std=0.120418
  Preview: [0.004871, 0.019821, 0.064758, 0.019211, 0.024994, 0.024811, 0.000693, 0.011948, 0.028442, 0.014214, 0.005535, -0.566895, 0.000872, 0.000878, 0.007687, 0.006084, ...]
------------------------------------------------------------
Node 245 (call 192) - model :: MoonshineModel :: output.decoder_hidden_states[1]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-25.843750 max=5.222656 mean=-0.009816 std=1.833451
  Preview: [0.113831, 0.094116, 0.311279, 0.076355, -0.142334, 0.344727, -0.423340, -0.544434, -0.426758, 0.551270, 0.279053, 0.244141, 0.076355, -1.074219, 0.317383, 0.593262, ...]
------------------------------------------------------------
Node 246 (call 192) - model :: MoonshineModel :: output.decoder_hidden_states[2]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-249.125000 max=32.437500 mean=-0.123784 std=15.895808
  Preview: [-3.035156, 1.492188, 0.179199, -0.179932, 2.341797, 1.000977, 1.125000, 0.480957, 0.224609, 1.528320, 1.719727, 2.949219, -0.624023, -10.773438, -0.583496, -0.958984, ...]
------------------------------------------------------------
Node 247 (call 192) - model :: MoonshineModel :: output.decoder_hidden_states[3]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-277.250000 max=37.343750 mean=-0.107258 std=17.732203
  Preview: [-3.212891, 2.222656, 0.279053, -1.259766, 3.380859, 0.947266, -0.020264, 0.251953, 0.185669, 1.640625, 2.343750, 3.708984, -0.734375, -14.335938, -2.044922, 0.567871, ...]
------------------------------------------------------------
Node 248 (call 192) - model :: MoonshineModel :: output.decoder_hidden_states[4]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-285.000000 max=52.437500 mean=-0.048628 std=18.472553
  Preview: [-3.511719, 1.693359, 0.104248, -0.630371, 4.757812, 1.961914, -1.196289, -0.556641, 1.635742, 2.503906, 2.146484, 4.468750, -1.657227, -15.914062, -3.824219, 2.482422, ...]
------------------------------------------------------------
Node 249 (call 192) - model :: MoonshineModel :: output.decoder_hidden_states[5]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-292.750000 max=145.625000 mean=0.100037 std=21.192844
  Preview: [-0.201172, 0.796875, 2.042969, 5.546875, 7.175781, 12.296875, 2.326172, 7.800781, -5.093750, -3.158203, 7.367188, 3.468750, -3.404297, -9.968750, -4.921875, 4.687500, ...]
------------------------------------------------------------
Node 250 (call 192) - model :: MoonshineModel :: output.decoder_hidden_states[6]
  Shape: [1,1,288]  Precision: FLOAT16
  Stats: min=-4.738281 max=2.316406 mean=-0.058492 std=0.892553
  Preview: [-0.203735, 1.079102, -0.595215, 0.834961, 0.242188, 1.560547, 0.070129, 0.668457, -1.244141, 0.206909, 0.895996, -0.621094, 0.212158, -0.293213, -0.872070, 1.045898, ...]
------------------------------------------------------------
Node 251 (call 192) - model :: MoonshineModel :: output.decoder_attentions[0]
  Shape: [1,8,1,1]  Precision: FLOAT16
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 252 (call 192) - model :: MoonshineModel :: output.decoder_attentions[1]
  Shape: [1,8,1,1]  Precision: FLOAT16
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 253 (call 192) - model :: MoonshineModel :: output.decoder_attentions[2]
  Shape: [1,8,1,1]  Precision: FLOAT16
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 254 (call 192) - model :: MoonshineModel :: output.decoder_attentions[3]
  Shape: [1,8,1,1]  Precision: FLOAT16
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 255 (call 192) - model :: MoonshineModel :: output.decoder_attentions[4]
  Shape: [1,8,1,1]  Precision: FLOAT16
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 256 (call 192) - model :: MoonshineModel :: output.decoder_attentions[5]
  Shape: [1,8,1,1]  Precision: FLOAT16
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 257 (call 192) - model :: MoonshineModel :: output.cross_attentions[0]
  Shape: [1,8,1,833]  Precision: FLOAT16
  Stats: min=0.000018 max=0.011856 mean=0.001284 std=0.001561
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.001385, 0.001405, 0.001395, 0.001336, 0.001243, 0.001089, 0.000926, 0.000795, 0.000710, 0.000628, 0.000574, 0.000698, 0.000813, 0.001390, 0.001546, 0.000906, ...]
------------------------------------------------------------
Node 258 (call 192) - model :: MoonshineModel :: output.cross_attentions[1]
  Shape: [1,8,1,833]  Precision: FLOAT16
  Stats: min=0.000006 max=0.017487 mean=0.001057 std=0.001490
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.006748, 0.006649, 0.006420, 0.005737, 0.004841, 0.004017, 0.003323, 0.002907, 0.002655, 0.002171, 0.001599, 0.000990, 0.002533, 0.007042, 0.007133, 0.000679, ...]
------------------------------------------------------------
Node 259 (call 192) - model :: MoonshineModel :: output.cross_attentions[2]
  Shape: [1,8,1,833]  Precision: FLOAT16
  Stats: min=0.000006 max=0.031952 mean=0.001368 std=0.003781
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.022751, 0.022308, 0.021530, 0.020554, 0.019073, 0.017166, 0.015091, 0.013451, 0.012054, 0.010117, 0.008614, 0.008270, 0.013397, 0.019684, 0.018127, 0.008209, ...]
------------------------------------------------------------
Node 260 (call 192) - model :: MoonshineModel :: output.cross_attentions[3]
  Shape: [1,8,1,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.040985 mean=0.001100 std=0.004784
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.012894, 0.012115, 0.011833, 0.011833, 0.012207, 0.012207, 0.011467, 0.010689, 0.009811, 0.008797, 0.008133, 0.009148, 0.010361, 0.004089, 0.002520, 0.012398, ...]
------------------------------------------------------------
Node 261 (call 192) - model :: MoonshineModel :: output.cross_attentions[4]
  Shape: [1,8,1,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.131104 mean=0.001964 std=0.009368
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.026382, 0.025772, 0.025558, 0.024780, 0.023270, 0.020706, 0.017990, 0.016373, 0.015388, 0.014450, 0.014793, 0.017715, 0.017990, 0.021194, 0.020218, 0.026169, ...]
------------------------------------------------------------
Node 262 (call 192) - model :: MoonshineModel :: output.cross_attentions[5]
  Shape: [1,8,1,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.119751 mean=0.001948 std=0.009545
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.005981, 0.005936, 0.005890, 0.005936, 0.005890, 0.005711, 0.005363, 0.005058, 0.004787, 0.004623, 0.004498, 0.003910, 0.004734, 0.005157, 0.004429, 0.003504, ...]
------------------------------------------------------------
Node 263 (call 192) - model :: MoonshineModel :: output.encoder_last_hidden_state
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-3.701172 max=4.226562 mean=0.029271 std=0.487593
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.180664, -0.195435, 0.162720, 0.162354, -1.183594, 0.366211, -0.022263, 0.021271, -0.057770, 0.387207, -0.011963, -0.082275, 0.152466, -0.176147, 0.085754, 0.049591, ...]
------------------------------------------------------------
Node 264 (call 192) - model :: MoonshineModel :: output.encoder_hidden_states[0]
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-0.169922 max=17.937500 mean=0.736143 std=1.061226
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.250000, 1.098633, 0.815918, 0.549316, 0.711914, 0.537109, 0.040771, 0.942871, 0.152344, 1.143555, 0.347900, 1.096680, 0.880859, 1.114258, -0.169922, 0.751953, ...]
------------------------------------------------------------
Node 265 (call 192) - model :: MoonshineModel :: output.encoder_hidden_states[1]
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-17.218750 max=44.656250 mean=0.672370 std=3.329599
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.925781, 0.047607, -0.174805, 0.297363, -0.421143, -1.183594, 0.032959, -1.406250, 3.177734, -0.785156, -1.071289, 0.006348, 0.154297, -0.375977, 0.962891, -0.447754, ...]
------------------------------------------------------------
Node 266 (call 192) - model :: MoonshineModel :: output.encoder_hidden_states[2]
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-52.468750 max=121.625000 mean=0.623737 std=8.783505
  Note: stats computed on first 1000 of 239904 values
  Preview: [3.945312, 0.179688, -4.558594, 6.570312, -9.648438, -2.425781, 0.180664, -4.234375, -0.672852, 1.275391, -1.276367, 3.583984, 5.570312, -7.789062, 0.263184, -1.621094, ...]
------------------------------------------------------------
Node 267 (call 192) - model :: MoonshineModel :: output.encoder_hidden_states[3]
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-91.000000 max=206.250000 mean=0.632176 std=14.623592
  Note: stats computed on first 1000 of 239904 values
  Preview: [8.203125, 0.368652, -4.500000, 13.453125, -20.500000, 1.839844, -2.031250, 1.167969, 3.203125, -0.643555, -1.074219, 0.818359, 5.359375, -16.437500, -1.957031, 2.921875, ...]
------------------------------------------------------------
Node 268 (call 192) - model :: MoonshineModel :: output.encoder_hidden_states[4]
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-119.812500 max=237.750000 mean=0.651533 std=19.011370
  Note: stats computed on first 1000 of 239904 values
  Preview: [2.714844, 0.082031, -1.601562, 23.546875, -25.031250, 2.988281, -11.031250, 4.355469, 1.816406, -1.885742, 5.914062, 5.699219, 4.066406, -21.937500, -3.333984, 11.906250, ...]
------------------------------------------------------------
Node 269 (call 192) - model :: MoonshineModel :: output.encoder_hidden_states[5]
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-177.000000 max=279.750000 mean=0.380527 std=25.223326
  Note: stats computed on first 1000 of 239904 values
  Preview: [7.308594, -3.902344, 3.363281, 16.875000, -41.343750, 13.476562, -6.953125, 4.414062, 3.998047, 6.750000, -7.179688, 2.554688, 0.501953, -11.937500, 0.097656, 6.898438, ...]
------------------------------------------------------------
Node 270 (call 192) - model :: MoonshineModel :: output.encoder_hidden_states[6]
  Shape: [1,833,288]  Precision: FLOAT16
  Stats: min=-3.701172 max=4.226562 mean=0.029271 std=0.487593
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.180664, -0.195435, 0.162720, 0.162354, -1.183594, 0.366211, -0.022263, 0.021271, -0.057770, 0.387207, -0.011963, -0.082275, 0.152466, -0.176147, 0.085754, 0.049591, ...]
------------------------------------------------------------
Node 271 (call 192) - model :: MoonshineModel :: output.encoder_attentions[0]
  Shape: [1,8,833,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.838379 mean=0.001930 std=0.027487
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.207886, 0.001838, 0.000144, 0.000424, 0.006985, 0.023499, 0.003481, 0.000117, 0.000023, 0.000144, 0.006039, 0.049042, 0.007927, 0.000047, 0.000001, 0.000002, ...]
------------------------------------------------------------
Node 272 (call 192) - model :: MoonshineModel :: output.encoder_attentions[1]
  Shape: [1,8,833,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.087097 mean=0.001840 std=0.006808
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.087097, 0.065247, 0.044128, 0.034241, 0.029633, 0.024948, 0.017685, 0.009903, 0.006008, 0.004162, 0.003933, 0.004059, 0.002859, 0.001321, 0.007290, 0.001943, ...]
------------------------------------------------------------
Node 273 (call 192) - model :: MoonshineModel :: output.encoder_attentions[2]
  Shape: [1,8,833,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.040527 mean=0.001987 std=0.005576
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.017563, 0.021698, 0.026993, 0.031799, 0.036194, 0.039734, 0.040527, 0.036316, 0.028961, 0.022385, 0.019745, 0.023270, 0.027420, 0.013893, 0.022644, 0.018127, ...]
------------------------------------------------------------
Node 274 (call 192) - model :: MoonshineModel :: output.encoder_attentions[3]
  Shape: [1,8,833,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.084167 mean=0.001963 std=0.007322
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.023926, 0.051056, 0.083557, 0.084167, 0.058319, 0.038239, 0.030014, 0.027435, 0.025085, 0.019226, 0.012611, 0.008369, 0.006699, 0.002874, 0.004826, 0.013214, ...]
------------------------------------------------------------
Node 275 (call 192) - model :: MoonshineModel :: output.encoder_attentions[4]
  Shape: [1,8,833,833]  Precision: FLOAT16
  Stats: min=0.000004 max=0.029572 mean=0.001199 std=0.002609
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.021179, 0.016052, 0.014000, 0.013206, 0.012848, 0.013046, 0.014053, 0.014557, 0.013206, 0.009323, 0.005089, 0.002312, 0.001400, 0.002817, 0.004269, 0.001370, ...]
------------------------------------------------------------
Node 276 (call 192) - model :: MoonshineModel :: output.encoder_attentions[5]
  Shape: [1,8,833,833]  Precision: FLOAT16
  Stats: min=0.000000 max=0.035858 mean=0.001003 std=0.004185
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.000105, 0.000106, 0.000108, 0.000107, 0.000102, 0.000093, 0.000082, 0.000077, 0.000072, 0.000069, 0.000058, 0.000046, 0.000014, 0.000001, 0.000001, 0.000031, ...]
------------------------------------------------------------
Node 277 (call 193) - proj_out :: Linear :: output
  Shape: [1,1,32768]  Precision: FLOAT16
  Stats: min=-16.375000 max=9.773438 mean=-3.531772 std=4.745719
  Note: stats computed on first 1000 of 32768 values
  Preview: [-16.375000, 3.169922, 0.457031, -8.242188, -8.039062, -7.960938, -8.039062, -8.281250, -7.808594, -8.218750, -8.203125, -8.242188, -8.140625, -8.296875, -8.085938, -8.257812, ...]
