=== HuggingFace Moonshine Named-Module Intermediate Outputs ===
Model: UsefulSensors/moonshine-tiny
Audio: /home/karen/Documents/cactus/tests/assets/test.wav
Device: cpu
Captured records: 224
Capture inputs: False
Transcription: Hello hello hello just um quickly testing out creating a wave file through voice memos um the goal is to use this wave file to test out whisper hopefully this will transcribe properly that's all I can hope for alright here we go

============================================================

------------------------------------------------------------
Node 0 (call 1) - model.encoder.conv1 :: Conv1d :: output
  Shape: [1,288,5007]  Precision: FLOAT32
  Stats: min=-0.426224 max=0.748098 mean=0.002023 std=0.065704
  Note: stats computed on first 1000 of 1442016 values
  Preview: [0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, ...]
------------------------------------------------------------
Node 1 (call 2) - model.encoder.groupnorm :: GroupNorm :: output
  Shape: [1,288,5007]  Precision: FLOAT32
  Stats: min=-0.082607 max=0.130744 mean=0.000592 std=0.013154
  Note: stats computed on first 1000 of 1442016 values
  Preview: [0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, ...]
------------------------------------------------------------
Node 2 (call 3) - model.encoder.conv2 :: Conv1d :: output
  Shape: [1,576,1667]  Precision: FLOAT32
  Stats: min=-21.477932 max=17.481733 mean=-0.051263 std=1.891103
  Note: stats computed on first 1000 of 960192 values
  Preview: [-0.055752, -0.055752, -0.055752, -0.055752, -0.055752, -0.055752, -0.055752, -0.055752, -0.055752, -0.055857, -0.053531, -0.061118, -0.056058, -0.033147, -0.099938, -0.041056, ...]
------------------------------------------------------------
Node 3 (call 4) - model.encoder.conv3 :: Conv1d :: output
  Shape: [1,288,833]  Precision: FLOAT32
  Stats: min=-58.900169 max=7.016562 mean=1.327753 std=5.011224
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.367095, 1.367095, 1.367095, 1.367095, 1.367118, 1.367283, 1.367231, 1.368005, 1.366999, 1.366978, 1.367521, 1.368032, 1.364585, 1.394792, 1.476753, 1.365572, ...]
------------------------------------------------------------
Node 4 (call 5) - model.encoder.rotary_emb :: MoonshineRotaryEmbedding :: output[0]
  Shape: [1,833,32]  Precision: FLOAT32
  Stats: min=-0.999961 max=1.000000 mean=0.635629 std=0.615898
  Note: stats computed on first 1000 of 26656 values
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, ...]
------------------------------------------------------------
Node 5 (call 5) - model.encoder.rotary_emb :: MoonshineRotaryEmbedding :: output[1]
  Shape: [1,833,32]  Precision: FLOAT32
  Stats: min=-0.999990 max=0.999993 mean=0.165604 std=0.434995
  Note: stats computed on first 1000 of 26656 values
  Preview: [0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, ...]
------------------------------------------------------------
Node 6 (call 6) - model.encoder.layers.0.input_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-0.862336 max=5.538082 mean=-0.029207 std=0.411664
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.209784, 0.173638, 0.036818, -0.091005, -0.017873, -0.085657, -0.388287, 0.109183, -0.158693, 0.128860, -0.162583, 0.166229, 0.047985, 0.180506, -0.476131, 0.006181, ...]
------------------------------------------------------------
Node 7 (call 7) - model.encoder.layers.0.self_attn.q_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-4.411019 max=4.298198 mean=0.054778 std=1.431083
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.345022, -2.352002, -2.277231, 0.107283, -1.559785, 0.676009, -1.372143, 0.880714, 1.597135, -0.175779, 1.618484, -0.282176, 0.174163, -0.928872, 0.653102, 1.446692, ...]
------------------------------------------------------------
Node 8 (call 8) - model.encoder.layers.0.self_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-7.515983 max=9.216268 mean=0.017474 std=2.523773
  Note: stats computed on first 1000 of 239904 values
  Preview: [8.482825, 2.366151, -3.863383, -3.008265, -3.995601, 1.043804, -3.600246, 1.207852, 3.328593, -0.570381, 2.646869, -0.632931, 1.395728, -1.508700, 1.689167, 3.117201, ...]
------------------------------------------------------------
Node 9 (call 9) - model.encoder.layers.0.self_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-1.547188 max=1.720422 mean=0.013610 std=0.444827
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.223439, -0.339504, 0.090574, 0.517715, 0.094528, 0.401758, -0.260477, -0.084854, 0.055928, -0.170395, -0.504441, 0.244036, -0.163175, -0.311767, 0.336967, -0.020661, ...]
------------------------------------------------------------
Node 10 (call 10) - model.encoder.layers.0.self_attn.o_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-12.897276 max=11.163143 mean=-0.085914 std=1.158436
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.561369, -0.626552, -0.169485, 0.282854, -0.256646, 0.384653, 0.278791, -0.857329, 1.061151, -0.244771, 0.045502, -0.584109, 0.325742, -0.784716, 0.856896, -0.560135, ...]
------------------------------------------------------------
Node 11 (call 11) - model.encoder.layers.0.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-12.897276 max=11.163143 mean=-0.085914 std=1.158436
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.561369, -0.626552, -0.169485, 0.282854, -0.256646, 0.384653, 0.278791, -0.857329, 1.061151, -0.244771, 0.045502, -0.584109, 0.325742, -0.784716, 0.856896, -0.560135, ...]
------------------------------------------------------------
Node 12 (call 12) - model.encoder.layers.0.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-2.092260 max=3.404137 mean=-0.045563 std=0.491279
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.018223, -0.128262, -0.011228, 0.121644, -0.151597, 0.206593, -0.231535, -0.573733, 0.327089, 0.181389, -0.220712, -0.112974, 0.290237, -0.239702, 0.041734, -0.523369, ...]
------------------------------------------------------------
Node 13 (call 13) - model.encoder.layers.0.mlp.fc1 :: Linear :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-6.570933 max=1.955544 mean=-1.079442 std=1.101575
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.445857, -0.128141, -0.596236, -2.250732, -1.012624, -0.359444, -0.677214, -0.027930, -2.362596, -0.734291, -1.628832, -1.444986, -1.016243, -2.296980, -2.165363, -2.914287, ...]
------------------------------------------------------------
Node 14 (call 14) - model.encoder.layers.0.mlp.activation_fn :: GELUActivation :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-0.169971 max=1.906148 mean=-0.035278 std=0.206524
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.146174, -0.057538, -0.164268, -0.027462, -0.157584, -0.129267, -0.168718, -0.013654, -0.021437, -0.169904, -0.084169, -0.107263, -0.157271, -0.024830, -0.032870, -0.005195, ...]
------------------------------------------------------------
Node 15 (call 15) - model.encoder.layers.0.mlp.fc2 :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-12.676361 max=32.522968 mean=0.022079 std=2.746209
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.231354, -0.427847, -0.829340, -0.537448, -0.865014, -2.109002, -0.293069, -1.487997, 1.959187, -1.678023, -1.467155, -0.508944, -1.055414, -0.710932, 0.280303, -0.645491, ...]
------------------------------------------------------------
Node 16 (call 16) - model.encoder.layers.0.mlp :: MoonshineEncoderMLP :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-12.676361 max=32.522968 mean=0.022079 std=2.746209
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.231354, -0.427847, -0.829340, -0.537448, -0.865014, -2.109002, -0.293069, -1.487997, 1.959187, -1.678023, -1.467155, -0.508944, -1.055414, -0.710932, 0.280303, -0.645491, ...]
------------------------------------------------------------
Node 17 (call 17) - model.encoder.layers.0 :: MoonshineEncoderLayer :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-17.224667 max=44.700821 mean=0.672309 std=3.330945
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.919788, 0.044346, -0.182903, 0.294990, -0.409678, -1.187226, 0.026535, -1.403606, 3.172574, -0.779908, -1.073637, 0.003678, 0.151032, -0.381488, 0.967268, -0.453867, ...]
------------------------------------------------------------
Node 18 (call 18) - model.encoder.layers.1.input_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.035305 max=7.451348 mean=-0.000069 std=0.574079
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.192442, -0.125263, -0.162037, -0.078904, -0.246914, -0.362125, -0.106797, -0.514778, 0.395027, -0.295405, -0.277761, -0.134881, -0.095106, -0.186275, 0.080867, -0.331800, ...]
------------------------------------------------------------
Node 19 (call 19) - model.encoder.layers.1.self_attn.q_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-7.431761 max=4.433569 mean=-0.178177 std=1.799361
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.485260, -0.400334, 1.814862, -0.103999, -2.096265, 1.087447, -1.408396, 2.865366, -0.517003, 2.192842, -0.424579, -2.906267, 2.603987, -0.974964, 1.066541, -2.667846, ...]
------------------------------------------------------------
Node 20 (call 20) - model.encoder.layers.1.self_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-7.201686 max=6.196480 mean=-0.111414 std=1.973930
  Note: stats computed on first 1000 of 239904 values
  Preview: [2.362592, 0.575113, 0.190098, 0.439631, -1.874382, 1.033322, -1.199612, 1.032127, 0.271637, 1.947471, -1.260710, -2.554000, 1.064060, -1.409331, -0.241773, -2.192155, ...]
------------------------------------------------------------
Node 21 (call 21) - model.encoder.layers.1.self_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.672433 max=3.578774 mean=-0.028641 std=1.117279
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.858158, -1.347375, -1.278311, 1.372388, 1.003395, -0.941527, 0.300901, -0.440267, -0.253994, 0.321806, -0.673640, -0.916473, -1.065961, -0.587611, 0.332001, -0.237802, ...]
------------------------------------------------------------
Node 22 (call 22) - model.encoder.layers.1.self_attn.o_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-17.160517 max=31.785357 mean=-0.014831 std=2.799585
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.973654, -2.122606, -1.055353, 1.363619, 0.216594, -0.218843, -0.818521, 1.203228, -3.575067, 0.927133, 1.902830, 0.586562, 1.576199, -1.010798, -1.103164, 0.519279, ...]
------------------------------------------------------------
Node 23 (call 23) - model.encoder.layers.1.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-17.160517 max=31.785357 mean=-0.014831 std=2.799585
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.973654, -2.122606, -1.055353, 1.363619, 0.216594, -0.218843, -0.818521, 1.203228, -3.575067, 0.927133, 1.902830, 0.586562, 1.576199, -1.010798, -1.103164, 0.519279, ...]
------------------------------------------------------------
Node 24 (call 24) - model.encoder.layers.1.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-4.530429 max=4.954497 mean=-0.017698 std=0.505812
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.033900, -0.337760, -0.265843, 0.122104, -0.140671, -0.275234, -0.168617, -0.149181, -0.129829, -0.086486, 0.017868, -0.011048, 0.132768, -0.282055, -0.164286, -0.111933, ...]
------------------------------------------------------------
Node 25 (call 25) - model.encoder.layers.1.mlp.fc1 :: Linear :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-6.439817 max=3.027680 mean=-0.999132 std=1.122905
  Note: stats computed on first 1000 of 959616 values
  Preview: [-1.807868, 1.361527, -2.206216, -0.372515, -0.565765, -2.976439, -1.001906, -2.070241, -2.992322, -0.024832, -1.612473, -0.690323, -2.796378, -2.269907, -1.938197, -1.323182, ...]
------------------------------------------------------------
Node 26 (call 26) - model.encoder.layers.1.mlp.activation_fn :: GELUActivation :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-0.169970 max=3.023950 mean=-0.006060 std=0.310607
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.063842, 1.243519, -0.030191, -0.132152, -0.161682, -0.004340, -0.158496, -0.039779, -0.004142, -0.012170, -0.086154, -0.169126, -0.007226, -0.026346, -0.050974, -0.122907, ...]
------------------------------------------------------------
Node 27 (call 27) - model.encoder.layers.1.mlp.fc2 :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-18.175730 max=45.414165 mean=-0.033969 std=4.751531
  Note: stats computed on first 1000 of 239904 values
  Preview: [3.007947, 2.254112, -3.330618, 4.906063, -9.453788, -1.021815, 0.957119, -4.035656, -0.273343, 1.119532, -2.097441, 2.973079, 3.841108, -6.415399, 0.395241, -1.682105, ...]
------------------------------------------------------------
Node 28 (call 28) - model.encoder.layers.1.mlp :: MoonshineEncoderMLP :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-18.175730 max=45.414165 mean=-0.033969 std=4.751531
  Note: stats computed on first 1000 of 239904 values
  Preview: [3.007947, 2.254112, -3.330618, 4.906063, -9.453788, -1.021815, 0.957119, -4.035656, -0.273343, 1.119532, -2.097441, 2.973079, 3.841108, -6.415399, 0.395241, -1.682105, ...]
------------------------------------------------------------
Node 29 (call 29) - model.encoder.layers.1 :: MoonshineEncoderLayer :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-52.424728 max=121.588913 mean=0.623508 std=8.783118
  Note: stats computed on first 1000 of 239904 values
  Preview: [3.954081, 0.175852, -4.568874, 6.564672, -9.646872, -2.427884, 0.165133, -4.236034, -0.675837, 1.266756, -1.268249, 3.563319, 5.568339, -7.807684, 0.259345, -1.616693, ...]
------------------------------------------------------------
Node 30 (call 30) - model.encoder.layers.2.input_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-2.933396 max=6.386298 mean=-0.000565 std=0.561087
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.324533, -0.043724, -0.433411, 0.543170, -0.965384, -0.259275, -0.038027, -0.505653, -0.117735, 0.072446, -0.159324, 0.223085, 0.394367, -0.818123, -0.040042, -0.244205, ...]
------------------------------------------------------------
Node 31 (call 31) - model.encoder.layers.2.self_attn.q_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-4.006958 max=4.383428 mean=0.092821 std=1.188761
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.863046, 0.091144, 0.435344, 0.282941, -0.706204, -0.402189, -0.509099, -0.482893, 0.646878, 0.674635, 0.866138, 1.447604, 1.172972, -1.064220, 2.288859, -1.054574, ...]
------------------------------------------------------------
Node 32 (call 32) - model.encoder.layers.2.self_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-10.812765 max=7.752889 mean=0.000747 std=2.443070
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.025724, -0.172445, -1.674155, -1.058569, -0.676723, 1.027430, -0.723575, 2.191213, 2.238913, -0.684843, 0.814000, 0.672712, -0.108565, -4.278103, 3.856594, -3.469474, ...]
------------------------------------------------------------
Node 33 (call 33) - model.encoder.layers.2.self_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-5.619289 max=6.741537 mean=0.018686 std=1.672800
  Note: stats computed on first 1000 of 239904 values
  Preview: [-4.483875, 2.357349, -0.882127, -1.853622, 0.253975, 4.718760, -0.119042, 0.631879, -2.391710, 0.689802, 0.581783, 2.026756, 0.941646, 1.318419, -0.947335, -1.560767, ...]
------------------------------------------------------------
Node 34 (call 34) - model.encoder.layers.2.self_attn.o_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-18.570335 max=42.997555 mean=0.035222 std=3.506826
  Note: stats computed on first 1000 of 239904 values
  Preview: [3.585690, 0.979981, 2.540822, -0.567747, -1.094153, 1.850344, -1.160552, 3.436589, 1.044642, -1.439476, -3.002086, 0.261936, -1.418292, -1.314624, -1.510548, 1.279121, ...]
------------------------------------------------------------
Node 35 (call 35) - model.encoder.layers.2.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-18.570335 max=42.997555 mean=0.035222 std=3.506826
  Note: stats computed on first 1000 of 239904 values
  Preview: [3.585690, 0.979981, 2.540822, -0.567747, -1.094153, 1.850344, -1.160552, 3.436589, 1.044642, -1.439476, -3.002086, 0.261936, -1.418292, -1.314624, -1.510548, 1.279121, ...]
------------------------------------------------------------
Node 36 (call 36) - model.encoder.layers.2.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-4.051925 max=5.989649 mean=-0.011612 std=0.538078
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.405395, 0.031401, -0.171253, 0.330048, -0.773781, -0.080412, -0.099440, -0.124135, -0.018247, -0.055335, -0.301669, 0.214956, 0.231584, -0.604788, -0.161867, -0.076434, ...]
------------------------------------------------------------
Node 37 (call 37) - model.encoder.layers.2.mlp.fc1 :: Linear :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-5.524934 max=5.976073 mean=-0.884260 std=1.075227
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.322035, -1.016096, -1.304748, -1.221418, -0.092022, -1.519014, -1.012946, 0.595391, -0.633371, -3.213105, -1.198609, -1.698552, -0.571496, -0.784792, -0.527222, -0.603954, ...]
------------------------------------------------------------
Node 38 (call 38) - model.encoder.layers.2.mlp.activation_fn :: GELUActivation :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-0.169971 max=5.976073 mean=0.000673 std=0.359272
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.120349, -0.157283, -0.125242, -0.135533, -0.042637, -0.097793, -0.157557, 0.431187, -0.166732, -0.002110, -0.138248, -0.075928, -0.162209, -0.169741, -0.157650, -0.164841, ...]
------------------------------------------------------------
Node 39 (call 39) - model.encoder.layers.2.mlp.fc2 :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-25.037777 max=41.759823 mean=-0.026819 std=4.806628
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.665902, -0.807745, -2.485750, 7.448405, -9.746355, 2.418214, -1.058247, 1.978506, 2.834848, -0.474751, 3.189390, -3.030795, 1.211649, -7.333964, -0.709869, 3.259298, ...]
------------------------------------------------------------
Node 40 (call 40) - model.encoder.layers.2.mlp :: MoonshineEncoderMLP :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-25.037777 max=41.759823 mean=-0.026819 std=4.806628
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.665902, -0.807745, -2.485750, 7.448405, -9.746355, 2.418214, -1.058247, 1.978506, 2.834848, -0.474751, 3.189390, -3.030795, 1.211649, -7.333964, -0.709869, 3.259298, ...]
------------------------------------------------------------
Node 41 (call 41) - model.encoder.layers.2 :: MoonshineEncoderLayer :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-90.951782 max=206.255768 mean=0.631912 std=14.625961
  Note: stats computed on first 1000 of 239904 values
  Preview: [8.205672, 0.348089, -4.513802, 13.445330, -20.487379, 1.840675, -2.053666, 1.179060, 3.203653, -0.647471, -1.080945, 0.794460, 5.361696, -16.456272, -1.961072, 2.921726, ...]
------------------------------------------------------------
Node 42 (call 42) - model.encoder.layers.3.input_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-2.343944 max=4.879797 mean=0.000056 std=0.456551
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.331531, -0.010306, -0.262149, 0.658674, -1.133126, 0.063993, -0.107216, 0.033557, 0.143944, -0.064663, -0.072602, 0.011940, 0.224872, -0.824325, -0.131296, 0.152175, ...]
------------------------------------------------------------
Node 43 (call 43) - model.encoder.layers.3.self_attn.q_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-9.523055 max=4.683912 mean=0.017555 std=1.524511
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.830168, 0.765891, 0.464835, 1.240577, 1.045187, 0.990878, -0.475832, 1.179418, 1.299566, -0.458752, 0.751645, -1.573988, -1.509513, -0.498103, 1.005309, -3.037591, ...]
------------------------------------------------------------
Node 44 (call 44) - model.encoder.layers.3.self_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-10.211807 max=9.491602 mean=0.023051 std=2.306301
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.458589, 0.476911, 2.127492, -0.957686, 1.608676, -0.549993, 1.139613, 0.903967, 0.241531, -2.331407, 0.271091, -2.075841, -3.661779, 0.798586, 0.254525, -2.257981, ...]
------------------------------------------------------------
Node 45 (call 45) - model.encoder.layers.3.self_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-5.262750 max=5.128078 mean=0.025442 std=1.612858
  Note: stats computed on first 1000 of 239904 values
  Preview: [-1.775319, -0.053935, -0.312248, 0.373670, 0.531402, -0.887631, 0.084170, -0.009564, -0.220681, -1.706302, -0.994907, 2.594962, -1.694782, 0.661007, 3.187026, -0.467381, ...]
------------------------------------------------------------
Node 46 (call 46) - model.encoder.layers.3.self_attn.o_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-23.665062 max=12.255764 mean=0.007675 std=4.271673
  Note: stats computed on first 1000 of 239904 values
  Preview: [-2.076327, 3.059214, 5.901386, 10.286231, -0.808328, -1.626117, -0.767660, 1.553918, -0.720972, -2.396868, 2.196794, -1.051112, 2.028029, -2.880391, 3.946676, 2.314526, ...]
------------------------------------------------------------
Node 47 (call 47) - model.encoder.layers.3.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-23.665062 max=12.255764 mean=0.007675 std=4.271673
  Note: stats computed on first 1000 of 239904 values
  Preview: [-2.076327, 3.059214, 5.901386, 10.286231, -0.808328, -1.626117, -0.767660, 1.553918, -0.720972, -2.396868, 2.196794, -1.051112, 2.028029, -2.880391, 3.946676, 2.314526, ...]
------------------------------------------------------------
Node 48 (call 48) - model.encoder.layers.3.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-4.797860 max=6.422097 mean=-0.008583 std=0.657913
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.254638, 0.127936, 0.039140, 1.088163, -1.038629, -0.017188, -0.150793, 0.112100, 0.100434, -0.183112, 0.024140, -0.038890, 0.316923, -0.971235, 0.083390, 0.276100, ...]
------------------------------------------------------------
Node 49 (call 49) - model.encoder.layers.3.mlp.fc1 :: Linear :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-7.017831 max=5.583987 mean=-1.036841 std=1.264622
  Note: stats computed on first 1000 of 959616 values
  Preview: [-1.382795, -3.714824, -1.049452, 0.205276, -1.823157, -1.641595, -2.190908, -1.726215, -1.558939, 0.533696, -1.431949, -1.263709, -1.618604, -1.132487, -1.516414, -2.336562, ...]
------------------------------------------------------------
Node 50 (call 50) - model.encoder.layers.3.mlp.activation_fn :: GELUActivation :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-0.169970 max=5.583987 mean=0.031784 std=0.401888
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.115275, -0.000378, -0.154254, 0.119331, -0.062242, -0.082633, -0.031175, -0.072767, -0.092765, 0.375308, -0.108941, -0.130373, -0.085408, -0.145768, -0.098123, -0.022737, ...]
------------------------------------------------------------
Node 51 (call 51) - model.encoder.layers.3.mlp.fc2 :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-32.414585 max=26.871717 mean=0.011417 std=5.776135
  Note: stats computed on first 1000 of 239904 values
  Preview: [-3.401307, -3.311428, -2.997042, -0.206886, -3.718237, 2.770243, -8.226439, 1.634117, -0.652353, 1.162661, 4.779648, 5.934916, -3.332844, -2.588309, -5.319272, 6.652748, ...]
------------------------------------------------------------
Node 52 (call 52) - model.encoder.layers.3.mlp :: MoonshineEncoderMLP :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-32.414585 max=26.871717 mean=0.011417 std=5.776135
  Note: stats computed on first 1000 of 239904 values
  Preview: [-3.401307, -3.311428, -2.997042, -0.206886, -3.718237, 2.770243, -8.226439, 1.634117, -0.652353, 1.162661, 4.779648, 5.934916, -3.332844, -2.588309, -5.319272, 6.652748, ...]
------------------------------------------------------------
Node 53 (call 53) - model.encoder.layers.3 :: MoonshineEncoderLayer :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-119.795975 max=237.744263 mean=0.651004 std=19.008587
  Note: stats computed on first 1000 of 239904 values
  Preview: [2.728039, 0.095875, -1.609458, 23.524673, -25.013945, 2.984801, -11.047765, 4.367095, 1.830329, -1.881678, 5.895497, 5.678264, 4.056880, -21.924973, -3.333668, 11.889000, ...]
------------------------------------------------------------
Node 54 (call 54) - model.encoder.layers.4.input_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.007016 max=7.435551 mean=0.006715 std=0.682520
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.103364, -0.018608, -0.100431, 1.122695, -1.182150, 0.115960, -0.488697, 0.201378, 0.068968, -0.133173, 0.180497, 0.245230, 0.180494, -1.063498, -0.176842, 0.632001, ...]
------------------------------------------------------------
Node 55 (call 55) - model.encoder.layers.4.self_attn.q_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-5.522124 max=5.865768 mean=0.066717 std=1.569184
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.105498, 0.294954, -1.336909, 0.390757, -0.090194, 0.764792, 0.160621, 0.532733, 0.037672, -0.035919, 0.234716, 0.520776, -0.949365, 0.174210, 0.580676, -0.523978, ...]
------------------------------------------------------------
Node 56 (call 56) - model.encoder.layers.4.self_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-8.938631 max=9.530460 mean=-0.145951 std=2.650853
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.186656, 0.058267, 1.114900, -1.841656, -1.344701, -4.011315, -1.873646, -0.915020, 3.037019, 1.149236, 0.342515, -1.844830, -0.672452, -2.821979, 2.348047, 0.968721, ...]
------------------------------------------------------------
Node 57 (call 57) - model.encoder.layers.4.self_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-9.118508 max=6.948774 mean=-0.278485 std=2.750566
  Note: stats computed on first 1000 of 239904 values
  Preview: [-2.123553, 4.865614, -4.283841, -1.646595, -0.009944, 1.577939, 0.800836, -0.681778, -1.536667, -0.798184, 1.998346, 0.875323, 0.032694, 2.815025, -3.170079, 1.245813, ...]
------------------------------------------------------------
Node 58 (call 58) - model.encoder.layers.4.self_attn.o_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-45.707535 max=29.098980 mean=-0.258181 std=7.398413
  Note: stats computed on first 1000 of 239904 values
  Preview: [5.876953, -4.199611, 8.203736, -2.175681, -11.479305, 9.470024, 2.749527, 4.194856, 5.137972, 7.274520, -12.992732, -7.020968, -0.633316, 6.697636, 8.191266, -1.290912, ...]
------------------------------------------------------------
Node 59 (call 59) - model.encoder.layers.4.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-45.707535 max=29.098980 mean=-0.258181 std=7.398413
  Note: stats computed on first 1000 of 239904 values
  Preview: [5.876953, -4.199611, 8.203736, -2.175681, -11.479305, 9.470024, 2.749527, 4.194856, 5.137972, 7.274520, -12.992732, -7.020968, -0.633316, 6.697636, 8.191266, -1.290912, ...]
------------------------------------------------------------
Node 60 (call 60) - model.encoder.layers.4.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-5.256736 max=7.634163 mean=0.004924 std=0.762765
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.288274, -0.155419, 0.244434, 0.826160, -1.380841, 0.485091, -0.318254, 0.338663, 0.225958, 0.200603, -0.235963, -0.061121, 0.121667, -0.556283, 0.220201, 0.420960, ...]
------------------------------------------------------------
Node 61 (call 61) - model.encoder.layers.4.mlp.fc1 :: Linear :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-11.287509 max=3.344824 mean=-1.423806 std=1.708199
  Note: stats computed on first 1000 of 959616 values
  Preview: [-4.088411, 2.010000, -0.345452, 0.577484, -0.574008, -0.601519, -0.039539, -1.283215, 0.304234, -3.405406, -2.468292, -1.308786, -0.582268, -1.548230, -1.800307, -2.306837, ...]
------------------------------------------------------------
Node 62 (call 62) - model.encoder.layers.4.mlp.activation_fn :: GELUActivation :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-0.169971 max=3.343447 mean=0.054892 std=0.390949
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.000089, 1.965347, -0.126048, 0.414745, -0.162433, -0.164664, -0.019146, -0.127947, 0.188480, -0.001125, -0.016755, -0.124732, -0.163147, -0.094107, -0.064642, -0.024295, ...]
------------------------------------------------------------
Node 63 (call 63) - model.encoder.layers.4.mlp.fc2 :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-23.189440 max=14.968502 mean=-0.013309 std=4.653527
  Note: stats computed on first 1000 of 239904 values
  Preview: [-1.309709, 0.207580, -3.197252, -4.504705, -4.829877, 1.030710, 1.324225, -4.146436, -2.955070, 1.393756, -0.129812, 3.882285, -2.924155, 3.268807, -4.779128, -3.683291, ...]
------------------------------------------------------------
Node 64 (call 64) - model.encoder.layers.4.mlp :: MoonshineEncoderMLP :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-23.189440 max=14.968502 mean=-0.013309 std=4.653527
  Note: stats computed on first 1000 of 239904 values
  Preview: [-1.309709, 0.207580, -3.197252, -4.504705, -4.829877, 1.030710, 1.324225, -4.146436, -2.955070, 1.393756, -0.129812, 3.882285, -2.924155, 3.268807, -4.779128, -3.683291, ...]
------------------------------------------------------------
Node 65 (call 65) - model.encoder.layers.4 :: MoonshineEncoderLayer :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-177.130875 max=279.572144 mean=0.379514 std=25.222710
  Note: stats computed on first 1000 of 239904 values
  Preview: [7.295283, -3.896157, 3.397026, 16.844286, -41.323124, 13.485535, -6.974013, 4.415515, 4.013230, 6.786599, -7.227046, 2.539580, 0.499409, -11.958530, 0.078471, 6.914798, ...]
------------------------------------------------------------
Node 66 (call 66) - model.encoder.layers.5.input_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.769085 max=7.161864 mean=0.023188 std=0.724021
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.242005, -0.147170, 0.148414, 0.624025, -1.550419, 0.501365, -0.308524, 0.169064, 0.137925, 0.310454, -0.243352, 0.118461, 0.007630, -0.452293, -0.007622, 0.271978, ...]
------------------------------------------------------------
Node 67 (call 67) - model.encoder.layers.5.self_attn.q_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-5.066799 max=5.283534 mean=0.076685 std=1.390642
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.178660, 0.177492, -0.667453, -0.871811, -0.476052, 0.107569, 0.291399, -0.035171, -0.508728, -0.594987, 0.214654, 0.320798, 0.011164, 0.171441, -0.465546, 0.308201, ...]
------------------------------------------------------------
Node 68 (call 68) - model.encoder.layers.5.self_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-6.694927 max=8.856272 mean=0.314051 std=2.277671
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.249670, -0.072013, 0.204000, 0.453570, -0.377066, 0.179185, 0.034870, -0.184264, -0.168079, 0.343649, 0.113368, -0.315794, -0.108333, 1.302778, 1.960501, -0.078163, ...]
------------------------------------------------------------
Node 69 (call 69) - model.encoder.layers.5.self_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-15.035705 max=11.350104 mean=0.129373 std=4.041001
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.594195, -2.983452, -4.508447, -5.546756, 7.023084, -8.561199, -0.465321, -1.384868, 8.551833, -2.298260, 4.096010, -4.961094, 2.212938, 3.161082, -3.801377, -1.271361, ...]
------------------------------------------------------------
Node 70 (call 70) - model.encoder.layers.5.self_attn.o_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-128.723373 max=120.134712 mean=-0.256332 std=20.303566
  Note: stats computed on first 1000 of 239904 values
  Preview: [2.645359, 1.738449, 2.209540, 5.601521, -49.412209, 9.081795, 5.755299, -4.931903, -7.232399, 14.271057, 14.028616, -6.261621, -0.461735, -4.324127, -0.168735, -1.049284, ...]
------------------------------------------------------------
Node 71 (call 71) - model.encoder.layers.5.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-128.723373 max=120.134712 mean=-0.256332 std=20.303566
  Note: stats computed on first 1000 of 239904 values
  Preview: [2.645359, 1.738449, 2.209540, 5.601521, -49.412209, 9.081795, 5.755299, -4.931903, -7.232399, 14.271057, 14.028616, -6.261621, -0.461735, -4.324127, -0.168735, -1.049284, ...]
------------------------------------------------------------
Node 72 (call 72) - model.encoder.layers.5.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-5.235021 max=4.305076 mean=0.026847 std=0.811538
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.313143, -0.066178, 0.208237, 0.752950, -2.643876, 0.783405, -0.041549, -0.020031, -0.105822, 0.765220, 0.214835, -0.128291, 0.002063, -0.564359, -0.002992, 0.204582, ...]
------------------------------------------------------------
Node 73 (call 73) - model.encoder.layers.5.mlp.fc1 :: Linear :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-27.218401 max=11.109096 mean=-1.910452 std=3.195572
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.982387, 1.413789, -2.844930, -0.741806, -0.273013, -2.249527, 1.485076, -5.252395, -2.206298, -1.545585, -1.711793, -1.273115, -5.857591, -0.399104, -1.685337, -0.342554, ...]
------------------------------------------------------------
Node 74 (call 74) - model.encoder.layers.5.mlp.activation_fn :: GELUActivation :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-0.169967 max=11.109096 mean=0.319678 std=1.133814
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.160085, 1.302507, -0.006319, -0.169950, -0.107136, -0.027533, 1.382960, -0.000000, -0.030186, -0.094439, -0.074407, -0.129207, -0.000000, -0.137654, -0.077461, -0.125363, ...]
------------------------------------------------------------
Node 75 (call 75) - model.encoder.layers.5.mlp.fc2 :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-307.010681 max=306.720215 mean=-0.105168 std=41.413677
  Note: stats computed on first 1000 of 239904 values
  Preview: [5.581933, -14.522901, 6.911893, -8.338845, -86.763031, 8.888703, -0.783083, 1.933500, -1.273424, 6.843052, -8.098682, -2.962870, 10.893191, 1.939475, 7.863358, -2.376709, ...]
------------------------------------------------------------
Node 76 (call 76) - model.encoder.layers.5.mlp :: MoonshineEncoderMLP :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-307.010681 max=306.720215 mean=-0.105168 std=41.413677
  Note: stats computed on first 1000 of 239904 values
  Preview: [5.581933, -14.522901, 6.911893, -8.338845, -86.763031, 8.888703, -0.783083, 1.933500, -1.273424, 6.843052, -8.098682, -2.962870, 10.893191, 1.939475, 7.863358, -2.376709, ...]
------------------------------------------------------------
Node 77 (call 77) - model.encoder.layers.5 :: MoonshineEncoderLayer :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-561.035461 max=477.929840 mean=0.018014 std=70.777222
  Note: stats computed on first 1000 of 239904 values
  Preview: [15.522575, -16.680609, 12.518459, 14.106961, -177.498367, 31.456032, -2.001798, 1.417112, -4.492592, 27.900707, -1.297113, -6.684912, 10.930865, -14.343183, 7.773093, 3.488805, ...]
------------------------------------------------------------
Node 78 (call 78) - model.encoder.layer_norm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.701852 max=4.225048 mean=0.029268 std=0.487575
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.180410, -0.194982, 0.163126, 0.162613, -1.183519, 0.366340, -0.022310, 0.021833, -0.057942, 0.387811, -0.012668, -0.082833, 0.152838, -0.176949, 0.085385, 0.049317, ...]
------------------------------------------------------------
Node 79 (call 79) - model.encoder :: MoonshineEncoder :: output.last_hidden_state
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.701852 max=4.225048 mean=0.029268 std=0.487575
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.180410, -0.194982, 0.163126, 0.162613, -1.183519, 0.366340, -0.022310, 0.021833, -0.057942, 0.387811, -0.012668, -0.082833, 0.152838, -0.176949, 0.085385, 0.049317, ...]
------------------------------------------------------------
Node 80 (call 79) - model.encoder :: MoonshineEncoder :: output.hidden_states[0]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-0.169931 max=17.944090 mean=0.736144 std=1.061540
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.249802, 1.098746, 0.815923, 0.549584, 0.711983, 0.537123, 0.040814, 0.941720, 0.152236, 1.142886, 0.348016, 1.096730, 0.880705, 1.114160, -0.169931, 0.751759, ...]
------------------------------------------------------------
Node 81 (call 79) - model.encoder :: MoonshineEncoder :: output.hidden_states[1]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-17.224667 max=44.700821 mean=0.672309 std=3.330945
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.919788, 0.044346, -0.182903, 0.294990, -0.409678, -1.187226, 0.026535, -1.403606, 3.172574, -0.779908, -1.073637, 0.003678, 0.151032, -0.381488, 0.967268, -0.453867, ...]
------------------------------------------------------------
Node 82 (call 79) - model.encoder :: MoonshineEncoder :: output.hidden_states[2]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-52.424728 max=121.588913 mean=0.623508 std=8.783118
  Note: stats computed on first 1000 of 239904 values
  Preview: [3.954081, 0.175852, -4.568874, 6.564672, -9.646872, -2.427884, 0.165133, -4.236034, -0.675837, 1.266756, -1.268249, 3.563319, 5.568339, -7.807684, 0.259345, -1.616693, ...]
------------------------------------------------------------
Node 83 (call 79) - model.encoder :: MoonshineEncoder :: output.hidden_states[3]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-90.951782 max=206.255768 mean=0.631912 std=14.625961
  Note: stats computed on first 1000 of 239904 values
  Preview: [8.205672, 0.348089, -4.513802, 13.445330, -20.487379, 1.840675, -2.053666, 1.179060, 3.203653, -0.647471, -1.080945, 0.794460, 5.361696, -16.456272, -1.961072, 2.921726, ...]
------------------------------------------------------------
Node 84 (call 79) - model.encoder :: MoonshineEncoder :: output.hidden_states[4]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-119.795975 max=237.744263 mean=0.651004 std=19.008587
  Note: stats computed on first 1000 of 239904 values
  Preview: [2.728039, 0.095875, -1.609458, 23.524673, -25.013945, 2.984801, -11.047765, 4.367095, 1.830329, -1.881678, 5.895497, 5.678264, 4.056880, -21.924973, -3.333668, 11.889000, ...]
------------------------------------------------------------
Node 85 (call 79) - model.encoder :: MoonshineEncoder :: output.hidden_states[5]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-177.130875 max=279.572144 mean=0.379514 std=25.222710
  Note: stats computed on first 1000 of 239904 values
  Preview: [7.295283, -3.896157, 3.397026, 16.844286, -41.323124, 13.485535, -6.974013, 4.415515, 4.013230, 6.786599, -7.227046, 2.539580, 0.499409, -11.958530, 0.078471, 6.914798, ...]
------------------------------------------------------------
Node 86 (call 79) - model.encoder :: MoonshineEncoder :: output.hidden_states[6]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.701852 max=4.225048 mean=0.029268 std=0.487575
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.180410, -0.194982, 0.163126, 0.162613, -1.183519, 0.366340, -0.022310, 0.021833, -0.057942, 0.387811, -0.012668, -0.082833, 0.152838, -0.176949, 0.085385, 0.049317, ...]
------------------------------------------------------------
Node 87 (call 80) - model.decoder.embed_tokens :: Embedding :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-1.484618 max=0.408362 mean=0.000947 std=0.120420
  Preview: [0.004872, 0.019829, 0.064762, 0.019214, 0.024987, 0.024814, 0.000693, 0.011946, 0.028443, 0.014216, 0.005537, -0.566699, 0.000872, 0.000878, 0.007687, 0.006083, ...]
------------------------------------------------------------
Node 88 (call 81) - model.decoder.rotary_emb :: MoonshineRotaryEmbedding :: output[0]
  Shape: [1,1,32]  Precision: FLOAT32
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, ...]
------------------------------------------------------------
Node 89 (call 81) - model.decoder.rotary_emb :: MoonshineRotaryEmbedding :: output[1]
  Shape: [1,1,32]  Precision: FLOAT32
  Stats: min=0.000000 max=0.000000 mean=0.000000 std=0.000000
  Preview: [0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, ...]
------------------------------------------------------------
Node 90 (call 82) - model.decoder.layers.0.input_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.135571 max=1.396079 mean=0.024525 std=0.321397
  Preview: [0.019871, 0.078903, 0.228516, 0.078273, 0.091792, 0.099843, -0.001252, 0.050807, 0.109827, 0.069799, 0.024213, -1.705974, -0.000343, -0.000376, 0.034958, 0.034797, ...]
------------------------------------------------------------
Node 91 (call 83) - model.decoder.layers.0.self_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.069655 max=3.322939 mean=0.024217 std=1.216284
  Preview: [-0.443647, 1.872825, 1.289198, 2.081692, 0.737177, 1.023980, -0.963331, 0.127345, 0.081408, -0.211538, 1.060192, -2.348042, 1.960960, 0.454271, -1.346165, 0.164287, ...]
------------------------------------------------------------
Node 92 (call 84) - model.decoder.layers.0.self_attn.k_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-5.873570 max=6.579009 mean=-0.105709 std=1.466108
  Preview: [-0.043757, 0.038889, -0.036634, 0.041352, -0.006354, -0.011296, 0.009928, -0.049535, 0.015337, 0.019158, -0.043990, 0.012071, 0.009845, 0.111604, 0.035283, 0.193531, ...]
------------------------------------------------------------
Node 93 (call 85) - model.decoder.layers.0.self_attn.v_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-0.075558 max=0.048674 mean=0.000320 std=0.015943
  Preview: [0.006452, -0.021475, 0.012806, -0.000457, -0.010250, -0.023445, 0.005077, 0.039248, 0.030381, 0.006966, 0.012036, -0.017269, -0.005237, -0.025094, -0.003607, -0.006911, ...]
------------------------------------------------------------
Node 94 (call 86) - model.decoder.layers.0.self_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-0.337900 max=0.117294 mean=0.000001 std=0.025483
  Preview: [0.000678, 0.016308, 0.016810, 0.004502, -0.015827, -0.011354, 0.015615, -0.010402, 0.010377, 0.011891, -0.002410, 0.017582, 0.015272, 0.016242, -0.015279, 0.002499, ...]
------------------------------------------------------------
Node 95 (call 87) - model.decoder.layers.0.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-0.337900 max=0.117294 mean=0.000001 std=0.025483
  Preview: [0.000678, 0.016308, 0.016810, 0.004502, -0.015827, -0.011354, 0.015615, -0.010402, 0.010377, 0.011891, -0.002410, 0.017582, 0.015272, 0.016242, -0.015279, 0.002499, ...]
------------------------------------------------------------
Node 96 (call 88) - model.decoder.layers.0.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-8.206243 max=2.642032 mean=0.003074 std=0.692873
  Preview: [0.025690, 0.199412, 0.409525, 0.147107, 0.049957, 0.058226, 0.115505, 0.003083, 0.223133, 0.164730, 0.014542, -2.750203, 0.087198, 0.116001, -0.051852, 0.052147, ...]
------------------------------------------------------------
Node 97 (call 89) - model.decoder.layers.0.encoder_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.387999 max=5.883209 mean=0.072343 std=1.066462
  Preview: [0.067056, 0.286449, 0.117082, -1.007199, 0.082359, -0.962201, 1.292394, -0.459260, 0.228615, -0.518751, -0.348880, -0.121645, 0.316516, 0.332093, 0.335595, -0.050116, ...]
------------------------------------------------------------
Node 98 (call 90) - model.decoder.layers.0.encoder_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.528200 max=2.479198 mean=0.114306 std=0.823154
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.206521, 0.266131, 0.167159, 0.916221, -0.828312, 0.148366, 0.072628, -0.815823, 0.545468, 1.776612, 1.439992, 0.281076, 0.890942, 0.050053, -0.157603, 0.429337, ...]
------------------------------------------------------------
Node 99 (call 91) - model.decoder.layers.0.encoder_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-1.061750 max=0.982517 mean=-0.035121 std=0.315888
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.133049, -0.275513, -0.257395, 0.124218, -0.353732, -0.328399, -0.352519, -0.237522, 0.108386, -0.411487, -0.334781, -0.706774, 0.058794, -0.231709, 0.103488, 0.044889, ...]
------------------------------------------------------------
Node 100 (call 92) - model.decoder.layers.0.encoder_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-5.353359 max=0.558868 mean=-0.017859 std=0.359291
  Preview: [0.165077, -0.017588, 0.069048, -0.056439, -0.013641, 0.029563, -0.002354, 0.034151, -0.044450, -0.069294, -0.003067, 0.359349, 0.133998, -0.090455, 0.047840, 0.387473, ...]
------------------------------------------------------------
Node 101 (call 93) - model.decoder.layers.0.encoder_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-5.353359 max=0.558868 mean=-0.017859 std=0.359291
  Preview: [0.165077, -0.017588, 0.069048, -0.056439, -0.013641, 0.029563, -0.002354, 0.034151, -0.044450, -0.069294, -0.003067, 0.359349, 0.133998, -0.090455, 0.047840, 0.387473, ...]
------------------------------------------------------------
Node 102 (call 94) - model.decoder.layers.0.final_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-1.444525 max=0.919843 mean=0.041494 std=0.305972
  Preview: [0.347193, 0.065296, 0.321855, -0.033064, 0.026638, 0.122698, 0.076369, 0.113490, 0.023927, -0.054859, 0.038320, -0.362573, 0.319574, -0.119465, 0.118151, 0.734737, ...]
------------------------------------------------------------
Node 103 (call 95) - model.decoder.layers.0.mlp.fc1 :: Linear :: output
  Shape: [1,1,2304]  Precision: FLOAT32
  Stats: min=-2.899223 max=2.176899 mean=0.006080 std=0.428656
  Note: stats computed on first 1000 of 2304 values
  Preview: [-0.010170, -0.238096, 0.164056, -0.958124, -0.782203, -0.791765, 0.032023, 0.570391, 0.245254, 0.004291, -0.128101, -0.011668, 0.170356, -0.006207, 1.129198, -0.423597, ...]
------------------------------------------------------------
Node 104 (call 96) - model.decoder.layers.0.mlp.activation_fn :: SiLUActivation :: output
  Shape: [1,1,1152]  Precision: FLOAT32
  Stats: min=-0.278465 max=1.626801 mean=-0.075840 std=0.198230
  Note: stats computed on first 1000 of 1152 values
  Preview: [-0.192864, -0.244424, -0.262797, 0.985598, -0.114454, -0.060120, -0.133686, -0.251144, -0.255016, -0.169177, -0.008366, -0.060017, -0.218041, -0.054157, -0.009072, 0.005542, ...]
------------------------------------------------------------
Node 105 (call 97) - model.decoder.layers.0.mlp.fc2 :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-20.184191 max=4.710290 mean=0.007058 std=1.530692
  Preview: [-0.056994, 0.074795, 0.161036, 0.109839, -0.137685, 0.301642, -0.437355, -0.580330, -0.421765, 0.594305, 0.280164, 0.434589, -0.074090, -1.002674, 0.277331, 0.197858, ...]
------------------------------------------------------------
Node 106 (call 98) - model.decoder.layers.0.mlp :: MoonshineDecoderMLP :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-20.184191 max=4.710290 mean=0.007058 std=1.530692
  Preview: [-0.056994, 0.074795, 0.161036, 0.109839, -0.137685, 0.301642, -0.437355, -0.580330, -0.421765, 0.594305, 0.280164, 0.434589, -0.074090, -1.002674, 0.277331, 0.197858, ...]
------------------------------------------------------------
Node 107 (call 99) - model.decoder.layers.0 :: MoonshineDecoderLayer :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-25.851522 max=5.221372 mean=-0.009852 std=1.834135
  Preview: [0.113633, 0.093343, 0.311656, 0.077115, -0.142165, 0.344664, -0.423401, -0.544635, -0.427395, 0.551118, 0.280224, 0.244821, 0.076052, -1.076009, 0.317580, 0.593914, ...]
------------------------------------------------------------
Node 108 (call 100) - model.decoder.layers.1.input_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-7.032995 max=1.295371 mean=0.000237 std=0.492268
  Preview: [0.035346, 0.026519, 0.079518, 0.025878, -0.034581, 0.100624, -0.120690, -0.154188, -0.113971, 0.158354, 0.083055, 0.077516, 0.029858, -0.254566, 0.123011, 0.164051, ...]
------------------------------------------------------------
Node 109 (call 101) - model.decoder.layers.1.self_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-2.148512 max=3.414831 mean=-0.029639 std=0.807213
  Preview: [-0.904260, -0.310015, -0.758445, 0.321094, -1.272700, -0.693998, -0.451742, -1.072504, 1.337942, -1.161715, -0.027996, 0.671672, -0.354712, 0.885095, 0.620142, 2.438873, ...]
------------------------------------------------------------
Node 110 (call 102) - model.decoder.layers.1.self_attn.k_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-9.111242 max=5.110323 mean=-0.325340 std=1.966549
  Preview: [0.178559, 0.117483, 0.156782, -0.208028, 0.344774, -0.037514, 0.283848, 0.075085, 0.408865, -0.116540, -0.496408, 0.668134, -0.145575, 0.427238, 1.965079, 2.698233, ...]
------------------------------------------------------------
Node 111 (call 103) - model.decoder.layers.1.self_attn.v_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-0.563659 max=0.623929 mean=0.009211 std=0.204625
  Preview: [0.286587, 0.069833, 0.330545, -0.091907, -0.004338, -0.068563, -0.013800, 0.140681, 0.076166, -0.098783, -0.225345, -0.087078, 0.014661, 0.178306, -0.094420, -0.374652, ...]
------------------------------------------------------------
Node 112 (call 104) - model.decoder.layers.1.self_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.679848 max=4.374696 mean=0.003646 std=0.539067
  Preview: [-0.286326, -0.011743, -0.279818, -0.217468, 0.463531, 0.523434, -0.667961, -0.106626, -0.008733, 0.357294, 0.010171, 0.924580, -0.431694, -0.763727, -0.518196, 0.230887, ...]
------------------------------------------------------------
Node 113 (call 105) - model.decoder.layers.1.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.679848 max=4.374696 mean=0.003646 std=0.539067
  Preview: [-0.286326, -0.011743, -0.279818, -0.217468, 0.463531, 0.523434, -0.667961, -0.106626, -0.008733, 0.357294, 0.010171, 0.924580, -0.431694, -0.763727, -0.518196, 0.230887, ...]
------------------------------------------------------------
Node 114 (call 106) - model.decoder.layers.1.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-10.938636 max=1.295633 mean=-0.002217 std=0.740881
  Preview: [-0.066199, 0.031164, 0.013563, -0.053302, 0.131673, 0.397320, -0.332496, -0.269529, -0.176862, 0.345871, 0.093745, 0.379126, -0.145961, -0.690237, -0.073308, 0.316710, ...]
------------------------------------------------------------
Node 115 (call 107) - model.decoder.layers.1.encoder_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.147662 max=4.296786 mean=-0.033339 std=1.002324
  Preview: [-3.002295, -0.445714, 1.465219, 0.235273, -1.774537, 0.101323, 0.682345, -0.308061, 0.151951, -0.164315, -1.509928, -1.443759, -0.837927, -0.873782, 1.142530, -1.597016, ...]
------------------------------------------------------------
Node 116 (call 108) - model.decoder.layers.1.encoder_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.021557 max=5.469246 mean=-0.003791 std=1.154418
  Note: stats computed on first 1000 of 239904 values
  Preview: [-1.046386, -0.370423, -2.346113, -1.197732, -1.103188, -0.706582, 2.656167, 0.642980, 1.598650, -0.885252, -0.494035, -0.774049, 0.298032, -0.286338, 0.650856, 0.570009, ...]
------------------------------------------------------------
Node 117 (call 109) - model.decoder.layers.1.encoder_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-2.267687 max=2.358465 mean=-0.035957 std=0.552206
  Note: stats computed on first 1000 of 239904 values
  Preview: [-1.513605, -0.122522, 0.863317, 0.015237, -0.316334, -0.294853, 0.104540, 0.333238, -0.130077, 0.679311, -0.670638, -0.072840, -0.202890, 0.145624, 0.367397, -0.215231, ...]
------------------------------------------------------------
Node 118 (call 110) - model.decoder.layers.1.encoder_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-2.140795 max=3.139105 mean=0.006293 std=0.462723
  Preview: [-0.193217, 0.131660, -0.622888, 0.102773, 0.236643, 0.068307, 0.147787, 0.245647, 0.341031, -0.694541, -0.368856, -0.107888, 0.154422, 0.332325, -0.240701, 0.248275, ...]
------------------------------------------------------------
Node 119 (call 111) - model.decoder.layers.1.encoder_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-2.140795 max=3.139105 mean=0.006293 std=0.462723
  Preview: [-0.193217, 0.131660, -0.622888, 0.102773, 0.236643, 0.068307, 0.147787, 0.245647, 0.341031, -0.694541, -0.368856, -0.107888, 0.154422, 0.332325, -0.240701, 0.248275, ...]
------------------------------------------------------------
Node 120 (call 112) - model.decoder.layers.1.final_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-2.203865 max=0.905368 mean=0.015600 std=0.257770
  Preview: [-0.080038, 0.044188, -0.134529, -0.008894, 0.134246, 0.192524, -0.189581, -0.096029, -0.022455, 0.046318, -0.018096, 0.246729, -0.045450, -0.313857, -0.108641, 0.208560, ...]
------------------------------------------------------------
Node 121 (call 113) - model.decoder.layers.1.mlp.fc1 :: Linear :: output
  Shape: [1,1,2304]  Precision: FLOAT32
  Stats: min=-4.806587 max=4.947193 mean=-0.010388 std=0.535289
  Note: stats computed on first 1000 of 2304 values
  Preview: [-0.031237, 0.095411, -0.408471, -0.268277, -0.004781, -0.130023, -0.034624, -0.146114, 0.486365, -0.214485, -0.296680, 0.223480, -0.119760, -2.816988, -0.241708, 0.322906, ...]
------------------------------------------------------------
Node 122 (call 114) - model.decoder.layers.1.mlp.activation_fn :: SiLUActivation :: output
  Shape: [1,1,1152]  Precision: FLOAT32
  Stats: min=-0.278459 max=5.835696 mean=0.036906 std=0.465289
  Note: stats computed on first 1000 of 1152 values
  Preview: [-0.030103, -0.218079, 0.566861, -0.158233, 0.039102, -0.150232, -0.267001, -0.270601, 0.098212, 0.007008, -0.185211, -0.060892, -0.086725, 0.246397, -0.211959, -0.005367, ...]
------------------------------------------------------------
Node 123 (call 115) - model.decoder.layers.1.mlp.fc2 :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-217.585495 max=30.053619 mean=-0.123963 std=13.869914
  Preview: [-2.671905, 1.278706, 0.770418, -0.145161, 1.785085, 0.067942, 2.067506, 0.886314, 0.318614, 1.314206, 1.799968, 1.890006, -0.425233, -9.268069, -0.142840, -2.029645, ...]
------------------------------------------------------------
Node 124 (call 116) - model.decoder.layers.1.mlp :: MoonshineDecoderMLP :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-217.585495 max=30.053619 mean=-0.123963 std=13.869914
  Preview: [-2.671905, 1.278706, 0.770418, -0.145161, 1.785085, 0.067942, 2.067506, 0.886314, 0.318614, 1.314206, 1.799968, 1.890006, -0.425233, -9.268069, -0.142840, -2.029645, ...]
------------------------------------------------------------
Node 125 (call 117) - model.decoder.layers.1 :: MoonshineDecoderLayer :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-249.257660 max=32.474289 mean=-0.123876 std=15.904207
  Preview: [-3.037814, 1.491966, 0.179368, -0.182741, 2.343094, 1.004347, 1.123932, 0.480699, 0.223517, 1.528077, 1.721508, 2.951519, -0.626453, -10.775481, -0.584157, -0.956570, ...]
------------------------------------------------------------
Node 126 (call 118) - model.decoder.layers.2.input_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-7.759813 max=1.170620 mean=0.008518 std=0.501538
  Preview: [-0.123385, 0.052782, 0.011522, -0.002743, 0.107809, 0.045766, 0.042802, 0.022614, 0.013101, 0.068066, 0.077198, 0.125981, -0.021220, -0.417654, -0.022689, -0.029333, ...]
------------------------------------------------------------
Node 127 (call 119) - model.decoder.layers.2.self_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-2.298456 max=2.926952 mean=0.025459 std=0.771886
  Preview: [0.561999, -0.736484, 0.185018, 0.283774, -0.189456, -0.135011, -0.175066, 1.740969, 0.365530, 0.604851, 0.461951, -0.291492, -0.641393, 0.444908, 0.672813, 0.621339, ...]
------------------------------------------------------------
Node 128 (call 120) - model.decoder.layers.2.self_attn.k_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.348945 max=11.039694 mean=0.103170 std=1.589160
  Preview: [0.011388, -0.075377, -0.086567, 0.002344, 0.084173, -0.023727, -0.490957, 0.128784, -0.245802, -0.077947, -0.146964, -0.908258, -0.215138, 2.796980, 1.832982, 0.277181, ...]
------------------------------------------------------------
Node 129 (call 121) - model.decoder.layers.2.self_attn.v_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-0.500043 max=0.604048 mean=-0.006433 std=0.130253
  Preview: [-0.043151, 0.002673, -0.022655, -0.103507, -0.087849, -0.233759, -0.129728, -0.166159, -0.067645, -0.003145, -0.176717, -0.085801, 0.013417, 0.090355, -0.013600, -0.292510, ...]
------------------------------------------------------------
Node 130 (call 122) - model.decoder.layers.2.self_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.214204 max=2.554273 mean=0.002762 std=0.386840
  Preview: [0.059715, 0.312231, 0.068232, -0.173203, 0.167884, 0.521165, -0.513265, -0.446965, -0.197806, 0.665097, 0.080815, 0.552113, -0.167989, -0.269679, -0.517549, 0.023918, ...]
------------------------------------------------------------
Node 131 (call 123) - model.decoder.layers.2.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.214204 max=2.554273 mean=0.002762 std=0.386840
  Preview: [0.059715, 0.312231, 0.068232, -0.173203, 0.167884, 0.521165, -0.513265, -0.446965, -0.197806, 0.665097, 0.080815, 0.552113, -0.167989, -0.269679, -0.517549, 0.023918, ...]
------------------------------------------------------------
Node 132 (call 124) - model.decoder.layers.2.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-10.332359 max=1.650000 mean=0.014941 std=0.669224
  Preview: [-0.146282, 0.102540, 0.019008, -0.013549, 0.153777, 0.108965, 0.040662, 0.008975, 0.009547, 0.145365, 0.109469, 0.182346, -0.038401, -0.614946, -0.060615, -0.042410, ...]
------------------------------------------------------------
Node 133 (call 125) - model.decoder.layers.2.encoder_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.553596 max=2.312725 mean=0.049311 std=0.955857
  Preview: [0.240415, -0.673886, 0.443707, 0.159731, 1.152068, -1.284500, -1.261325, 2.312725, -0.081821, 1.822313, 0.672848, -1.041030, -0.030674, 1.493089, 1.504420, 0.485612, ...]
------------------------------------------------------------
Node 134 (call 126) - model.decoder.layers.2.encoder_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-9.387086 max=7.556969 mean=-0.098317 std=2.354851
  Note: stats computed on first 1000 of 239904 values
  Preview: [-9.317142, 2.186321, -0.275068, -1.380523, -1.150462, -1.964368, -1.590604, 3.186650, -0.359505, -0.496870, 5.980823, -2.530366, -4.593721, -4.632139, 2.636918, -3.641470, ...]
------------------------------------------------------------
Node 135 (call 127) - model.decoder.layers.2.encoder_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-1.434097 max=1.839710 mean=-0.036815 std=0.588418
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.522015, -0.223781, -0.034070, -0.015164, -0.461824, -0.593176, -0.676790, -0.009000, 0.672319, 0.250979, 0.167067, 0.404526, -0.057523, 0.550300, -0.828063, -0.415803, ...]
------------------------------------------------------------
Node 136 (call 128) - model.decoder.layers.2.encoder_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-7.421011 max=7.077032 mean=0.028139 std=0.944110
  Preview: [-0.559577, 0.013676, -0.204492, -0.938456, 0.762464, -0.722376, -0.871374, 0.388083, 0.103687, -0.715186, 0.040029, 0.047794, 0.403766, -2.334682, -1.029735, 1.194335, ...]
------------------------------------------------------------
Node 137 (call 129) - model.decoder.layers.2.encoder_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-7.421011 max=7.077032 mean=0.028139 std=0.944110
  Preview: [-0.559577, 0.013676, -0.204492, -0.938456, 0.762464, -0.722376, -0.871374, 0.388083, 0.103687, -0.715186, 0.040029, 0.047794, 0.403766, -2.334682, -1.029735, 1.194335, ...]
------------------------------------------------------------
Node 138 (call 130) - model.decoder.layers.2.final_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.272654 max=0.879834 mean=0.013625 std=0.253903
  Preview: [-0.100565, 0.054049, 0.003798, -0.036711, 0.103681, 0.021990, -0.004781, 0.016633, 0.006696, 0.044734, 0.051946, 0.104020, -0.009429, -0.365821, -0.060217, 0.007861, ...]
------------------------------------------------------------
Node 139 (call 131) - model.decoder.layers.2.mlp.fc1 :: Linear :: output
  Shape: [1,1,2304]  Precision: FLOAT32
  Stats: min=-1.952690 max=13.219485 mean=0.001962 std=0.559356
  Note: stats computed on first 1000 of 2304 values
  Preview: [0.031749, -0.330023, -0.021655, 0.298830, 0.858574, 0.163193, -1.042964, 0.020164, -1.198454, -0.307067, 0.028373, 0.099203, -0.001451, 0.040046, -0.033049, 0.115192, ...]
------------------------------------------------------------
Node 140 (call 132) - model.decoder.layers.2.mlp.activation_fn :: SiLUActivation :: output
  Shape: [1,1,1152]  Precision: FLOAT32
  Stats: min=-0.278464 max=18.790651 mean=0.024448 std=0.663148
  Note: stats computed on first 1000 of 1152 values
  Preview: [-0.273245, -0.103949, 0.042578, 0.054258, -0.000364, -0.231278, -0.035540, -0.134255, 0.006012, 0.152174, -0.071466, -0.090602, -0.085576, -0.148108, -0.083232, -0.239614, ...]
------------------------------------------------------------
Node 141 (call 133) - model.decoder.layers.2.mlp.fc2 :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-17.423664 max=3.021662 mean=-0.014516 std=1.146758
  Preview: [0.323557, 0.404833, 0.237105, 0.029939, 0.107874, 0.147732, 0.238466, -0.170760, 0.054089, 0.161514, 0.504013, 0.161345, -0.345062, -0.948027, 0.086673, 0.310129, ...]
------------------------------------------------------------
Node 142 (call 134) - model.decoder.layers.2.mlp :: MoonshineDecoderMLP :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-17.423664 max=3.021662 mean=-0.014516 std=1.146758
  Preview: [0.323557, 0.404833, 0.237105, 0.029939, 0.107874, 0.147732, 0.238466, -0.170760, 0.054089, 0.161514, 0.504013, 0.161345, -0.345062, -0.948027, 0.086673, 0.310129, ...]
------------------------------------------------------------
Node 143 (call 135) - model.decoder.layers.2 :: MoonshineDecoderLayer :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-277.316559 max=37.373386 mean=-0.107492 std=17.736361
  Preview: [-3.214118, 2.222707, 0.280212, -1.264462, 3.381316, 0.950868, -0.022242, 0.251057, 0.183486, 1.639503, 2.346364, 3.712770, -0.735738, -14.327869, -2.044768, 0.571812, ...]
------------------------------------------------------------
Node 144 (call 136) - model.decoder.layers.3.input_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-8.017371 max=1.435529 mean=0.018245 std=0.538382
  Preview: [-0.143292, 0.114387, 0.016515, -0.053400, 0.174043, 0.050940, 0.004044, 0.016428, 0.012553, 0.083046, 0.105777, 0.174849, -0.031097, -0.669338, -0.096395, 0.025929, ...]
------------------------------------------------------------
Node 145 (call 137) - model.decoder.layers.3.self_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-1.972654 max=2.425939 mean=0.006478 std=0.739659
  Preview: [-0.441415, 0.562748, -0.496210, 0.766138, -0.284037, -0.876917, -0.052918, -0.269095, 0.609555, 1.019484, 0.499807, -1.254153, 0.158507, -1.449490, 1.048883, 0.503575, ...]
------------------------------------------------------------
Node 146 (call 138) - model.decoder.layers.3.self_attn.k_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.910290 max=5.416154 mean=0.026355 std=1.182949
  Preview: [-0.066138, -0.093385, -0.015377, 0.036720, 0.198160, -0.164011, 0.075709, 0.049361, 0.184961, 0.051609, -0.115505, -0.016431, -0.041008, -0.029990, 0.238238, 0.209606, ...]
------------------------------------------------------------
Node 147 (call 139) - model.decoder.layers.3.self_attn.v_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-0.649085 max=0.697403 mean=0.011386 std=0.168416
  Preview: [0.082940, -0.102117, -0.155553, 0.009566, 0.027690, 0.134537, 0.069570, 0.020136, -0.107501, -0.148237, 0.180153, 0.168373, 0.042631, -0.005789, -0.005810, 0.016518, ...]
------------------------------------------------------------
Node 148 (call 140) - model.decoder.layers.3.self_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.091553 max=5.956141 mean=0.006886 std=0.603261
  Preview: [-0.343551, 0.404617, -0.779895, 0.484509, -0.203984, 0.428816, -0.483234, -0.434197, 0.279088, 0.878111, -0.189423, 1.054620, -0.124606, -0.557297, -0.775801, -0.118127, ...]
------------------------------------------------------------
Node 149 (call 141) - model.decoder.layers.3.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.091553 max=5.956141 mean=0.006886 std=0.603261
  Preview: [-0.343551, 0.404617, -0.779895, 0.484509, -0.203984, 0.428816, -0.483234, -0.434197, 0.279088, 0.878111, -0.189423, 1.054620, -0.124606, -0.557297, -0.775801, -0.118127, ...]
------------------------------------------------------------
Node 150 (call 142) - model.decoder.layers.3.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-10.900543 max=2.055443 mean=0.013766 std=0.712623
  Preview: [-0.177608, 0.128629, -0.021695, -0.036044, 0.193521, 0.089708, -0.023218, -0.004680, 0.033657, 0.146832, 0.127472, 0.278798, -0.039633, -0.721319, -0.152032, 0.024045, ...]
------------------------------------------------------------
Node 151 (call 143) - model.decoder.layers.3.encoder_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-2.859456 max=3.770737 mean=0.099510 std=1.060882
  Preview: [0.824479, -1.734340, 0.099485, 0.058943, 0.444305, -2.559816, 0.045941, -0.640852, 0.896189, 1.370367, -0.669902, -0.620677, -0.859973, 0.082899, -0.243045, -0.079895, ...]
------------------------------------------------------------
Node 152 (call 144) - model.decoder.layers.3.encoder_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-11.022845 max=11.591249 mean=-0.034846 std=4.112493
  Note: stats computed on first 1000 of 239904 values
  Preview: [7.844735, -9.885494, 2.370599, -3.154065, -0.912456, -7.910452, -4.926421, -3.861460, 2.529500, 6.092584, -4.952181, -10.954900, -1.399451, 2.927901, -5.628325, 0.881282, ...]
------------------------------------------------------------
Node 153 (call 145) - model.decoder.layers.3.encoder_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-2.227301 max=1.796620 mean=-0.039175 std=0.692212
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.628683, -0.103250, -0.492126, -0.187050, -0.561007, 1.337912, 0.051751, -0.225904, 0.598668, 0.013192, -0.485201, -0.507562, -0.987676, 0.601599, -0.778201, -0.793377, ...]
------------------------------------------------------------
Node 154 (call 146) - model.decoder.layers.3.encoder_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.062586 max=21.100067 mean=0.049012 std=1.686666
  Preview: [-0.692047, -0.858127, 0.670898, 0.051365, 1.851254, 0.099278, -0.638521, -0.383789, 1.616022, -0.266717, -1.135662, -0.635902, -1.077219, -0.154252, -0.728826, 2.218506, ...]
------------------------------------------------------------
Node 155 (call 147) - model.decoder.layers.3.encoder_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.062586 max=21.100067 mean=0.049012 std=1.686666
  Preview: [-0.692047, -0.858127, 0.670898, 0.051365, 1.851254, 0.099278, -0.638521, -0.383789, 1.616022, -0.266717, -1.135662, -0.635902, -1.077219, -0.154252, -0.728826, 2.218506, ...]
------------------------------------------------------------
Node 156 (call 148) - model.decoder.layers.3.final_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.328381 max=0.938708 mean=0.014132 std=0.275229
  Preview: [-0.128851, 0.049323, 0.006215, -0.017838, 0.147462, 0.038496, -0.030064, -0.015289, 0.058166, 0.065748, 0.029091, 0.110531, -0.049986, -0.447524, -0.090291, 0.066156, ...]
------------------------------------------------------------
Node 157 (call 149) - model.decoder.layers.3.mlp.fc1 :: Linear :: output
  Shape: [1,1,2304]  Precision: FLOAT32
  Stats: min=-5.751443 max=2.681311 mean=0.003745 std=0.627352
  Note: stats computed on first 1000 of 2304 values
  Preview: [0.163861, -0.000337, 0.119243, 0.178437, 0.589370, -0.435526, -0.581731, 0.024253, -0.207395, 1.017465, -0.088733, 0.187182, 0.768372, 0.293710, -1.112507, 1.503618, ...]
------------------------------------------------------------
Node 158 (call 150) - model.decoder.layers.3.mlp.activation_fn :: SiLUActivation :: output
  Shape: [1,1,1152]  Precision: FLOAT32
  Stats: min=-0.278463 max=5.534267 mean=0.082332 std=0.467162
  Note: stats computed on first 1000 of 1152 values
  Preview: [-0.201134, -0.044832, -0.203076, 0.132036, -0.071277, 0.403257, -0.021335, -0.096087, 0.023773, 0.027596, -0.139765, -0.251029, -0.012307, -0.215971, -0.024863, 0.028299, ...]
------------------------------------------------------------
Node 159 (call 151) - model.decoder.layers.3.mlp.fc2 :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-1.675661 max=1.758051 mean=0.002518 std=0.593029
  Preview: [0.737635, -0.076410, -0.065831, 0.094137, -0.271171, 0.489109, -0.054138, 0.009462, -0.439536, 0.251570, 1.126202, 0.344919, 0.276498, -0.867701, -0.274157, -0.184847, ...]
------------------------------------------------------------
Node 160 (call 152) - model.decoder.layers.3.mlp :: MoonshineDecoderMLP :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-1.675661 max=1.758051 mean=0.002518 std=0.593029
  Preview: [0.737635, -0.076410, -0.065831, 0.094137, -0.271171, 0.489109, -0.054138, 0.009462, -0.439536, 0.251570, 1.126202, 0.344919, 0.276498, -0.867701, -0.274157, -0.184847, ...]
------------------------------------------------------------
Node 161 (call 153) - model.decoder.layers.3 :: MoonshineDecoderLayer :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-285.072937 max=52.406719 mean=-0.049076 std=18.476643
  Preview: [-3.512081, 1.692787, 0.105383, -0.634450, 4.757415, 1.968071, -1.198135, -0.557468, 1.639060, 2.502467, 2.147481, 4.476406, -1.661065, -15.907120, -3.823553, 2.487344, ...]
------------------------------------------------------------
Node 162 (call 154) - model.decoder.layers.4.input_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-8.286160 max=1.657168 mean=0.012546 std=0.558981
  Preview: [-0.162959, 0.072550, 0.005632, -0.024889, 0.198535, 0.080397, -0.051155, -0.020552, 0.069337, 0.092362, 0.099561, 0.168842, -0.067113, -0.670515, -0.171236, 0.089512, ...]
------------------------------------------------------------
Node 163 (call 155) - model.decoder.layers.4.self_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-2.632122 max=3.069417 mean=0.010681 std=0.831216
  Preview: [0.150743, -0.723253, 0.385751, 0.961550, 0.050328, -0.837753, -0.057244, 0.531223, 0.375986, -0.273741, 0.269749, 0.512821, -0.051962, 0.324024, -1.710259, -0.304741, ...]
------------------------------------------------------------
Node 164 (call 156) - model.decoder.layers.4.self_attn.k_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.841141 max=3.991256 mean=-0.037380 std=1.148463
  Preview: [-0.113759, -0.019604, 0.046478, -0.048418, -0.047670, -0.036448, 0.186331, 0.033769, -0.026402, -0.235933, 0.066844, -0.036448, 0.209394, 0.048509, -0.120553, -0.250702, ...]
------------------------------------------------------------
Node 165 (call 157) - model.decoder.layers.4.self_attn.v_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-0.864231 max=0.618314 mean=-0.015630 std=0.210904
  Preview: [-0.031095, 0.079284, 0.253997, 0.239555, 0.151994, -0.059929, -0.117791, 0.121540, 0.240180, -0.269198, -0.028481, -0.227151, 0.079352, 0.221373, 0.375804, -0.001687, ...]
------------------------------------------------------------
Node 166 (call 158) - model.decoder.layers.4.self_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.330384 max=14.923236 mean=0.007634 std=1.127059
  Preview: [-0.295960, 0.724767, -1.033685, 0.897042, -0.676362, 1.091961, -0.807410, 0.106797, -0.146903, 0.989571, 0.105229, 0.560338, -0.049662, -0.487520, -0.974387, 0.192651, ...]
------------------------------------------------------------
Node 167 (call 159) - model.decoder.layers.4.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.330384 max=14.923236 mean=0.007634 std=1.127059
  Preview: [-0.295960, 0.724767, -1.033685, 0.897042, -0.676362, 1.091961, -0.807410, 0.106797, -0.146903, 0.989571, 0.105229, 0.560338, -0.049662, -0.487520, -0.974387, 0.192651, ...]
------------------------------------------------------------
Node 168 (call 160) - model.decoder.layers.4.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-9.752035 max=2.592777 mean=0.018390 std=0.677545
  Preview: [-0.167753, 0.115204, -0.048855, 0.016231, 0.227629, 0.170060, -0.100335, -0.018700, 0.082302, 0.205557, 0.122414, 0.263172, -0.081010, -0.740101, -0.237553, 0.133539, ...]
------------------------------------------------------------
Node 169 (call 161) - model.decoder.layers.4.encoder_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.376779 max=3.524984 mean=-0.013002 std=1.153686
  Preview: [-0.422400, -1.062721, -0.021804, -0.522471, -1.336259, -0.394302, -0.540243, -2.099150, -0.676911, -1.909371, -0.573553, -0.831364, -0.803563, -0.925022, -1.038800, -0.787564, ...]
------------------------------------------------------------
Node 170 (call 162) - model.decoder.layers.4.encoder_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-11.389569 max=12.581012 mean=-0.563348 std=4.797197
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.507047, -2.166147, 1.263911, -8.276999, -0.944438, -3.311570, -2.421981, -5.672021, -0.395937, -5.759308, -8.653313, 0.352328, 3.196682, 4.611023, 4.372238, 6.334619, ...]
------------------------------------------------------------
Node 171 (call 163) - model.decoder.layers.4.encoder_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.417264 max=2.695800 mean=0.072559 std=0.780755
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.084635, 0.012768, 0.275173, -0.868508, 0.073665, -0.508540, -1.805523, -1.005085, 0.032049, 0.472151, -0.146109, 0.616408, -0.292015, -0.220420, -0.375869, -0.247893, ...]
------------------------------------------------------------
Node 172 (call 164) - model.decoder.layers.4.encoder_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-10.347693 max=66.881218 mean=0.166795 std=5.462744
  Preview: [2.160947, 0.494311, 3.886529, 6.372110, 3.229250, 7.102351, 0.948389, 7.721800, -5.444530, -5.357724, 4.072117, -2.461295, -0.840690, 6.186862, 0.122842, 2.546684, ...]
------------------------------------------------------------
Node 173 (call 165) - model.decoder.layers.4.encoder_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-10.347693 max=66.881218 mean=0.166795 std=5.462744
  Preview: [2.160947, 0.494311, 3.886529, 6.372110, 3.229250, 7.102351, 0.948389, 7.721800, -5.444530, -5.357724, 4.072117, -2.461295, -0.840690, 6.186862, 0.122842, 2.546684, ...]
------------------------------------------------------------
Node 174 (call 166) - model.decoder.layers.4.final_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.300159 max=1.949729 mean=0.003100 std=0.368426
  Preview: [-0.053277, 0.084456, 0.083705, 0.178840, 0.200238, 0.245926, -0.037299, 0.228470, -0.120345, -0.061470, 0.155800, 0.070942, -0.079273, -0.314883, -0.121304, 0.138864, ...]
------------------------------------------------------------
Node 175 (call 167) - model.decoder.layers.4.mlp.fc1 :: Linear :: output
  Shape: [1,1,2304]  Precision: FLOAT32
  Stats: min=-2.964644 max=3.048249 mean=-0.002669 std=0.789459
  Note: stats computed on first 1000 of 2304 values
  Preview: [0.391078, -0.627396, -0.257206, 0.523752, 0.614039, 0.286048, -1.646033, -0.539792, -0.039230, 0.914311, -0.163477, 0.647071, 0.380072, 0.146172, -0.579104, -0.056136, ...]
------------------------------------------------------------
Node 176 (call 168) - model.decoder.layers.4.mlp.activation_fn :: SiLUActivation :: output
  Shape: [1,1,1152]  Precision: FLOAT32
  Stats: min=-0.278454 max=8.351475 mean=0.186981 std=0.566675
  Note: stats computed on first 1000 of 1152 values
  Preview: [0.866151, -0.169736, -0.084823, -0.031843, 0.391288, -0.194822, -0.003912, -0.061359, 0.159947, -0.223360, 0.203971, 0.402530, -0.132608, 0.120105, -0.091278, 0.326229, ...]
------------------------------------------------------------
Node 177 (call 169) - model.decoder.layers.4.mlp.fc2 :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-22.657955 max=11.316532 mean=-0.025863 std=2.218213
  Preview: [1.437320, -2.126806, -0.904169, -1.097555, -0.134847, 2.158911, 3.379395, 0.522772, -1.133623, -1.297077, 1.046464, 0.901172, -0.852508, 0.259486, -0.237461, -0.516485, ...]
------------------------------------------------------------
Node 178 (call 170) - model.decoder.layers.4.mlp :: MoonshineDecoderMLP :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-22.657955 max=11.316532 mean=-0.025863 std=2.218213
  Preview: [1.437320, -2.126806, -0.904169, -1.097555, -0.134847, 2.158911, 3.379395, 0.522772, -1.133623, -1.297077, 1.046464, 0.901172, -0.852508, 0.259486, -0.237461, -0.516485, ...]
------------------------------------------------------------
Node 179 (call 171) - model.decoder.layers.4 :: MoonshineDecoderLayer :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-292.786469 max=145.527710 mean=0.099490 std=21.192310
  Preview: [-0.209774, 0.785059, 2.054058, 5.537148, 7.175457, 12.321294, 2.322240, 7.793901, -5.085996, -3.162763, 7.371292, 3.476620, -3.403925, -9.948292, -4.912559, 4.710194, ...]
------------------------------------------------------------
Node 180 (call 172) - model.decoder.layers.5.input_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-7.016649 max=3.798139 mean=0.007211 std=0.552924
  Preview: [-0.011660, 0.025176, 0.069390, 0.207685, 0.227538, 0.348585, 0.090942, 0.251928, -0.193518, -0.106718, 0.245925, 0.101802, -0.121177, -0.392906, -0.188555, 0.158790, ...]
------------------------------------------------------------
Node 181 (call 173) - model.decoder.layers.5.self_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.777847 max=4.614856 mean=-0.005931 std=1.091508
  Preview: [-0.833872, 0.166580, -0.046192, -1.085451, -0.803971, 0.710525, -1.194815, -0.355789, -0.128867, -0.661177, -0.339106, 0.718159, -0.495387, -0.293850, -0.969940, 0.750516, ...]
------------------------------------------------------------
Node 182 (call 174) - model.decoder.layers.5.self_attn.k_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-5.818909 max=5.042671 mean=0.090744 std=1.598065
  Preview: [-0.047225, -0.118470, 0.295123, 0.165441, 0.023006, -0.280409, 0.133093, -0.224764, 0.035540, -0.171781, -0.152943, 0.159049, 0.668024, -0.070356, -0.961179, -0.389783, ...]
------------------------------------------------------------
Node 183 (call 175) - model.decoder.layers.5.self_attn.v_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-2.296764 max=1.570116 mean=0.032456 std=0.483684
  Preview: [-0.068396, 0.068243, 0.208476, 0.242100, -0.269042, 0.025827, 0.086089, 0.097385, -0.145929, 0.061384, 0.126103, -0.027160, -0.212962, -0.064515, -0.166074, -0.013533, ...]
------------------------------------------------------------
Node 184 (call 176) - model.decoder.layers.5.self_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-9.212766 max=58.684517 mean=0.097454 std=4.068988
  Preview: [1.562744, -0.967822, -1.525608, 0.106660, -0.177903, 0.449505, 1.549421, 0.966372, -1.862696, -1.031874, 0.668518, -0.610072, -1.581349, 2.773265, -0.885035, 1.317786, ...]
------------------------------------------------------------
Node 185 (call 177) - model.decoder.layers.5.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-9.212766 max=58.684517 mean=0.097454 std=4.068988
  Preview: [1.562744, -0.967822, -1.525608, 0.106660, -0.177903, 0.449505, 1.549421, 0.966372, -1.862696, -1.031874, 0.668518, -0.610072, -1.581349, 2.773265, -0.885035, 1.317786, ...]
------------------------------------------------------------
Node 186 (call 178) - model.decoder.layers.5.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-6.335146 max=5.938776 mean=0.012913 std=0.618826
  Preview: [0.040340, -0.015668, 0.015803, 0.237905, 0.387719, 0.537059, 0.160771, 0.342730, -0.326703, -0.245621, 0.427443, 0.095969, -0.236979, -0.321341, -0.223728, 0.253072, ...]
------------------------------------------------------------
Node 187 (call 179) - model.decoder.layers.5.encoder_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.689580 max=2.768861 mean=-0.039353 std=1.147174
  Preview: [1.045381, -0.032355, -2.200940, 0.119666, -0.993437, 1.284565, -0.118422, 0.214245, 0.469757, 1.721873, 0.068059, -0.896606, -3.689580, 1.472785, 2.301311, 0.194630, ...]
------------------------------------------------------------
Node 188 (call 180) - model.decoder.layers.5.encoder_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-13.822813 max=11.878078 mean=0.277508 std=4.596017
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.745197, 2.452032, 0.279946, 3.522686, -3.804673, -0.568849, -7.950832, 1.197013, 10.283093, 7.677632, -0.395081, 1.131051, -0.787303, 3.500203, 0.499387, 1.904269, ...]
------------------------------------------------------------
Node 189 (call 181) - model.decoder.layers.5.encoder_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-2.853751 max=2.847139 mean=-0.060727 std=0.904414
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.979656, -0.589914, -0.950025, -0.411975, -2.197618, -0.803770, -0.541201, -0.279489, -0.069801, -0.050932, 0.569789, 0.005584, -0.734874, 0.402279, -1.075056, -0.670146, ...]
------------------------------------------------------------
Node 190 (call 182) - model.decoder.layers.5.encoder_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-25.622704 max=108.303795 mean=0.259264 std=11.338026
  Preview: [1.173678, 9.566764, -5.154658, 12.368044, 4.956698, 6.659906, -5.681980, 5.252526, -2.800117, 8.807096, 2.002155, -11.225492, 9.374974, 4.083317, -7.605823, -0.394300, ...]
------------------------------------------------------------
Node 191 (call 183) - model.decoder.layers.5.encoder_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-25.622704 max=108.303795 mean=0.259264 std=11.338026
  Preview: [1.173678, 9.566764, -5.154658, 12.368044, 4.956698, 6.659906, -5.681980, 5.252526, -2.800117, 8.807096, 2.002155, -11.225492, 9.374974, 4.083317, -7.605823, -0.394300, ...]
------------------------------------------------------------
Node 192 (call 184) - model.decoder.layers.5.final_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-2.211680 max=4.766235 mean=-0.007129 std=0.457753
  Preview: [0.061487, 0.296001, -0.162520, 0.532359, 0.349977, 0.587964, -0.076313, 0.408386, -0.316130, 0.131313, 0.271157, -0.234978, 0.112194, -0.112526, -0.397588, 0.146720, ...]
------------------------------------------------------------
Node 193 (call 185) - model.decoder.layers.5.mlp.fc1 :: Linear :: output
  Shape: [1,1,2304]  Precision: FLOAT32
  Stats: min=-4.238378 max=19.582315 mean=0.017591 std=1.494617
  Note: stats computed on first 1000 of 2304 values
  Preview: [2.142191, -0.574786, -4.238378, -1.446914, -1.786882, -0.358325, 0.299630, 1.635725, -1.362126, 0.641640, 0.553880, -1.889986, 0.718290, -1.180731, -1.187900, 1.404279, ...]
------------------------------------------------------------
Node 194 (call 186) - model.decoder.layers.5.mlp.activation_fn :: SiLUActivation :: output
  Shape: [1,1,1152]  Precision: FLOAT32
  Stats: min=-0.278464 max=4.053688 mean=0.144987 std=0.528229
  Note: stats computed on first 1000 of 1152 values
  Preview: [-0.111240, -0.108353, -0.118686, 0.466934, 0.032230, -0.203671, -0.087621, -0.109717, -0.243668, 0.152134, -0.132395, 0.034958, -0.256085, 0.082368, -0.195091, 0.366663, ...]
------------------------------------------------------------
Node 195 (call 187) - model.decoder.layers.5.mlp.fc2 :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-13.755093 max=28.773373 mean=0.142903 std=5.120591
  Preview: [-4.597227, 5.087208, -2.871413, -6.333598, -7.944021, 3.790861, 3.183697, -4.808052, -5.588724, -1.207498, 2.817123, 0.530060, -1.106300, -0.519381, 1.759082, 15.528008, ...]
------------------------------------------------------------
Node 196 (call 188) - model.decoder.layers.5.mlp :: MoonshineDecoderMLP :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-13.755093 max=28.773373 mean=0.142903 std=5.120591
  Preview: [-4.597227, 5.087208, -2.871413, -6.333598, -7.944021, 3.790861, 3.183697, -4.808052, -5.588724, -1.207498, 2.817123, 0.530060, -1.106300, -0.519381, 1.759082, 15.528008, ...]
------------------------------------------------------------
Node 197 (call 189) - model.decoder.layers.5 :: MoonshineDecoderLayer :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-97.024796 max=281.265991 mean=0.599111 std=21.093788
  Preview: [-2.070579, 14.471209, -7.497621, 11.678254, 4.010231, 23.221565, 1.373378, 9.204748, -15.337533, 3.404961, 12.859088, -7.828884, 3.283400, -3.611090, -11.644336, 21.161688, ...]
------------------------------------------------------------
Node 198 (call 190) - model.decoder.norm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.735964 max=2.314784 mean=-0.058501 std=0.892523
  Preview: [-0.204960, 1.076454, -0.596377, 0.833116, 0.240400, 1.562285, 0.071194, 0.667391, -1.243413, 0.205376, 0.897277, -0.620338, 0.212629, -0.292664, -0.871694, 1.047184, ...]
------------------------------------------------------------
Node 199 (call 191) - model.decoder :: MoonshineDecoder :: output.last_hidden_state
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.735964 max=2.314784 mean=-0.058501 std=0.892523
  Preview: [-0.204960, 1.076454, -0.596377, 0.833116, 0.240400, 1.562285, 0.071194, 0.667391, -1.243413, 0.205376, 0.897277, -0.620338, 0.212629, -0.292664, -0.871694, 1.047184, ...]
------------------------------------------------------------
Node 200 (call 191) - model.decoder :: MoonshineDecoder :: output.hidden_states[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-1.484618 max=0.408362 mean=0.000947 std=0.120420
  Preview: [0.004872, 0.019829, 0.064762, 0.019214, 0.024987, 0.024814, 0.000693, 0.011946, 0.028443, 0.014216, 0.005537, -0.566699, 0.000872, 0.000878, 0.007687, 0.006083, ...]
------------------------------------------------------------
Node 201 (call 191) - model.decoder :: MoonshineDecoder :: output.hidden_states[1]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-25.851522 max=5.221372 mean=-0.009852 std=1.834135
  Preview: [0.113633, 0.093343, 0.311656, 0.077115, -0.142165, 0.344664, -0.423401, -0.544635, -0.427395, 0.551118, 0.280224, 0.244821, 0.076052, -1.076009, 0.317580, 0.593914, ...]
------------------------------------------------------------
Node 202 (call 191) - model.decoder :: MoonshineDecoder :: output.hidden_states[2]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-249.257660 max=32.474289 mean=-0.123876 std=15.904207
  Preview: [-3.037814, 1.491966, 0.179368, -0.182741, 2.343094, 1.004347, 1.123932, 0.480699, 0.223517, 1.528077, 1.721508, 2.951519, -0.626453, -10.775481, -0.584157, -0.956570, ...]
------------------------------------------------------------
Node 203 (call 191) - model.decoder :: MoonshineDecoder :: output.hidden_states[3]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-277.316559 max=37.373386 mean=-0.107492 std=17.736361
  Preview: [-3.214118, 2.222707, 0.280212, -1.264462, 3.381316, 0.950868, -0.022242, 0.251057, 0.183486, 1.639503, 2.346364, 3.712770, -0.735738, -14.327869, -2.044768, 0.571812, ...]
------------------------------------------------------------
Node 204 (call 191) - model.decoder :: MoonshineDecoder :: output.hidden_states[4]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-285.072937 max=52.406719 mean=-0.049076 std=18.476643
  Preview: [-3.512081, 1.692787, 0.105383, -0.634450, 4.757415, 1.968071, -1.198135, -0.557468, 1.639060, 2.502467, 2.147481, 4.476406, -1.661065, -15.907120, -3.823553, 2.487344, ...]
------------------------------------------------------------
Node 205 (call 191) - model.decoder :: MoonshineDecoder :: output.hidden_states[5]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-292.786469 max=145.527710 mean=0.099490 std=21.192310
  Preview: [-0.209774, 0.785059, 2.054058, 5.537148, 7.175457, 12.321294, 2.322240, 7.793901, -5.085996, -3.162763, 7.371292, 3.476620, -3.403925, -9.948292, -4.912559, 4.710194, ...]
------------------------------------------------------------
Node 206 (call 191) - model.decoder :: MoonshineDecoder :: output.hidden_states[6]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.735964 max=2.314784 mean=-0.058501 std=0.892523
  Preview: [-0.204960, 1.076454, -0.596377, 0.833116, 0.240400, 1.562285, 0.071194, 0.667391, -1.243413, 0.205376, 0.897277, -0.620338, 0.212629, -0.292664, -0.871694, 1.047184, ...]
------------------------------------------------------------
Node 207 (call 192) - model :: MoonshineModel :: output.last_hidden_state
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.735964 max=2.314784 mean=-0.058501 std=0.892523
  Preview: [-0.204960, 1.076454, -0.596377, 0.833116, 0.240400, 1.562285, 0.071194, 0.667391, -1.243413, 0.205376, 0.897277, -0.620338, 0.212629, -0.292664, -0.871694, 1.047184, ...]
------------------------------------------------------------
Node 208 (call 192) - model :: MoonshineModel :: output.decoder_hidden_states[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-1.484618 max=0.408362 mean=0.000947 std=0.120420
  Preview: [0.004872, 0.019829, 0.064762, 0.019214, 0.024987, 0.024814, 0.000693, 0.011946, 0.028443, 0.014216, 0.005537, -0.566699, 0.000872, 0.000878, 0.007687, 0.006083, ...]
------------------------------------------------------------
Node 209 (call 192) - model :: MoonshineModel :: output.decoder_hidden_states[1]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-25.851522 max=5.221372 mean=-0.009852 std=1.834135
  Preview: [0.113633, 0.093343, 0.311656, 0.077115, -0.142165, 0.344664, -0.423401, -0.544635, -0.427395, 0.551118, 0.280224, 0.244821, 0.076052, -1.076009, 0.317580, 0.593914, ...]
------------------------------------------------------------
Node 210 (call 192) - model :: MoonshineModel :: output.decoder_hidden_states[2]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-249.257660 max=32.474289 mean=-0.123876 std=15.904207
  Preview: [-3.037814, 1.491966, 0.179368, -0.182741, 2.343094, 1.004347, 1.123932, 0.480699, 0.223517, 1.528077, 1.721508, 2.951519, -0.626453, -10.775481, -0.584157, -0.956570, ...]
------------------------------------------------------------
Node 211 (call 192) - model :: MoonshineModel :: output.decoder_hidden_states[3]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-277.316559 max=37.373386 mean=-0.107492 std=17.736361
  Preview: [-3.214118, 2.222707, 0.280212, -1.264462, 3.381316, 0.950868, -0.022242, 0.251057, 0.183486, 1.639503, 2.346364, 3.712770, -0.735738, -14.327869, -2.044768, 0.571812, ...]
------------------------------------------------------------
Node 212 (call 192) - model :: MoonshineModel :: output.decoder_hidden_states[4]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-285.072937 max=52.406719 mean=-0.049076 std=18.476643
  Preview: [-3.512081, 1.692787, 0.105383, -0.634450, 4.757415, 1.968071, -1.198135, -0.557468, 1.639060, 2.502467, 2.147481, 4.476406, -1.661065, -15.907120, -3.823553, 2.487344, ...]
------------------------------------------------------------
Node 213 (call 192) - model :: MoonshineModel :: output.decoder_hidden_states[5]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-292.786469 max=145.527710 mean=0.099490 std=21.192310
  Preview: [-0.209774, 0.785059, 2.054058, 5.537148, 7.175457, 12.321294, 2.322240, 7.793901, -5.085996, -3.162763, 7.371292, 3.476620, -3.403925, -9.948292, -4.912559, 4.710194, ...]
------------------------------------------------------------
Node 214 (call 192) - model :: MoonshineModel :: output.decoder_hidden_states[6]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.735964 max=2.314784 mean=-0.058501 std=0.892523
  Preview: [-0.204960, 1.076454, -0.596377, 0.833116, 0.240400, 1.562285, 0.071194, 0.667391, -1.243413, 0.205376, 0.897277, -0.620338, 0.212629, -0.292664, -0.871694, 1.047184, ...]
------------------------------------------------------------
Node 215 (call 192) - model :: MoonshineModel :: output.encoder_last_hidden_state
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.701852 max=4.225048 mean=0.029268 std=0.487575
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.180410, -0.194982, 0.163126, 0.162613, -1.183519, 0.366340, -0.022310, 0.021833, -0.057942, 0.387811, -0.012668, -0.082833, 0.152838, -0.176949, 0.085385, 0.049317, ...]
------------------------------------------------------------
Node 216 (call 192) - model :: MoonshineModel :: output.encoder_hidden_states[0]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-0.169931 max=17.944090 mean=0.736144 std=1.061540
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.249802, 1.098746, 0.815923, 0.549584, 0.711983, 0.537123, 0.040814, 0.941720, 0.152236, 1.142886, 0.348016, 1.096730, 0.880705, 1.114160, -0.169931, 0.751759, ...]
------------------------------------------------------------
Node 217 (call 192) - model :: MoonshineModel :: output.encoder_hidden_states[1]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-17.224667 max=44.700821 mean=0.672309 std=3.330945
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.919788, 0.044346, -0.182903, 0.294990, -0.409678, -1.187226, 0.026535, -1.403606, 3.172574, -0.779908, -1.073637, 0.003678, 0.151032, -0.381488, 0.967268, -0.453867, ...]
------------------------------------------------------------
Node 218 (call 192) - model :: MoonshineModel :: output.encoder_hidden_states[2]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-52.424728 max=121.588913 mean=0.623508 std=8.783118
  Note: stats computed on first 1000 of 239904 values
  Preview: [3.954081, 0.175852, -4.568874, 6.564672, -9.646872, -2.427884, 0.165133, -4.236034, -0.675837, 1.266756, -1.268249, 3.563319, 5.568339, -7.807684, 0.259345, -1.616693, ...]
------------------------------------------------------------
Node 219 (call 192) - model :: MoonshineModel :: output.encoder_hidden_states[3]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-90.951782 max=206.255768 mean=0.631912 std=14.625961
  Note: stats computed on first 1000 of 239904 values
  Preview: [8.205672, 0.348089, -4.513802, 13.445330, -20.487379, 1.840675, -2.053666, 1.179060, 3.203653, -0.647471, -1.080945, 0.794460, 5.361696, -16.456272, -1.961072, 2.921726, ...]
------------------------------------------------------------
Node 220 (call 192) - model :: MoonshineModel :: output.encoder_hidden_states[4]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-119.795975 max=237.744263 mean=0.651004 std=19.008587
  Note: stats computed on first 1000 of 239904 values
  Preview: [2.728039, 0.095875, -1.609458, 23.524673, -25.013945, 2.984801, -11.047765, 4.367095, 1.830329, -1.881678, 5.895497, 5.678264, 4.056880, -21.924973, -3.333668, 11.889000, ...]
------------------------------------------------------------
Node 221 (call 192) - model :: MoonshineModel :: output.encoder_hidden_states[5]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-177.130875 max=279.572144 mean=0.379514 std=25.222710
  Note: stats computed on first 1000 of 239904 values
  Preview: [7.295283, -3.896157, 3.397026, 16.844286, -41.323124, 13.485535, -6.974013, 4.415515, 4.013230, 6.786599, -7.227046, 2.539580, 0.499409, -11.958530, 0.078471, 6.914798, ...]
------------------------------------------------------------
Node 222 (call 192) - model :: MoonshineModel :: output.encoder_hidden_states[6]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.701852 max=4.225048 mean=0.029268 std=0.487575
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.180410, -0.194982, 0.163126, 0.162613, -1.183519, 0.366340, -0.022310, 0.021833, -0.057942, 0.387811, -0.012668, -0.082833, 0.152838, -0.176949, 0.085385, 0.049317, ...]
------------------------------------------------------------
Node 223 (call 193) - proj_out :: Linear :: output
  Shape: [1,1,32768]  Precision: FLOAT32
  Stats: min=-16.367409 max=9.771973 mean=-3.532688 std=4.745609
  Note: stats computed on first 1000 of 32768 values
  Preview: [-16.367409, 3.175433, 0.455281, -8.236915, -8.039355, -7.960202, -8.034779, -8.276384, -7.807445, -8.215768, -8.198946, -8.238001, -8.138678, -8.293655, -8.085892, -8.260395, ...]
