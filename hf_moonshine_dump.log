=== HuggingFace Moonshine Named-Module Intermediate Outputs ===
Model: UsefulSensors/moonshine-tiny
Audio: /home/karen/Documents/cactus/tests/assets/test.wav
Device: cpu
Captured records: 278
Capture inputs: False
Transcription: Hello hello hello just um quickly testing out creating a wave file through voice memos um the goal is to use this wave file to test out whisper hopefully this will transcribe properly that's all I can hope for alright here we go

============================================================

------------------------------------------------------------
Node 0 (call 1) - model.encoder.conv1 :: Conv1d :: output
  Shape: [1,288,5007]  Precision: FLOAT32
  Stats: min=-0.426224 max=0.748098 mean=0.002023 std=0.065704
  Note: stats computed on first 1000 of 1442016 values
  Preview: [0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, ...]
------------------------------------------------------------
Node 1 (call 2) - model.encoder.groupnorm :: GroupNorm :: output
  Shape: [1,288,5007]  Precision: FLOAT32
  Stats: min=-0.082607 max=0.130744 mean=0.000592 std=0.013154
  Note: stats computed on first 1000 of 1442016 values
  Preview: [0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, 0.000199, ...]
------------------------------------------------------------
Node 2 (call 3) - model.encoder.conv2 :: Conv1d :: output
  Shape: [1,576,1667]  Precision: FLOAT32
  Stats: min=-21.477932 max=17.481733 mean=-0.051263 std=1.891103
  Note: stats computed on first 1000 of 960192 values
  Preview: [-0.055752, -0.055752, -0.055752, -0.055752, -0.055752, -0.055752, -0.055752, -0.055752, -0.055752, -0.055857, -0.053531, -0.061118, -0.056058, -0.033147, -0.099938, -0.041056, ...]
------------------------------------------------------------
Node 3 (call 4) - model.encoder.conv3 :: Conv1d :: output
  Shape: [1,288,833]  Precision: FLOAT32
  Stats: min=-58.900169 max=7.016562 mean=1.327753 std=5.011224
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.367095, 1.367095, 1.367095, 1.367095, 1.367118, 1.367283, 1.367231, 1.368005, 1.366999, 1.366978, 1.367521, 1.368032, 1.364585, 1.394792, 1.476753, 1.365572, ...]
------------------------------------------------------------
Node 4 (call 5) - model.encoder.rotary_emb :: MoonshineRotaryEmbedding :: output[0]
  Shape: [1,833,32]  Precision: FLOAT32
  Stats: min=-0.999961 max=1.000000 mean=0.635629 std=0.615898
  Note: stats computed on first 1000 of 26656 values
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, ...]
------------------------------------------------------------
Node 5 (call 5) - model.encoder.rotary_emb :: MoonshineRotaryEmbedding :: output[1]
  Shape: [1,833,32]  Precision: FLOAT32
  Stats: min=-0.999990 max=0.999993 mean=0.165604 std=0.434995
  Note: stats computed on first 1000 of 26656 values
  Preview: [0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, ...]
------------------------------------------------------------
Node 6 (call 6) - model.encoder.layers.0.input_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-0.862336 max=5.538082 mean=-0.029207 std=0.411664
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.209784, 0.173638, 0.036818, -0.091005, -0.017873, -0.085657, -0.388287, 0.109183, -0.158693, 0.128860, -0.162583, 0.166229, 0.047985, 0.180506, -0.476131, 0.006181, ...]
------------------------------------------------------------
Node 7 (call 7) - model.encoder.layers.0.self_attn.q_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-4.411019 max=4.298198 mean=0.054778 std=1.431083
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.345022, -2.352002, -2.277231, 0.107283, -1.559785, 0.676009, -1.372143, 0.880714, 1.597135, -0.175779, 1.618484, -0.282176, 0.174163, -0.928872, 0.653102, 1.446692, ...]
------------------------------------------------------------
Node 8 (call 8) - model.encoder.layers.0.self_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-7.515983 max=9.216268 mean=0.017474 std=2.523773
  Note: stats computed on first 1000 of 239904 values
  Preview: [8.482825, 2.366151, -3.863383, -3.008265, -3.995601, 1.043804, -3.600246, 1.207852, 3.328593, -0.570381, 2.646869, -0.632931, 1.395728, -1.508700, 1.689167, 3.117201, ...]
------------------------------------------------------------
Node 9 (call 9) - model.encoder.layers.0.self_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-1.547188 max=1.720422 mean=0.013610 std=0.444827
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.223439, -0.339504, 0.090574, 0.517715, 0.094528, 0.401758, -0.260477, -0.084854, 0.055928, -0.170395, -0.504441, 0.244036, -0.163175, -0.311767, 0.336967, -0.020661, ...]
------------------------------------------------------------
Node 10 (call 10) - model.encoder.layers.0.self_attn.o_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-12.897277 max=11.163140 mean=-0.085914 std=1.158436
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.561368, -0.626552, -0.169485, 0.282854, -0.256646, 0.384653, 0.278791, -0.857329, 1.061151, -0.244771, 0.045502, -0.584108, 0.325741, -0.784716, 0.856896, -0.560135, ...]
------------------------------------------------------------
Node 11 (call 11) - model.encoder.layers.0.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-12.897277 max=11.163140 mean=-0.085914 std=1.158436
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.561368, -0.626552, -0.169485, 0.282854, -0.256646, 0.384653, 0.278791, -0.857329, 1.061151, -0.244771, 0.045502, -0.584108, 0.325741, -0.784716, 0.856896, -0.560135, ...]
------------------------------------------------------------
Node 12 (call 11) - model.encoder.layers.0.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,833,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.837600 mean=0.001929 std=0.027456
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.206417, 0.001823, 0.000143, 0.000421, 0.006926, 0.023363, 0.003455, 0.000116, 0.000023, 0.000143, 0.005992, 0.048824, 0.007898, 0.000046, 0.000001, 0.000002, ...]
------------------------------------------------------------
Node 13 (call 12) - model.encoder.layers.0.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-2.092260 max=3.404136 mean=-0.045563 std=0.491279
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.018223, -0.128261, -0.011228, 0.121644, -0.151596, 0.206593, -0.231535, -0.573733, 0.327089, 0.181389, -0.220712, -0.112974, 0.290237, -0.239702, 0.041734, -0.523369, ...]
------------------------------------------------------------
Node 14 (call 13) - model.encoder.layers.0.mlp.fc1 :: Linear :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-6.570932 max=1.955545 mean=-1.079442 std=1.101575
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.445857, -0.128140, -0.596236, -2.250732, -1.012625, -0.359444, -0.677213, -0.027930, -2.362596, -0.734291, -1.628832, -1.444986, -1.016242, -2.296980, -2.165362, -2.914286, ...]
------------------------------------------------------------
Node 15 (call 14) - model.encoder.layers.0.mlp.activation_fn :: GELUActivation :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-0.169971 max=1.906149 mean=-0.035278 std=0.206524
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.146174, -0.057537, -0.164268, -0.027462, -0.157584, -0.129267, -0.168718, -0.013654, -0.021437, -0.169904, -0.084169, -0.107263, -0.157271, -0.024830, -0.032870, -0.005195, ...]
------------------------------------------------------------
Node 16 (call 15) - model.encoder.layers.0.mlp.fc2 :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-12.676382 max=32.523010 mean=0.022078 std=2.746210
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.231356, -0.427847, -0.829342, -0.537449, -0.865015, -2.109001, -0.293066, -1.487998, 1.959187, -1.678025, -1.467154, -0.508944, -1.055416, -0.710934, 0.280303, -0.645493, ...]
------------------------------------------------------------
Node 17 (call 16) - model.encoder.layers.0.mlp :: MoonshineEncoderMLP :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-12.676382 max=32.523010 mean=0.022078 std=2.746210
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.231356, -0.427847, -0.829342, -0.537449, -0.865015, -2.109001, -0.293066, -1.487998, 1.959187, -1.678025, -1.467154, -0.508944, -1.055416, -0.710934, 0.280303, -0.645493, ...]
------------------------------------------------------------
Node 18 (call 17) - model.encoder.layers.0 :: MoonshineEncoderLayer :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-17.224686 max=44.700859 mean=0.672309 std=3.330945
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.919789, 0.044347, -0.182905, 0.294989, -0.409678, -1.187226, 0.026538, -1.403607, 3.172574, -0.779909, -1.073636, 0.003678, 0.151031, -0.381490, 0.967268, -0.453869, ...]
------------------------------------------------------------
Node 19 (call 18) - model.encoder.layers.1.input_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.035306 max=7.451347 mean=-0.000069 std=0.574079
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.192443, -0.125263, -0.162037, -0.078904, -0.246914, -0.362125, -0.106797, -0.514778, 0.395027, -0.295405, -0.277761, -0.134881, -0.095106, -0.186275, 0.080867, -0.331800, ...]
------------------------------------------------------------
Node 20 (call 19) - model.encoder.layers.1.self_attn.q_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-7.431763 max=4.433567 mean=-0.178177 std=1.799361
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.485259, -0.400333, 1.814862, -0.103998, -2.096267, 1.087448, -1.408395, 2.865367, -0.517003, 2.192842, -0.424579, -2.906268, 2.603988, -0.974964, 1.066540, -2.667846, ...]
------------------------------------------------------------
Node 21 (call 20) - model.encoder.layers.1.self_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-7.201685 max=6.196480 mean=-0.111414 std=1.973930
  Note: stats computed on first 1000 of 239904 values
  Preview: [2.362592, 0.575113, 0.190097, 0.439631, -1.874382, 1.033321, -1.199612, 1.032127, 0.271637, 1.947471, -1.260712, -2.554000, 1.064061, -1.409330, -0.241773, -2.192155, ...]
------------------------------------------------------------
Node 22 (call 21) - model.encoder.layers.1.self_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.672432 max=3.578771 mean=-0.028641 std=1.117279
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.858159, -1.347376, -1.278312, 1.372389, 1.003395, -0.941527, 0.300901, -0.440267, -0.253993, 0.321807, -0.673640, -0.916474, -1.065960, -0.587611, 0.332002, -0.237801, ...]
------------------------------------------------------------
Node 23 (call 22) - model.encoder.layers.1.self_attn.o_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-17.160509 max=31.785351 mean=-0.014831 std=2.799585
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.973654, -2.122605, -1.055352, 1.363618, 0.216590, -0.218842, -0.818522, 1.203229, -3.575066, 0.927133, 1.902829, 0.586562, 1.576199, -1.010796, -1.103164, 0.519279, ...]
------------------------------------------------------------
Node 24 (call 23) - model.encoder.layers.1.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-17.160509 max=31.785351 mean=-0.014831 std=2.799585
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.973654, -2.122605, -1.055352, 1.363618, 0.216590, -0.218842, -0.818522, 1.203229, -3.575066, 0.927133, 1.902829, 0.586562, 1.576199, -1.010796, -1.103164, 0.519279, ...]
------------------------------------------------------------
Node 25 (call 23) - model.encoder.layers.1.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,833,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.088339 mean=0.001842 std=0.006838
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.088339, 0.065523, 0.044084, 0.034436, 0.029763, 0.024873, 0.017679, 0.009979, 0.006033, 0.004165, 0.003947, 0.004073, 0.002893, 0.001316, 0.007295, 0.001946, ...]
------------------------------------------------------------
Node 26 (call 24) - model.encoder.layers.1.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-4.530429 max=4.954497 mean=-0.017698 std=0.505812
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.033900, -0.337759, -0.265843, 0.122104, -0.140672, -0.275234, -0.168616, -0.149181, -0.129829, -0.086486, 0.017868, -0.011048, 0.132767, -0.282055, -0.164286, -0.111934, ...]
------------------------------------------------------------
Node 27 (call 25) - model.encoder.layers.1.mlp.fc1 :: Linear :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-6.439821 max=3.027681 mean=-0.999132 std=1.122905
  Note: stats computed on first 1000 of 959616 values
  Preview: [-1.807866, 1.361528, -2.206217, -0.372517, -0.565765, -2.976440, -1.001905, -2.070240, -2.992321, -0.024831, -1.612473, -0.690324, -2.796378, -2.269907, -1.938197, -1.323183, ...]
------------------------------------------------------------
Node 28 (call 26) - model.encoder.layers.1.mlp.activation_fn :: GELUActivation :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-0.169970 max=3.023950 mean=-0.006060 std=0.310607
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.063842, 1.243519, -0.030191, -0.132152, -0.161682, -0.004340, -0.158496, -0.039780, -0.004142, -0.012170, -0.086154, -0.169126, -0.007226, -0.026346, -0.050974, -0.122907, ...]
------------------------------------------------------------
Node 29 (call 27) - model.encoder.layers.1.mlp.fc2 :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-18.175732 max=45.414185 mean=-0.033969 std=4.751531
  Note: stats computed on first 1000 of 239904 values
  Preview: [3.007946, 2.254107, -3.330614, 4.906060, -9.453775, -1.021817, 0.957116, -4.035656, -0.273342, 1.119531, -2.097437, 2.973075, 3.841107, -6.415398, 0.395241, -1.682106, ...]
------------------------------------------------------------
Node 30 (call 28) - model.encoder.layers.1.mlp :: MoonshineEncoderMLP :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-18.175732 max=45.414185 mean=-0.033969 std=4.751531
  Note: stats computed on first 1000 of 239904 values
  Preview: [3.007946, 2.254107, -3.330614, 4.906060, -9.453775, -1.021817, 0.957116, -4.035656, -0.273342, 1.119531, -2.097437, 2.973075, 3.841107, -6.415398, 0.395241, -1.682106, ...]
------------------------------------------------------------
Node 31 (call 29) - model.encoder.layers.1 :: MoonshineEncoderLayer :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-52.424725 max=121.588928 mean=0.623509 std=8.783118
  Note: stats computed on first 1000 of 239904 values
  Preview: [3.954081, 0.175849, -4.568871, 6.564666, -9.646864, -2.427885, 0.165132, -4.236034, -0.675834, 1.266754, -1.268244, 3.563315, 5.568336, -7.807683, 0.259346, -1.616696, ...]
------------------------------------------------------------
Node 32 (call 30) - model.encoder.layers.2.input_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-2.933396 max=6.386298 mean=-0.000565 std=0.561087
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.324533, -0.043724, -0.433411, 0.543170, -0.965384, -0.259275, -0.038027, -0.505653, -0.117735, 0.072445, -0.159324, 0.223085, 0.394367, -0.818123, -0.040041, -0.244205, ...]
------------------------------------------------------------
Node 33 (call 31) - model.encoder.layers.2.self_attn.q_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-4.006958 max=4.383430 mean=0.092820 std=1.188761
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.863046, 0.091146, 0.435344, 0.282942, -0.706204, -0.402189, -0.509099, -0.482892, 0.646878, 0.674634, 0.866138, 1.447604, 1.172973, -1.064220, 2.288859, -1.054574, ...]
------------------------------------------------------------
Node 34 (call 32) - model.encoder.layers.2.self_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-10.812767 max=7.752890 mean=0.000747 std=2.443070
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.025724, -0.172446, -1.674155, -1.058569, -0.676722, 1.027431, -0.723574, 2.191214, 2.238913, -0.684844, 0.814001, 0.672713, -0.108565, -4.278103, 3.856595, -3.469475, ...]
------------------------------------------------------------
Node 35 (call 33) - model.encoder.layers.2.self_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-5.619289 max=6.741537 mean=0.018686 std=1.672800
  Note: stats computed on first 1000 of 239904 values
  Preview: [-4.483876, 2.357349, -0.882128, -1.853623, 0.253976, 4.718763, -0.119042, 0.631879, -2.391709, 0.689803, 0.581782, 2.026757, 0.941647, 1.318418, -0.947335, -1.560766, ...]
------------------------------------------------------------
Node 36 (call 34) - model.encoder.layers.2.self_attn.o_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-18.570349 max=42.997559 mean=0.035222 std=3.506826
  Note: stats computed on first 1000 of 239904 values
  Preview: [3.585690, 0.979974, 2.540822, -0.567747, -1.094153, 1.850341, -1.160551, 3.436591, 1.044644, -1.439478, -3.002088, 0.261939, -1.418292, -1.314624, -1.510545, 1.279120, ...]
------------------------------------------------------------
Node 37 (call 35) - model.encoder.layers.2.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-18.570349 max=42.997559 mean=0.035222 std=3.506826
  Note: stats computed on first 1000 of 239904 values
  Preview: [3.585690, 0.979974, 2.540822, -0.567747, -1.094153, 1.850341, -1.160551, 3.436591, 1.044644, -1.439478, -3.002088, 0.261939, -1.418292, -1.314624, -1.510545, 1.279120, ...]
------------------------------------------------------------
Node 38 (call 35) - model.encoder.layers.2.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,833,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.040443 mean=0.001987 std=0.005571
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.017569, 0.021672, 0.026987, 0.031813, 0.036181, 0.039707, 0.040443, 0.036341, 0.028950, 0.022327, 0.019708, 0.023282, 0.027502, 0.013863, 0.022641, 0.018133, ...]
------------------------------------------------------------
Node 39 (call 36) - model.encoder.layers.2.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-4.051926 max=5.989649 mean=-0.011612 std=0.538078
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.405395, 0.031400, -0.171252, 0.330048, -0.773780, -0.080412, -0.099440, -0.124135, -0.018246, -0.055336, -0.301669, 0.214956, 0.231583, -0.604788, -0.161866, -0.076434, ...]
------------------------------------------------------------
Node 40 (call 37) - model.encoder.layers.2.mlp.fc1 :: Linear :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-5.524936 max=5.976075 mean=-0.884260 std=1.075227
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.322037, -1.016096, -1.304748, -1.221417, -0.092022, -1.519016, -1.012946, 0.595392, -0.633371, -3.213106, -1.198609, -1.698552, -0.571495, -0.784793, -0.527222, -0.603954, ...]
------------------------------------------------------------
Node 41 (call 38) - model.encoder.layers.2.mlp.activation_fn :: GELUActivation :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-0.169971 max=5.976075 mean=0.000673 std=0.359272
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.120349, -0.157283, -0.125242, -0.135533, -0.042638, -0.097793, -0.157557, 0.431188, -0.166732, -0.002109, -0.138248, -0.075928, -0.162209, -0.169741, -0.157650, -0.164841, ...]
------------------------------------------------------------
Node 42 (call 39) - model.encoder.layers.2.mlp.fc2 :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-25.037798 max=41.759842 mean=-0.026819 std=4.806628
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.665895, -0.807741, -2.485749, 7.448413, -9.746348, 2.418222, -1.058251, 1.978509, 2.834846, -0.474750, 3.189389, -3.030794, 1.211647, -7.333958, -0.709869, 3.259298, ...]
------------------------------------------------------------
Node 43 (call 40) - model.encoder.layers.2.mlp :: MoonshineEncoderMLP :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-25.037798 max=41.759842 mean=-0.026819 std=4.806628
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.665895, -0.807741, -2.485749, 7.448413, -9.746348, 2.418222, -1.058251, 1.978509, 2.834846, -0.474750, 3.189389, -3.030794, 1.211647, -7.333958, -0.709869, 3.259298, ...]
------------------------------------------------------------
Node 44 (call 41) - model.encoder.layers.2 :: MoonshineEncoderLayer :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-90.951782 max=206.255829 mean=0.631912 std=14.625962
  Note: stats computed on first 1000 of 239904 values
  Preview: [8.205666, 0.348083, -4.513798, 13.445332, -20.487366, 1.840679, -2.053669, 1.179065, 3.203657, -0.647474, -1.080943, 0.794459, 5.361691, -16.456264, -1.961069, 2.921722, ...]
------------------------------------------------------------
Node 45 (call 42) - model.encoder.layers.3.input_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-2.343944 max=4.879796 mean=0.000056 std=0.456551
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.331530, -0.010306, -0.262149, 0.658674, -1.133126, 0.063994, -0.107216, 0.033558, 0.143944, -0.064663, -0.072602, 0.011940, 0.224872, -0.824325, -0.131295, 0.152174, ...]
------------------------------------------------------------
Node 46 (call 43) - model.encoder.layers.3.self_attn.q_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-9.523055 max=4.683911 mean=0.017555 std=1.524511
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.830168, 0.765891, 0.464835, 1.240576, 1.045187, 0.990879, -0.475832, 1.179418, 1.299567, -0.458752, 0.751644, -1.573988, -1.509513, -0.498103, 1.005308, -3.037590, ...]
------------------------------------------------------------
Node 47 (call 44) - model.encoder.layers.3.self_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-10.211806 max=9.491602 mean=0.023051 std=2.306301
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.458590, 0.476913, 2.127491, -0.957686, 1.608676, -0.549993, 1.139613, 0.903967, 0.241531, -2.331407, 0.271090, -2.075841, -3.661779, 0.798587, 0.254526, -2.257981, ...]
------------------------------------------------------------
Node 48 (call 45) - model.encoder.layers.3.self_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-5.262751 max=5.128075 mean=0.025442 std=1.612858
  Note: stats computed on first 1000 of 239904 values
  Preview: [-1.775319, -0.053937, -0.312249, 0.373669, 0.531401, -0.887631, 0.084168, -0.009564, -0.220682, -1.706302, -0.994906, 2.594960, -1.694783, 0.661007, 3.187025, -0.467380, ...]
------------------------------------------------------------
Node 49 (call 46) - model.encoder.layers.3.self_attn.o_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-23.665070 max=12.255773 mean=0.007675 std=4.271673
  Note: stats computed on first 1000 of 239904 values
  Preview: [-2.076323, 3.059222, 5.901384, 10.286223, -0.808328, -1.626112, -0.767662, 1.553915, -0.720975, -2.396867, 2.196792, -1.051112, 2.028032, -2.880387, 3.946670, 2.314526, ...]
------------------------------------------------------------
Node 50 (call 47) - model.encoder.layers.3.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-23.665070 max=12.255773 mean=0.007675 std=4.271673
  Note: stats computed on first 1000 of 239904 values
  Preview: [-2.076323, 3.059222, 5.901384, 10.286223, -0.808328, -1.626112, -0.767662, 1.553915, -0.720975, -2.396867, 2.196792, -1.051112, 2.028032, -2.880387, 3.946670, 2.314526, ...]
------------------------------------------------------------
Node 51 (call 47) - model.encoder.layers.3.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,833,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.083949 mean=0.001963 std=0.007324
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.024036, 0.051044, 0.083751, 0.083949, 0.058506, 0.038361, 0.030023, 0.027527, 0.025139, 0.019248, 0.012609, 0.008372, 0.006696, 0.002881, 0.004792, 0.013203, ...]
------------------------------------------------------------
Node 52 (call 48) - model.encoder.layers.3.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-4.797861 max=6.422096 mean=-0.008583 std=0.657913
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.254638, 0.127937, 0.039140, 1.088163, -1.038628, -0.017188, -0.150793, 0.112100, 0.100434, -0.183112, 0.024140, -0.038890, 0.316923, -0.971235, 0.083390, 0.276100, ...]
------------------------------------------------------------
Node 53 (call 49) - model.encoder.layers.3.mlp.fc1 :: Linear :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-7.017831 max=5.583984 mean=-1.036842 std=1.264622
  Note: stats computed on first 1000 of 959616 values
  Preview: [-1.382794, -3.714823, -1.049453, 0.205275, -1.823157, -1.641594, -2.190908, -1.726216, -1.558938, 0.533694, -1.431950, -1.263709, -1.618603, -1.132487, -1.516413, -2.336563, ...]
------------------------------------------------------------
Node 54 (call 50) - model.encoder.layers.3.mlp.activation_fn :: GELUActivation :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-0.169970 max=5.583984 mean=0.031784 std=0.401888
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.115275, -0.000378, -0.154254, 0.119331, -0.062242, -0.082633, -0.031175, -0.072767, -0.092765, 0.375306, -0.108941, -0.130373, -0.085408, -0.145768, -0.098123, -0.022737, ...]
------------------------------------------------------------
Node 55 (call 51) - model.encoder.layers.3.mlp.fc2 :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-32.414562 max=26.871695 mean=0.011417 std=5.776134
  Note: stats computed on first 1000 of 239904 values
  Preview: [-3.401297, -3.311429, -2.997041, -0.206885, -3.718235, 2.770240, -8.226437, 1.634121, -0.652355, 1.162656, 4.779647, 5.934916, -3.332840, -2.588303, -5.319268, 6.652750, ...]
------------------------------------------------------------
Node 56 (call 52) - model.encoder.layers.3.mlp :: MoonshineEncoderMLP :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-32.414562 max=26.871695 mean=0.011417 std=5.776134
  Note: stats computed on first 1000 of 239904 values
  Preview: [-3.401297, -3.311429, -2.997041, -0.206885, -3.718235, 2.770240, -8.226437, 1.634121, -0.652355, 1.162656, 4.779647, 5.934916, -3.332840, -2.588303, -5.319268, 6.652750, ...]
------------------------------------------------------------
Node 57 (call 53) - model.encoder.layers.3 :: MoonshineEncoderLayer :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-119.795982 max=237.744293 mean=0.651004 std=19.008587
  Note: stats computed on first 1000 of 239904 values
  Preview: [2.728045, 0.095876, -1.609455, 23.524672, -25.013927, 2.984807, -11.047768, 4.367102, 1.830327, -1.881685, 5.895497, 5.678263, 4.056883, -21.924953, -3.333667, 11.888998, ...]
------------------------------------------------------------
Node 58 (call 54) - model.encoder.layers.4.input_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.007017 max=7.435551 mean=0.006715 std=0.682520
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.103364, -0.018608, -0.100431, 1.122695, -1.182149, 0.115960, -0.488698, 0.201378, 0.068968, -0.133174, 0.180497, 0.245230, 0.180495, -1.063497, -0.176842, 0.632001, ...]
------------------------------------------------------------
Node 59 (call 55) - model.encoder.layers.4.self_attn.q_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-5.522125 max=5.865769 mean=0.066717 std=1.569184
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.105498, 0.294956, -1.336910, 0.390756, -0.090195, 0.764791, 0.160620, 0.532734, 0.037672, -0.035919, 0.234716, 0.520776, -0.949365, 0.174209, 0.580675, -0.523978, ...]
------------------------------------------------------------
Node 60 (call 56) - model.encoder.layers.4.self_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-8.938631 max=9.530460 mean=-0.145951 std=2.650853
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.186655, 0.058266, 1.114901, -1.841658, -1.344701, -4.011315, -1.873646, -0.915020, 3.037019, 1.149236, 0.342515, -1.844830, -0.672452, -2.821979, 2.348047, 0.968721, ...]
------------------------------------------------------------
Node 61 (call 57) - model.encoder.layers.4.self_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-9.118510 max=6.948771 mean=-0.278485 std=2.750566
  Note: stats computed on first 1000 of 239904 values
  Preview: [-2.123553, 4.865613, -4.283845, -1.646591, -0.009942, 1.577940, 0.800837, -0.681779, -1.536665, -0.798181, 1.998346, 0.875322, 0.032694, 2.815026, -3.170080, 1.245812, ...]
------------------------------------------------------------
Node 62 (call 58) - model.encoder.layers.4.self_attn.o_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-45.707561 max=29.098986 mean=-0.258181 std=7.398414
  Note: stats computed on first 1000 of 239904 values
  Preview: [5.876956, -4.199610, 8.203743, -2.175675, -11.479306, 9.470018, 2.749528, 4.194856, 5.137977, 7.274524, -12.992734, -7.020973, -0.633322, 6.697638, 8.191267, -1.290912, ...]
------------------------------------------------------------
Node 63 (call 59) - model.encoder.layers.4.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-45.707561 max=29.098986 mean=-0.258181 std=7.398414
  Note: stats computed on first 1000 of 239904 values
  Preview: [5.876956, -4.199610, 8.203743, -2.175675, -11.479306, 9.470018, 2.749528, 4.194856, 5.137977, 7.274524, -12.992734, -7.020973, -0.633322, 6.697638, 8.191267, -1.290912, ...]
------------------------------------------------------------
Node 64 (call 59) - model.encoder.layers.4.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,833,833]  Precision: FLOAT32
  Stats: min=0.000004 max=0.029645 mean=0.001199 std=0.002607
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.021287, 0.016123, 0.014071, 0.013301, 0.012963, 0.013161, 0.014118, 0.014687, 0.013328, 0.009408, 0.005139, 0.002332, 0.001407, 0.002840, 0.004298, 0.001381, ...]
------------------------------------------------------------
Node 65 (call 60) - model.encoder.layers.4.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-5.256735 max=7.634162 mean=0.004924 std=0.762765
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.288275, -0.155419, 0.244435, 0.826161, -1.380840, 0.485091, -0.318254, 0.338663, 0.225958, 0.200603, -0.235963, -0.061122, 0.121667, -0.556283, 0.220201, 0.420960, ...]
------------------------------------------------------------
Node 66 (call 61) - model.encoder.layers.4.mlp.fc1 :: Linear :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-11.287512 max=3.344824 mean=-1.423806 std=1.708199
  Note: stats computed on first 1000 of 959616 values
  Preview: [-4.088411, 2.009997, -0.345452, 0.577483, -0.574007, -0.601518, -0.039538, -1.283214, 0.304233, -3.405406, -2.468293, -1.308785, -0.582268, -1.548229, -1.800308, -2.306838, ...]
------------------------------------------------------------
Node 67 (call 62) - model.encoder.layers.4.mlp.activation_fn :: GELUActivation :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-0.169971 max=3.343447 mean=0.054892 std=0.390949
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.000089, 1.965343, -0.126048, 0.414745, -0.162433, -0.164664, -0.019146, -0.127947, 0.188480, -0.001125, -0.016755, -0.124732, -0.163147, -0.094107, -0.064642, -0.024295, ...]
------------------------------------------------------------
Node 68 (call 63) - model.encoder.layers.4.mlp.fc2 :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-23.189438 max=14.968501 mean=-0.013309 std=4.653526
  Note: stats computed on first 1000 of 239904 values
  Preview: [-1.309721, 0.207578, -3.197249, -4.504704, -4.829871, 1.030718, 1.324222, -4.146431, -2.955072, 1.393754, -0.129811, 3.882285, -2.924155, 3.268802, -4.779126, -3.683295, ...]
------------------------------------------------------------
Node 69 (call 64) - model.encoder.layers.4.mlp :: MoonshineEncoderMLP :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-23.189438 max=14.968501 mean=-0.013309 std=4.653526
  Note: stats computed on first 1000 of 239904 values
  Preview: [-1.309721, 0.207578, -3.197249, -4.504704, -4.829871, 1.030718, 1.324222, -4.146431, -2.955072, 1.393754, -0.129811, 3.882285, -2.924155, 3.268802, -4.779126, -3.683295, ...]
------------------------------------------------------------
Node 70 (call 65) - model.encoder.layers.4 :: MoonshineEncoderLayer :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-177.130890 max=279.572205 mean=0.379514 std=25.222710
  Note: stats computed on first 1000 of 239904 values
  Preview: [7.295281, -3.896157, 3.397039, 16.844292, -41.323105, 13.485542, -6.974017, 4.415526, 4.013231, 6.786592, -7.227048, 2.539574, 0.499406, -11.958514, 0.078474, 6.914792, ...]
------------------------------------------------------------
Node 71 (call 66) - model.encoder.layers.5.input_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.769085 max=7.161863 mean=0.023188 std=0.724021
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.242005, -0.147170, 0.148415, 0.624026, -1.550419, 0.501366, -0.308525, 0.169065, 0.137925, 0.310454, -0.243352, 0.118461, 0.007630, -0.452293, -0.007622, 0.271978, ...]
------------------------------------------------------------
Node 72 (call 67) - model.encoder.layers.5.self_attn.q_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-5.066801 max=5.283536 mean=0.076685 std=1.390642
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.178661, 0.177492, -0.667453, -0.871811, -0.476052, 0.107569, 0.291399, -0.035172, -0.508727, -0.594988, 0.214654, 0.320799, 0.011163, 0.171441, -0.465545, 0.308201, ...]
------------------------------------------------------------
Node 73 (call 68) - model.encoder.layers.5.self_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-6.694926 max=8.856271 mean=0.314050 std=2.277671
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.249669, -0.072013, 0.204000, 0.453569, -0.377066, 0.179185, 0.034870, -0.184263, -0.168079, 0.343649, 0.113367, -0.315793, -0.108333, 1.302778, 1.960501, -0.078163, ...]
------------------------------------------------------------
Node 74 (call 69) - model.encoder.layers.5.self_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-15.035702 max=11.350099 mean=0.129373 std=4.041001
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.594193, -2.983451, -4.508448, -5.546752, 7.023083, -8.561198, -0.465320, -1.384869, 8.551837, -2.298259, 4.096009, -4.961095, 2.212939, 3.161080, -3.801377, -1.271359, ...]
------------------------------------------------------------
Node 75 (call 70) - model.encoder.layers.5.self_attn.o_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-128.723404 max=120.134735 mean=-0.256332 std=20.303566
  Note: stats computed on first 1000 of 239904 values
  Preview: [2.645356, 1.738435, 2.209552, 5.601516, -49.412186, 9.081799, 5.755300, -4.931899, -7.232386, 14.271059, 14.028609, -6.261628, -0.461735, -4.324121, -0.168743, -1.049286, ...]
------------------------------------------------------------
Node 76 (call 71) - model.encoder.layers.5.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-128.723404 max=120.134735 mean=-0.256332 std=20.303566
  Note: stats computed on first 1000 of 239904 values
  Preview: [2.645356, 1.738435, 2.209552, 5.601516, -49.412186, 9.081799, 5.755300, -4.931899, -7.232386, 14.271059, 14.028609, -6.261628, -0.461735, -4.324121, -0.168743, -1.049286, ...]
------------------------------------------------------------
Node 77 (call 71) - model.encoder.layers.5.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,833,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.036430 mean=0.001003 std=0.004184
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.000105, 0.000106, 0.000109, 0.000108, 0.000102, 0.000093, 0.000082, 0.000077, 0.000072, 0.000068, 0.000058, 0.000046, 0.000014, 0.000002, 0.000001, 0.000031, ...]
------------------------------------------------------------
Node 78 (call 72) - model.encoder.layers.5.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-5.235021 max=4.305078 mean=0.026847 std=0.811538
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.313143, -0.066178, 0.208237, 0.752950, -2.643875, 0.783406, -0.041550, -0.020030, -0.105821, 0.765220, 0.214834, -0.128292, 0.002063, -0.564358, -0.002992, 0.204581, ...]
------------------------------------------------------------
Node 79 (call 73) - model.encoder.layers.5.mlp.fc1 :: Linear :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-27.218390 max=11.109094 mean=-1.910451 std=3.195572
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.982387, 1.413787, -2.844927, -0.741804, -0.273013, -2.249529, 1.485077, -5.252393, -2.206297, -1.545584, -1.711794, -1.273115, -5.857589, -0.399105, -1.685340, -0.342555, ...]
------------------------------------------------------------
Node 80 (call 74) - model.encoder.layers.5.mlp.activation_fn :: GELUActivation :: output
  Shape: [1,833,1152]  Precision: FLOAT32
  Stats: min=-0.169967 max=11.109094 mean=0.319678 std=1.133814
  Note: stats computed on first 1000 of 959616 values
  Preview: [-0.160085, 1.302505, -0.006319, -0.169950, -0.107136, -0.027533, 1.382960, -0.000000, -0.030186, -0.094439, -0.074407, -0.129207, -0.000000, -0.137654, -0.077461, -0.125364, ...]
------------------------------------------------------------
Node 81 (call 75) - model.encoder.layers.5.mlp.fc2 :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-307.010468 max=306.720062 mean=-0.105167 std=41.413654
  Note: stats computed on first 1000 of 239904 values
  Preview: [5.581937, -14.522903, 6.911898, -8.338839, -86.763023, 8.888704, -0.783091, 1.933493, -1.273420, 6.843068, -8.098689, -2.962881, 10.893194, 1.939468, 7.863350, -2.376713, ...]
------------------------------------------------------------
Node 82 (call 76) - model.encoder.layers.5.mlp :: MoonshineEncoderMLP :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-307.010468 max=306.720062 mean=-0.105167 std=41.413654
  Note: stats computed on first 1000 of 239904 values
  Preview: [5.581937, -14.522903, 6.911898, -8.338839, -86.763023, 8.888704, -0.783091, 1.933493, -1.273420, 6.843068, -8.098689, -2.962881, 10.893194, 1.939468, 7.863350, -2.376713, ...]
------------------------------------------------------------
Node 83 (call 77) - model.encoder.layers.5 :: MoonshineEncoderLayer :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-561.035278 max=477.929657 mean=0.018015 std=70.777206
  Note: stats computed on first 1000 of 239904 values
  Preview: [15.522574, -16.680626, 12.518488, 14.106970, -177.498322, 31.456045, -2.001809, 1.417120, -4.492575, 27.900719, -1.297128, -6.684935, 10.930864, -14.343167, 7.773081, 3.488794, ...]
------------------------------------------------------------
Node 84 (call 78) - model.encoder.layer_norm :: LayerNorm :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.701851 max=4.225048 mean=0.029268 std=0.487575
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.180410, -0.194983, 0.163126, 0.162613, -1.183519, 0.366340, -0.022310, 0.021833, -0.057942, 0.387811, -0.012669, -0.082833, 0.152838, -0.176949, 0.085384, 0.049317, ...]
------------------------------------------------------------
Node 85 (call 79) - model.encoder :: MoonshineEncoder :: output.last_hidden_state
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.701851 max=4.225048 mean=0.029268 std=0.487575
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.180410, -0.194983, 0.163126, 0.162613, -1.183519, 0.366340, -0.022310, 0.021833, -0.057942, 0.387811, -0.012669, -0.082833, 0.152838, -0.176949, 0.085384, 0.049317, ...]
------------------------------------------------------------
Node 86 (call 79) - model.encoder :: MoonshineEncoder :: output.hidden_states[0]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-0.169931 max=17.944090 mean=0.736144 std=1.061540
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.249802, 1.098746, 0.815923, 0.549584, 0.711983, 0.537123, 0.040814, 0.941720, 0.152236, 1.142886, 0.348016, 1.096730, 0.880705, 1.114160, -0.169931, 0.751759, ...]
------------------------------------------------------------
Node 87 (call 79) - model.encoder :: MoonshineEncoder :: output.hidden_states[1]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-17.224686 max=44.700859 mean=0.672309 std=3.330945
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.919789, 0.044347, -0.182905, 0.294989, -0.409678, -1.187226, 0.026538, -1.403607, 3.172574, -0.779909, -1.073636, 0.003678, 0.151031, -0.381490, 0.967268, -0.453869, ...]
------------------------------------------------------------
Node 88 (call 79) - model.encoder :: MoonshineEncoder :: output.hidden_states[2]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-52.424725 max=121.588928 mean=0.623509 std=8.783118
  Note: stats computed on first 1000 of 239904 values
  Preview: [3.954081, 0.175849, -4.568871, 6.564666, -9.646864, -2.427885, 0.165132, -4.236034, -0.675834, 1.266754, -1.268244, 3.563315, 5.568336, -7.807683, 0.259346, -1.616696, ...]
------------------------------------------------------------
Node 89 (call 79) - model.encoder :: MoonshineEncoder :: output.hidden_states[3]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-90.951782 max=206.255829 mean=0.631912 std=14.625962
  Note: stats computed on first 1000 of 239904 values
  Preview: [8.205666, 0.348083, -4.513798, 13.445332, -20.487366, 1.840679, -2.053669, 1.179065, 3.203657, -0.647474, -1.080943, 0.794459, 5.361691, -16.456264, -1.961069, 2.921722, ...]
------------------------------------------------------------
Node 90 (call 79) - model.encoder :: MoonshineEncoder :: output.hidden_states[4]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-119.795982 max=237.744293 mean=0.651004 std=19.008587
  Note: stats computed on first 1000 of 239904 values
  Preview: [2.728045, 0.095876, -1.609455, 23.524672, -25.013927, 2.984807, -11.047768, 4.367102, 1.830327, -1.881685, 5.895497, 5.678263, 4.056883, -21.924953, -3.333667, 11.888998, ...]
------------------------------------------------------------
Node 91 (call 79) - model.encoder :: MoonshineEncoder :: output.hidden_states[5]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-177.130890 max=279.572205 mean=0.379514 std=25.222710
  Note: stats computed on first 1000 of 239904 values
  Preview: [7.295281, -3.896157, 3.397039, 16.844292, -41.323105, 13.485542, -6.974017, 4.415526, 4.013231, 6.786592, -7.227048, 2.539574, 0.499406, -11.958514, 0.078474, 6.914792, ...]
------------------------------------------------------------
Node 92 (call 79) - model.encoder :: MoonshineEncoder :: output.hidden_states[6]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.701851 max=4.225048 mean=0.029268 std=0.487575
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.180410, -0.194983, 0.163126, 0.162613, -1.183519, 0.366340, -0.022310, 0.021833, -0.057942, 0.387811, -0.012669, -0.082833, 0.152838, -0.176949, 0.085384, 0.049317, ...]
------------------------------------------------------------
Node 93 (call 79) - model.encoder :: MoonshineEncoder :: output.attentions[0]
  Shape: [1,8,833,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.837600 mean=0.001929 std=0.027456
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.206417, 0.001823, 0.000143, 0.000421, 0.006926, 0.023363, 0.003455, 0.000116, 0.000023, 0.000143, 0.005992, 0.048824, 0.007898, 0.000046, 0.000001, 0.000002, ...]
------------------------------------------------------------
Node 94 (call 79) - model.encoder :: MoonshineEncoder :: output.attentions[1]
  Shape: [1,8,833,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.088339 mean=0.001842 std=0.006838
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.088339, 0.065523, 0.044084, 0.034436, 0.029763, 0.024873, 0.017679, 0.009979, 0.006033, 0.004165, 0.003947, 0.004073, 0.002893, 0.001316, 0.007295, 0.001946, ...]
------------------------------------------------------------
Node 95 (call 79) - model.encoder :: MoonshineEncoder :: output.attentions[2]
  Shape: [1,8,833,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.040443 mean=0.001987 std=0.005571
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.017569, 0.021672, 0.026987, 0.031813, 0.036181, 0.039707, 0.040443, 0.036341, 0.028950, 0.022327, 0.019708, 0.023282, 0.027502, 0.013863, 0.022641, 0.018133, ...]
------------------------------------------------------------
Node 96 (call 79) - model.encoder :: MoonshineEncoder :: output.attentions[3]
  Shape: [1,8,833,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.083949 mean=0.001963 std=0.007324
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.024036, 0.051044, 0.083751, 0.083949, 0.058506, 0.038361, 0.030023, 0.027527, 0.025139, 0.019248, 0.012609, 0.008372, 0.006696, 0.002881, 0.004792, 0.013203, ...]
------------------------------------------------------------
Node 97 (call 79) - model.encoder :: MoonshineEncoder :: output.attentions[4]
  Shape: [1,8,833,833]  Precision: FLOAT32
  Stats: min=0.000004 max=0.029645 mean=0.001199 std=0.002607
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.021287, 0.016123, 0.014071, 0.013301, 0.012963, 0.013161, 0.014118, 0.014687, 0.013328, 0.009408, 0.005139, 0.002332, 0.001407, 0.002840, 0.004298, 0.001381, ...]
------------------------------------------------------------
Node 98 (call 79) - model.encoder :: MoonshineEncoder :: output.attentions[5]
  Shape: [1,8,833,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.036430 mean=0.001003 std=0.004184
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.000105, 0.000106, 0.000109, 0.000108, 0.000102, 0.000093, 0.000082, 0.000077, 0.000072, 0.000068, 0.000058, 0.000046, 0.000014, 0.000002, 0.000001, 0.000031, ...]
------------------------------------------------------------
Node 99 (call 80) - model.decoder.embed_tokens :: Embedding :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-1.484618 max=0.408362 mean=0.000947 std=0.120420
  Preview: [0.004872, 0.019829, 0.064762, 0.019214, 0.024987, 0.024814, 0.000693, 0.011946, 0.028443, 0.014216, 0.005537, -0.566699, 0.000872, 0.000878, 0.007687, 0.006083, ...]
------------------------------------------------------------
Node 100 (call 81) - model.decoder.rotary_emb :: MoonshineRotaryEmbedding :: output[0]
  Shape: [1,1,32]  Precision: FLOAT32
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, ...]
------------------------------------------------------------
Node 101 (call 81) - model.decoder.rotary_emb :: MoonshineRotaryEmbedding :: output[1]
  Shape: [1,1,32]  Precision: FLOAT32
  Stats: min=0.000000 max=0.000000 mean=0.000000 std=0.000000
  Preview: [0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, ...]
------------------------------------------------------------
Node 102 (call 82) - model.decoder.layers.0.input_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.135571 max=1.396079 mean=0.024525 std=0.321397
  Preview: [0.019871, 0.078903, 0.228516, 0.078273, 0.091792, 0.099843, -0.001252, 0.050807, 0.109827, 0.069799, 0.024213, -1.705974, -0.000343, -0.000376, 0.034958, 0.034797, ...]
------------------------------------------------------------
Node 103 (call 83) - model.decoder.layers.0.self_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.069655 max=3.322939 mean=0.024217 std=1.216284
  Preview: [-0.443647, 1.872825, 1.289198, 2.081692, 0.737177, 1.023980, -0.963331, 0.127345, 0.081408, -0.211538, 1.060192, -2.348042, 1.960960, 0.454271, -1.346165, 0.164287, ...]
------------------------------------------------------------
Node 104 (call 84) - model.decoder.layers.0.self_attn.k_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-5.873570 max=6.579009 mean=-0.105709 std=1.466108
  Preview: [-0.043757, 0.038889, -0.036634, 0.041352, -0.006354, -0.011296, 0.009928, -0.049535, 0.015337, 0.019158, -0.043990, 0.012071, 0.009845, 0.111604, 0.035283, 0.193531, ...]
------------------------------------------------------------
Node 105 (call 85) - model.decoder.layers.0.self_attn.v_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-0.075558 max=0.048674 mean=0.000320 std=0.015943
  Preview: [0.006452, -0.021475, 0.012806, -0.000457, -0.010250, -0.023445, 0.005077, 0.039248, 0.030381, 0.006966, 0.012036, -0.017269, -0.005237, -0.025094, -0.003607, -0.006911, ...]
------------------------------------------------------------
Node 106 (call 86) - model.decoder.layers.0.self_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-0.337900 max=0.117294 mean=0.000001 std=0.025483
  Preview: [0.000678, 0.016308, 0.016810, 0.004502, -0.015827, -0.011354, 0.015615, -0.010402, 0.010377, 0.011891, -0.002410, 0.017582, 0.015272, 0.016242, -0.015279, 0.002499, ...]
------------------------------------------------------------
Node 107 (call 87) - model.decoder.layers.0.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-0.337900 max=0.117294 mean=0.000001 std=0.025483
  Preview: [0.000678, 0.016308, 0.016810, 0.004502, -0.015827, -0.011354, 0.015615, -0.010402, 0.010377, 0.011891, -0.002410, 0.017582, 0.015272, 0.016242, -0.015279, 0.002499, ...]
------------------------------------------------------------
Node 108 (call 87) - model.decoder.layers.0.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,1]  Precision: FLOAT32
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 109 (call 88) - model.decoder.layers.0.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-8.206243 max=2.642032 mean=0.003074 std=0.692873
  Preview: [0.025690, 0.199412, 0.409525, 0.147107, 0.049957, 0.058226, 0.115505, 0.003083, 0.223133, 0.164730, 0.014542, -2.750203, 0.087198, 0.116001, -0.051852, 0.052147, ...]
------------------------------------------------------------
Node 110 (call 89) - model.decoder.layers.0.encoder_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.387999 max=5.883209 mean=0.072343 std=1.066462
  Preview: [0.067056, 0.286449, 0.117082, -1.007199, 0.082359, -0.962201, 1.292394, -0.459260, 0.228615, -0.518751, -0.348880, -0.121645, 0.316516, 0.332093, 0.335595, -0.050116, ...]
------------------------------------------------------------
Node 111 (call 90) - model.decoder.layers.0.encoder_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.528200 max=2.479198 mean=0.114306 std=0.823154
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.206521, 0.266131, 0.167159, 0.916221, -0.828312, 0.148366, 0.072628, -0.815823, 0.545469, 1.776612, 1.439992, 0.281076, 0.890943, 0.050053, -0.157603, 0.429337, ...]
------------------------------------------------------------
Node 112 (call 91) - model.decoder.layers.0.encoder_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-1.061750 max=0.982518 mean=-0.035121 std=0.315888
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.133049, -0.275513, -0.257395, 0.124218, -0.353732, -0.328399, -0.352519, -0.237522, 0.108386, -0.411487, -0.334781, -0.706773, 0.058794, -0.231709, 0.103488, 0.044889, ...]
------------------------------------------------------------
Node 113 (call 92) - model.decoder.layers.0.encoder_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-5.353359 max=0.558868 mean=-0.017859 std=0.359291
  Preview: [0.165077, -0.017588, 0.069048, -0.056439, -0.013641, 0.029563, -0.002354, 0.034151, -0.044450, -0.069294, -0.003067, 0.359349, 0.133998, -0.090455, 0.047840, 0.387473, ...]
------------------------------------------------------------
Node 114 (call 93) - model.decoder.layers.0.encoder_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-5.353359 max=0.558868 mean=-0.017859 std=0.359291
  Preview: [0.165077, -0.017588, 0.069048, -0.056439, -0.013641, 0.029563, -0.002354, 0.034151, -0.044450, -0.069294, -0.003067, 0.359349, 0.133998, -0.090455, 0.047840, 0.387473, ...]
------------------------------------------------------------
Node 115 (call 93) - model.decoder.layers.0.encoder_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,833]  Precision: FLOAT32
  Stats: min=0.000018 max=0.011875 mean=0.001284 std=0.001561
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.001383, 0.001402, 0.001390, 0.001331, 0.001240, 0.001087, 0.000923, 0.000793, 0.000708, 0.000626, 0.000574, 0.000704, 0.000816, 0.001388, 0.001546, 0.000903, ...]
------------------------------------------------------------
Node 116 (call 94) - model.decoder.layers.0.final_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-1.444525 max=0.919843 mean=0.041494 std=0.305972
  Preview: [0.347193, 0.065296, 0.321854, -0.033064, 0.026638, 0.122697, 0.076369, 0.113490, 0.023927, -0.054859, 0.038320, -0.362572, 0.319574, -0.119465, 0.118151, 0.734737, ...]
------------------------------------------------------------
Node 117 (call 95) - model.decoder.layers.0.mlp.fc1 :: Linear :: output
  Shape: [1,1,2304]  Precision: FLOAT32
  Stats: min=-2.899224 max=2.176898 mean=0.006080 std=0.428656
  Note: stats computed on first 1000 of 2304 values
  Preview: [-0.010170, -0.238096, 0.164056, -0.958124, -0.782203, -0.791765, 0.032023, 0.570392, 0.245255, 0.004292, -0.128100, -0.011668, 0.170355, -0.006207, 1.129198, -0.423597, ...]
------------------------------------------------------------
Node 118 (call 96) - model.decoder.layers.0.mlp.activation_fn :: SiLUActivation :: output
  Shape: [1,1,1152]  Precision: FLOAT32
  Stats: min=-0.278465 max=1.626801 mean=-0.075840 std=0.198230
  Note: stats computed on first 1000 of 1152 values
  Preview: [-0.192864, -0.244424, -0.262797, 0.985597, -0.114454, -0.060120, -0.133686, -0.251144, -0.255016, -0.169177, -0.008366, -0.060017, -0.218041, -0.054158, -0.009072, 0.005542, ...]
------------------------------------------------------------
Node 119 (call 97) - model.decoder.layers.0.mlp.fc2 :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-20.184179 max=4.710287 mean=0.007058 std=1.530691
  Preview: [-0.056993, 0.074795, 0.161036, 0.109839, -0.137685, 0.301641, -0.437354, -0.580330, -0.421765, 0.594305, 0.280165, 0.434587, -0.074090, -1.002674, 0.277330, 0.197859, ...]
------------------------------------------------------------
Node 120 (call 98) - model.decoder.layers.0.mlp :: MoonshineDecoderMLP :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-20.184179 max=4.710287 mean=0.007058 std=1.530691
  Preview: [-0.056993, 0.074795, 0.161036, 0.109839, -0.137685, 0.301641, -0.437354, -0.580330, -0.421765, 0.594305, 0.280165, 0.434587, -0.074090, -1.002674, 0.277330, 0.197859, ...]
------------------------------------------------------------
Node 121 (call 99) - model.decoder.layers.0 :: MoonshineDecoderLayer :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-25.851511 max=5.221369 mean=-0.009852 std=1.834134
  Preview: [0.113633, 0.093343, 0.311656, 0.077115, -0.142165, 0.344664, -0.423400, -0.544636, -0.427395, 0.551119, 0.280225, 0.244820, 0.076052, -1.076009, 0.317579, 0.593914, ...]
------------------------------------------------------------
Node 122 (call 100) - model.decoder.layers.1.input_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-7.032995 max=1.295371 mean=0.000237 std=0.492268
  Preview: [0.035346, 0.026519, 0.079518, 0.025878, -0.034581, 0.100624, -0.120690, -0.154189, -0.113971, 0.158354, 0.083055, 0.077515, 0.029858, -0.254566, 0.123011, 0.164051, ...]
------------------------------------------------------------
Node 123 (call 101) - model.decoder.layers.1.self_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-2.148513 max=3.414831 mean=-0.029639 std=0.807213
  Preview: [-0.904260, -0.310016, -0.758445, 0.321094, -1.272700, -0.693998, -0.451743, -1.072504, 1.337942, -1.161715, -0.027995, 0.671673, -0.354713, 0.885095, 0.620142, 2.438873, ...]
------------------------------------------------------------
Node 124 (call 102) - model.decoder.layers.1.self_attn.k_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-9.111243 max=5.110324 mean=-0.325340 std=1.966550
  Preview: [0.178560, 0.117484, 0.156782, -0.208028, 0.344774, -0.037514, 0.283848, 0.075085, 0.408865, -0.116540, -0.496408, 0.668134, -0.145574, 0.427237, 1.965079, 2.698234, ...]
------------------------------------------------------------
Node 125 (call 103) - model.decoder.layers.1.self_attn.v_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-0.563659 max=0.623929 mean=0.009211 std=0.204625
  Preview: [0.286587, 0.069833, 0.330545, -0.091907, -0.004338, -0.068562, -0.013800, 0.140682, 0.076166, -0.098783, -0.225345, -0.087078, 0.014661, 0.178306, -0.094420, -0.374653, ...]
------------------------------------------------------------
Node 126 (call 104) - model.decoder.layers.1.self_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.679847 max=4.374696 mean=0.003646 std=0.539067
  Preview: [-0.286325, -0.011744, -0.279818, -0.217468, 0.463531, 0.523435, -0.667962, -0.106626, -0.008733, 0.357294, 0.010170, 0.924579, -0.431695, -0.763727, -0.518196, 0.230887, ...]
------------------------------------------------------------
Node 127 (call 105) - model.decoder.layers.1.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.679847 max=4.374696 mean=0.003646 std=0.539067
  Preview: [-0.286325, -0.011744, -0.279818, -0.217468, 0.463531, 0.523435, -0.667962, -0.106626, -0.008733, 0.357294, 0.010170, 0.924579, -0.431695, -0.763727, -0.518196, 0.230887, ...]
------------------------------------------------------------
Node 128 (call 105) - model.decoder.layers.1.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,1]  Precision: FLOAT32
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 129 (call 106) - model.decoder.layers.1.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-10.938635 max=1.295633 mean=-0.002217 std=0.740881
  Preview: [-0.066199, 0.031164, 0.013563, -0.053302, 0.131674, 0.397321, -0.332497, -0.269530, -0.176862, 0.345872, 0.093745, 0.379125, -0.145962, -0.690237, -0.073308, 0.316711, ...]
------------------------------------------------------------
Node 130 (call 107) - model.decoder.layers.1.encoder_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.147661 max=4.296784 mean=-0.033339 std=1.002324
  Preview: [-3.002295, -0.445715, 1.465218, 0.235272, -1.774537, 0.101322, 0.682345, -0.308063, 0.151950, -0.164315, -1.509928, -1.443759, -0.837928, -0.873781, 1.142532, -1.597017, ...]
------------------------------------------------------------
Node 131 (call 108) - model.decoder.layers.1.encoder_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.021557 max=5.469244 mean=-0.003791 std=1.154418
  Note: stats computed on first 1000 of 239904 values
  Preview: [-1.046387, -0.370422, -2.346113, -1.197732, -1.103187, -0.706582, 2.656166, 0.642980, 1.598651, -0.885252, -0.494035, -0.774048, 0.298031, -0.286337, 0.650856, 0.570009, ...]
------------------------------------------------------------
Node 132 (call 109) - model.decoder.layers.1.encoder_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-2.267688 max=2.358465 mean=-0.035957 std=0.552207
  Note: stats computed on first 1000 of 239904 values
  Preview: [-1.513606, -0.122522, 0.863317, 0.015236, -0.316334, -0.294853, 0.104540, 0.333239, -0.130077, 0.679311, -0.670638, -0.072840, -0.202890, 0.145623, 0.367398, -0.215231, ...]
------------------------------------------------------------
Node 133 (call 110) - model.decoder.layers.1.encoder_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-2.140797 max=3.139105 mean=0.006293 std=0.462723
  Preview: [-0.193217, 0.131661, -0.622887, 0.102773, 0.236643, 0.068307, 0.147788, 0.245646, 0.341031, -0.694541, -0.368855, -0.107888, 0.154422, 0.332325, -0.240701, 0.248274, ...]
------------------------------------------------------------
Node 134 (call 111) - model.decoder.layers.1.encoder_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-2.140797 max=3.139105 mean=0.006293 std=0.462723
  Preview: [-0.193217, 0.131661, -0.622887, 0.102773, 0.236643, 0.068307, 0.147788, 0.245646, 0.341031, -0.694541, -0.368855, -0.107888, 0.154422, 0.332325, -0.240701, 0.248274, ...]
------------------------------------------------------------
Node 135 (call 111) - model.decoder.layers.1.encoder_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,833]  Precision: FLOAT32
  Stats: min=0.000006 max=0.017489 mean=0.001057 std=0.001490
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.006728, 0.006645, 0.006420, 0.005744, 0.004857, 0.004018, 0.003334, 0.002908, 0.002656, 0.002167, 0.001605, 0.000995, 0.002544, 0.007036, 0.007128, 0.000681, ...]
------------------------------------------------------------
Node 136 (call 112) - model.decoder.layers.1.final_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-2.203865 max=0.905368 mean=0.015600 std=0.257770
  Preview: [-0.080038, 0.044188, -0.134529, -0.008894, 0.134246, 0.192524, -0.189581, -0.096030, -0.022455, 0.046318, -0.018096, 0.246729, -0.045450, -0.313856, -0.108641, 0.208560, ...]
------------------------------------------------------------
Node 137 (call 113) - model.decoder.layers.1.mlp.fc1 :: Linear :: output
  Shape: [1,1,2304]  Precision: FLOAT32
  Stats: min=-4.806587 max=4.947194 mean=-0.010388 std=0.535289
  Note: stats computed on first 1000 of 2304 values
  Preview: [-0.031237, 0.095411, -0.408471, -0.268277, -0.004782, -0.130023, -0.034624, -0.146114, 0.486366, -0.214485, -0.296680, 0.223480, -0.119759, -2.816988, -0.241709, 0.322905, ...]
------------------------------------------------------------
Node 138 (call 114) - model.decoder.layers.1.mlp.activation_fn :: SiLUActivation :: output
  Shape: [1,1,1152]  Precision: FLOAT32
  Stats: min=-0.278459 max=5.835698 mean=0.036906 std=0.465289
  Note: stats computed on first 1000 of 1152 values
  Preview: [-0.030103, -0.218078, 0.566861, -0.158233, 0.039102, -0.150232, -0.267001, -0.270601, 0.098212, 0.007008, -0.185211, -0.060891, -0.086725, 0.246397, -0.211959, -0.005366, ...]
------------------------------------------------------------
Node 139 (call 115) - model.decoder.layers.1.mlp.fc2 :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-217.585556 max=30.053627 mean=-0.123963 std=13.869918
  Preview: [-2.671907, 1.278708, 0.770418, -0.145162, 1.785086, 0.067943, 2.067508, 0.886315, 0.318613, 1.314207, 1.799970, 1.890007, -0.425233, -9.268067, -0.142841, -2.029644, ...]
------------------------------------------------------------
Node 140 (call 116) - model.decoder.layers.1.mlp :: MoonshineDecoderMLP :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-217.585556 max=30.053627 mean=-0.123963 std=13.869918
  Preview: [-2.671907, 1.278708, 0.770418, -0.145162, 1.785086, 0.067943, 2.067508, 0.886315, 0.318613, 1.314207, 1.799970, 1.890007, -0.425233, -9.268067, -0.142841, -2.029644, ...]
------------------------------------------------------------
Node 141 (call 117) - model.decoder.layers.1 :: MoonshineDecoderLayer :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-249.257706 max=32.474297 mean=-0.123876 std=15.904210
  Preview: [-3.037816, 1.491967, 0.179369, -0.182742, 2.343095, 1.004349, 1.123934, 0.480699, 0.223516, 1.528078, 1.721509, 2.951518, -0.626454, -10.775478, -0.584159, -0.956568, ...]
------------------------------------------------------------
Node 142 (call 118) - model.decoder.layers.2.input_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-7.759812 max=1.170620 mean=0.008518 std=0.501538
  Preview: [-0.123385, 0.052782, 0.011522, -0.002743, 0.107809, 0.045766, 0.042802, 0.022614, 0.013101, 0.068066, 0.077198, 0.125981, -0.021220, -0.417654, -0.022689, -0.029333, ...]
------------------------------------------------------------
Node 143 (call 119) - model.decoder.layers.2.self_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-2.298456 max=2.926952 mean=0.025459 std=0.771886
  Preview: [0.561999, -0.736484, 0.185018, 0.283774, -0.189456, -0.135011, -0.175066, 1.740968, 0.365530, 0.604851, 0.461951, -0.291492, -0.641393, 0.444908, 0.672813, 0.621338, ...]
------------------------------------------------------------
Node 144 (call 120) - model.decoder.layers.2.self_attn.k_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.348945 max=11.039693 mean=0.103170 std=1.589160
  Preview: [0.011388, -0.075377, -0.086567, 0.002344, 0.084173, -0.023727, -0.490958, 0.128784, -0.245802, -0.077946, -0.146964, -0.908258, -0.215138, 2.796980, 1.832982, 0.277181, ...]
------------------------------------------------------------
Node 145 (call 121) - model.decoder.layers.2.self_attn.v_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-0.500043 max=0.604049 mean=-0.006433 std=0.130253
  Preview: [-0.043151, 0.002673, -0.022655, -0.103507, -0.087849, -0.233759, -0.129728, -0.166159, -0.067645, -0.003145, -0.176717, -0.085801, 0.013417, 0.090356, -0.013600, -0.292510, ...]
------------------------------------------------------------
Node 146 (call 122) - model.decoder.layers.2.self_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.214202 max=2.554274 mean=0.002762 std=0.386840
  Preview: [0.059715, 0.312231, 0.068232, -0.173203, 0.167883, 0.521165, -0.513265, -0.446965, -0.197807, 0.665097, 0.080814, 0.552113, -0.167989, -0.269679, -0.517549, 0.023918, ...]
------------------------------------------------------------
Node 147 (call 123) - model.decoder.layers.2.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.214202 max=2.554274 mean=0.002762 std=0.386840
  Preview: [0.059715, 0.312231, 0.068232, -0.173203, 0.167883, 0.521165, -0.513265, -0.446965, -0.197807, 0.665097, 0.080814, 0.552113, -0.167989, -0.269679, -0.517549, 0.023918, ...]
------------------------------------------------------------
Node 148 (call 123) - model.decoder.layers.2.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,1]  Precision: FLOAT32
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 149 (call 124) - model.decoder.layers.2.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-10.332359 max=1.650000 mean=0.014941 std=0.669224
  Preview: [-0.146282, 0.102540, 0.019008, -0.013549, 0.153777, 0.108965, 0.040662, 0.008975, 0.009547, 0.145365, 0.109469, 0.182346, -0.038401, -0.614946, -0.060615, -0.042410, ...]
------------------------------------------------------------
Node 150 (call 125) - model.decoder.layers.2.encoder_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.553596 max=2.312725 mean=0.049311 std=0.955857
  Preview: [0.240416, -0.673887, 0.443707, 0.159731, 1.152069, -1.284500, -1.261326, 2.312725, -0.081822, 1.822313, 0.672847, -1.041030, -0.030674, 1.493089, 1.504419, 0.485612, ...]
------------------------------------------------------------
Node 151 (call 126) - model.decoder.layers.2.encoder_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-9.387084 max=7.556967 mean=-0.098317 std=2.354850
  Note: stats computed on first 1000 of 239904 values
  Preview: [-9.317142, 2.186322, -0.275067, -1.380525, -1.150462, -1.964366, -1.590603, 3.186649, -0.359505, -0.496870, 5.980822, -2.530366, -4.593719, -4.632139, 2.636919, -3.641470, ...]
------------------------------------------------------------
Node 152 (call 127) - model.decoder.layers.2.encoder_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-1.434097 max=1.839711 mean=-0.036815 std=0.588418
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.522015, -0.223781, -0.034071, -0.015164, -0.461824, -0.593176, -0.676790, -0.009001, 0.672319, 0.250979, 0.167067, 0.404526, -0.057522, 0.550301, -0.828063, -0.415804, ...]
------------------------------------------------------------
Node 153 (call 128) - model.decoder.layers.2.encoder_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-7.421014 max=7.077030 mean=0.028139 std=0.944110
  Preview: [-0.559577, 0.013676, -0.204493, -0.938456, 0.762464, -0.722376, -0.871375, 0.388082, 0.103687, -0.715186, 0.040029, 0.047793, 0.403768, -2.334679, -1.029735, 1.194335, ...]
------------------------------------------------------------
Node 154 (call 129) - model.decoder.layers.2.encoder_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-7.421014 max=7.077030 mean=0.028139 std=0.944110
  Preview: [-0.559577, 0.013676, -0.204493, -0.938456, 0.762464, -0.722376, -0.871375, 0.388082, 0.103687, -0.715186, 0.040029, 0.047793, 0.403768, -2.334679, -1.029735, 1.194335, ...]
------------------------------------------------------------
Node 155 (call 129) - model.decoder.layers.2.encoder_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,833]  Precision: FLOAT32
  Stats: min=0.000006 max=0.032000 mean=0.001368 std=0.003782
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.022682, 0.022276, 0.021532, 0.020491, 0.019051, 0.017139, 0.015081, 0.013444, 0.012068, 0.010074, 0.008628, 0.008272, 0.013412, 0.019699, 0.018101, 0.008217, ...]
------------------------------------------------------------
Node 156 (call 130) - model.decoder.layers.2.final_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.272654 max=0.879834 mean=0.013625 std=0.253903
  Preview: [-0.100565, 0.054049, 0.003798, -0.036711, 0.103681, 0.021990, -0.004781, 0.016633, 0.006696, 0.044734, 0.051946, 0.104020, -0.009429, -0.365821, -0.060217, 0.007861, ...]
------------------------------------------------------------
Node 157 (call 131) - model.decoder.layers.2.mlp.fc1 :: Linear :: output
  Shape: [1,1,2304]  Precision: FLOAT32
  Stats: min=-1.952690 max=13.219488 mean=0.001962 std=0.559356
  Note: stats computed on first 1000 of 2304 values
  Preview: [0.031749, -0.330023, -0.021655, 0.298830, 0.858575, 0.163193, -1.042963, 0.020164, -1.198454, -0.307066, 0.028372, 0.099203, -0.001451, 0.040046, -0.033049, 0.115192, ...]
------------------------------------------------------------
Node 158 (call 132) - model.decoder.layers.2.mlp.activation_fn :: SiLUActivation :: output
  Shape: [1,1,1152]  Precision: FLOAT32
  Stats: min=-0.278464 max=18.790655 mean=0.024448 std=0.663148
  Note: stats computed on first 1000 of 1152 values
  Preview: [-0.273245, -0.103949, 0.042578, 0.054258, -0.000364, -0.231278, -0.035540, -0.134255, 0.006011, 0.152175, -0.071467, -0.090602, -0.085576, -0.148108, -0.083232, -0.239614, ...]
------------------------------------------------------------
Node 159 (call 133) - model.decoder.layers.2.mlp.fc2 :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-17.423573 max=3.021649 mean=-0.014516 std=1.146753
  Preview: [0.323558, 0.404833, 0.237104, 0.029938, 0.107874, 0.147733, 0.238465, -0.170761, 0.054089, 0.161514, 0.504013, 0.161344, -0.345062, -0.948024, 0.086672, 0.310130, ...]
------------------------------------------------------------
Node 160 (call 134) - model.decoder.layers.2.mlp :: MoonshineDecoderMLP :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-17.423573 max=3.021649 mean=-0.014516 std=1.146753
  Preview: [0.323558, 0.404833, 0.237104, 0.029938, 0.107874, 0.147733, 0.238465, -0.170761, 0.054089, 0.161514, 0.504013, 0.161344, -0.345062, -0.948024, 0.086672, 0.310130, ...]
------------------------------------------------------------
Node 161 (call 135) - model.decoder.layers.2 :: MoonshineDecoderLayer :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-277.316498 max=37.373379 mean=-0.107492 std=17.736355
  Preview: [-3.214118, 2.222708, 0.280212, -1.264462, 3.381316, 0.950869, -0.022241, 0.251055, 0.183486, 1.639504, 2.346365, 3.712768, -0.735738, -14.327861, -2.044772, 0.571815, ...]
------------------------------------------------------------
Node 162 (call 136) - model.decoder.layers.3.input_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-8.017371 max=1.435529 mean=0.018245 std=0.538382
  Preview: [-0.143292, 0.114387, 0.016515, -0.053400, 0.174043, 0.050940, 0.004044, 0.016428, 0.012553, 0.083046, 0.105777, 0.174849, -0.031097, -0.669338, -0.096395, 0.025929, ...]
------------------------------------------------------------
Node 163 (call 137) - model.decoder.layers.3.self_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-1.972655 max=2.425939 mean=0.006478 std=0.739659
  Preview: [-0.441415, 0.562748, -0.496210, 0.766138, -0.284036, -0.876916, -0.052918, -0.269095, 0.609555, 1.019484, 0.499806, -1.254153, 0.158508, -1.449491, 1.048883, 0.503575, ...]
------------------------------------------------------------
Node 164 (call 138) - model.decoder.layers.3.self_attn.k_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.910291 max=5.416154 mean=0.026355 std=1.182949
  Preview: [-0.066137, -0.093385, -0.015377, 0.036720, 0.198160, -0.164011, 0.075709, 0.049361, 0.184961, 0.051610, -0.115505, -0.016431, -0.041008, -0.029990, 0.238238, 0.209606, ...]
------------------------------------------------------------
Node 165 (call 139) - model.decoder.layers.3.self_attn.v_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-0.649085 max=0.697404 mean=0.011386 std=0.168416
  Preview: [0.082940, -0.102117, -0.155553, 0.009566, 0.027690, 0.134537, 0.069570, 0.020136, -0.107501, -0.148237, 0.180153, 0.168373, 0.042631, -0.005789, -0.005809, 0.016518, ...]
------------------------------------------------------------
Node 166 (call 140) - model.decoder.layers.3.self_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.091555 max=5.956142 mean=0.006886 std=0.603261
  Preview: [-0.343551, 0.404617, -0.779895, 0.484509, -0.203985, 0.428817, -0.483235, -0.434198, 0.279088, 0.878111, -0.189423, 1.054621, -0.124605, -0.557298, -0.775801, -0.118125, ...]
------------------------------------------------------------
Node 167 (call 141) - model.decoder.layers.3.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.091555 max=5.956142 mean=0.006886 std=0.603261
  Preview: [-0.343551, 0.404617, -0.779895, 0.484509, -0.203985, 0.428817, -0.483235, -0.434198, 0.279088, 0.878111, -0.189423, 1.054621, -0.124605, -0.557298, -0.775801, -0.118125, ...]
------------------------------------------------------------
Node 168 (call 141) - model.decoder.layers.3.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,1]  Precision: FLOAT32
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 169 (call 142) - model.decoder.layers.3.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-10.900542 max=2.055443 mean=0.013766 std=0.712623
  Preview: [-0.177608, 0.128629, -0.021695, -0.036044, 0.193521, 0.089708, -0.023218, -0.004680, 0.033657, 0.146832, 0.127472, 0.278798, -0.039633, -0.721319, -0.152033, 0.024045, ...]
------------------------------------------------------------
Node 170 (call 143) - model.decoder.layers.3.encoder_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-2.859456 max=3.770736 mean=0.099510 std=1.060882
  Preview: [0.824479, -1.734339, 0.099485, 0.058943, 0.444305, -2.559815, 0.045941, -0.640851, 0.896189, 1.370366, -0.669902, -0.620676, -0.859973, 0.082900, -0.243045, -0.079895, ...]
------------------------------------------------------------
Node 171 (call 144) - model.decoder.layers.3.encoder_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-11.022846 max=11.591249 mean=-0.034846 std=4.112493
  Note: stats computed on first 1000 of 239904 values
  Preview: [7.844736, -9.885495, 2.370598, -3.154067, -0.912456, -7.910452, -4.926422, -3.861459, 2.529502, 6.092581, -4.952181, -10.954901, -1.399451, 2.927900, -5.628325, 0.881281, ...]
------------------------------------------------------------
Node 172 (call 145) - model.decoder.layers.3.encoder_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-2.227301 max=1.796621 mean=-0.039175 std=0.692212
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.628684, -0.103250, -0.492125, -0.187050, -0.561007, 1.337913, 0.051751, -0.225905, 0.598668, 0.013192, -0.485201, -0.507561, -0.987676, 0.601599, -0.778201, -0.793377, ...]
------------------------------------------------------------
Node 173 (call 146) - model.decoder.layers.3.encoder_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.062586 max=21.100077 mean=0.049012 std=1.686667
  Preview: [-0.692047, -0.858127, 0.670897, 0.051364, 1.851254, 0.099280, -0.638519, -0.383787, 1.616023, -0.266715, -1.135662, -0.635902, -1.077222, -0.154249, -0.728827, 2.218507, ...]
------------------------------------------------------------
Node 174 (call 147) - model.decoder.layers.3.encoder_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.062586 max=21.100077 mean=0.049012 std=1.686667
  Preview: [-0.692047, -0.858127, 0.670897, 0.051364, 1.851254, 0.099280, -0.638519, -0.383787, 1.616023, -0.266715, -1.135662, -0.635902, -1.077222, -0.154249, -0.728827, 2.218507, ...]
------------------------------------------------------------
Node 175 (call 147) - model.decoder.layers.3.encoder_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.040810 mean=0.001101 std=0.004786
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.012893, 0.012139, 0.011875, 0.011912, 0.012212, 0.012199, 0.011569, 0.010699, 0.009860, 0.008737, 0.008185, 0.009198, 0.010366, 0.004095, 0.002495, 0.012441, ...]
------------------------------------------------------------
Node 176 (call 148) - model.decoder.layers.3.final_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.328382 max=0.938708 mean=0.014132 std=0.275229
  Preview: [-0.128852, 0.049323, 0.006215, -0.017838, 0.147462, 0.038496, -0.030064, -0.015289, 0.058166, 0.065748, 0.029091, 0.110531, -0.049986, -0.447524, -0.090291, 0.066156, ...]
------------------------------------------------------------
Node 177 (call 149) - model.decoder.layers.3.mlp.fc1 :: Linear :: output
  Shape: [1,1,2304]  Precision: FLOAT32
  Stats: min=-5.751445 max=2.681312 mean=0.003745 std=0.627352
  Note: stats computed on first 1000 of 2304 values
  Preview: [0.163860, -0.000337, 0.119243, 0.178437, 0.589369, -0.435526, -0.581731, 0.024253, -0.207395, 1.017466, -0.088733, 0.187182, 0.768372, 0.293710, -1.112507, 1.503618, ...]
------------------------------------------------------------
Node 178 (call 150) - model.decoder.layers.3.mlp.activation_fn :: SiLUActivation :: output
  Shape: [1,1,1152]  Precision: FLOAT32
  Stats: min=-0.278463 max=5.534267 mean=0.082332 std=0.467162
  Note: stats computed on first 1000 of 1152 values
  Preview: [-0.201134, -0.044832, -0.203076, 0.132036, -0.071277, 0.403257, -0.021335, -0.096087, 0.023774, 0.027596, -0.139765, -0.251029, -0.012307, -0.215971, -0.024863, 0.028299, ...]
------------------------------------------------------------
Node 179 (call 151) - model.decoder.layers.3.mlp.fc2 :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-1.675660 max=1.758051 mean=0.002518 std=0.593030
  Preview: [0.737636, -0.076410, -0.065830, 0.094137, -0.271172, 0.489107, -0.054136, 0.009461, -0.439536, 0.251571, 1.126203, 0.344918, 0.276498, -0.867702, -0.274160, -0.184848, ...]
------------------------------------------------------------
Node 180 (call 152) - model.decoder.layers.3.mlp :: MoonshineDecoderMLP :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-1.675660 max=1.758051 mean=0.002518 std=0.593030
  Preview: [0.737636, -0.076410, -0.065830, 0.094137, -0.271172, 0.489107, -0.054136, 0.009461, -0.439536, 0.251571, 1.126203, 0.344918, 0.276498, -0.867702, -0.274160, -0.184848, ...]
------------------------------------------------------------
Node 181 (call 153) - model.decoder.layers.3 :: MoonshineDecoderLayer :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-285.072845 max=52.406712 mean=-0.049076 std=18.476637
  Preview: [-3.512081, 1.692788, 0.105384, -0.634452, 4.757413, 1.968073, -1.198132, -0.557469, 1.639060, 2.502470, 2.147482, 4.476405, -1.661068, -15.907109, -3.823559, 2.487349, ...]
------------------------------------------------------------
Node 182 (call 154) - model.decoder.layers.4.input_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-8.286160 max=1.657168 mean=0.012546 std=0.558981
  Preview: [-0.162959, 0.072550, 0.005632, -0.024889, 0.198535, 0.080397, -0.051155, -0.020552, 0.069338, 0.092362, 0.099561, 0.168842, -0.067113, -0.670515, -0.171236, 0.089512, ...]
------------------------------------------------------------
Node 183 (call 155) - model.decoder.layers.4.self_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-2.632121 max=3.069416 mean=0.010681 std=0.831216
  Preview: [0.150743, -0.723253, 0.385751, 0.961549, 0.050328, -0.837753, -0.057244, 0.531223, 0.375986, -0.273741, 0.269750, 0.512821, -0.051963, 0.324023, -1.710259, -0.304741, ...]
------------------------------------------------------------
Node 184 (call 156) - model.decoder.layers.4.self_attn.k_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.841141 max=3.991257 mean=-0.037380 std=1.148463
  Preview: [-0.113759, -0.019603, 0.046478, -0.048418, -0.047670, -0.036447, 0.186331, 0.033769, -0.026402, -0.235933, 0.066845, -0.036448, 0.209394, 0.048509, -0.120553, -0.250703, ...]
------------------------------------------------------------
Node 185 (call 157) - model.decoder.layers.4.self_attn.v_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-0.864232 max=0.618314 mean=-0.015630 std=0.210904
  Preview: [-0.031094, 0.079283, 0.253996, 0.239555, 0.151994, -0.059929, -0.117791, 0.121539, 0.240180, -0.269198, -0.028481, -0.227151, 0.079352, 0.221373, 0.375803, -0.001687, ...]
------------------------------------------------------------
Node 186 (call 158) - model.decoder.layers.4.self_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.330384 max=14.923233 mean=0.007634 std=1.127059
  Preview: [-0.295958, 0.724766, -1.033686, 0.897043, -0.676362, 1.091961, -0.807414, 0.106797, -0.146905, 0.989570, 0.105228, 0.560338, -0.049663, -0.487522, -0.974387, 0.192653, ...]
------------------------------------------------------------
Node 187 (call 159) - model.decoder.layers.4.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.330384 max=14.923233 mean=0.007634 std=1.127059
  Preview: [-0.295958, 0.724766, -1.033686, 0.897043, -0.676362, 1.091961, -0.807414, 0.106797, -0.146905, 0.989570, 0.105228, 0.560338, -0.049663, -0.487522, -0.974387, 0.192653, ...]
------------------------------------------------------------
Node 188 (call 159) - model.decoder.layers.4.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,1]  Precision: FLOAT32
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 189 (call 160) - model.decoder.layers.4.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-9.752034 max=2.592777 mean=0.018390 std=0.677545
  Preview: [-0.167753, 0.115204, -0.048855, 0.016231, 0.227629, 0.170060, -0.100335, -0.018700, 0.082302, 0.205557, 0.122414, 0.263172, -0.081010, -0.740101, -0.237554, 0.133539, ...]
------------------------------------------------------------
Node 190 (call 161) - model.decoder.layers.4.encoder_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.376779 max=3.524985 mean=-0.013002 std=1.153686
  Preview: [-0.422400, -1.062721, -0.021805, -0.522471, -1.336260, -0.394301, -0.540244, -2.099150, -0.676912, -1.909371, -0.573552, -0.831364, -0.803564, -0.925021, -1.038800, -0.787564, ...]
------------------------------------------------------------
Node 191 (call 162) - model.decoder.layers.4.encoder_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-11.389568 max=12.581007 mean=-0.563348 std=4.797197
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.507047, -2.166147, 1.263912, -8.276997, -0.944438, -3.311570, -2.421981, -5.672021, -0.395937, -5.759307, -8.653313, 0.352330, 3.196682, 4.611023, 4.372237, 6.334618, ...]
------------------------------------------------------------
Node 192 (call 163) - model.decoder.layers.4.encoder_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.417264 max=2.695800 mean=0.072559 std=0.780755
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.084636, 0.012769, 0.275173, -0.868508, 0.073665, -0.508540, -1.805522, -1.005085, 0.032048, 0.472151, -0.146109, 0.616407, -0.292015, -0.220420, -0.375869, -0.247893, ...]
------------------------------------------------------------
Node 193 (call 164) - model.decoder.layers.4.encoder_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-10.347695 max=66.881241 mean=0.166795 std=5.462747
  Preview: [2.160949, 0.494306, 3.886524, 6.372116, 3.229258, 7.102355, 0.948391, 7.721804, -5.444531, -5.357726, 4.072127, -2.461297, -0.840686, 6.186868, 0.122841, 2.546684, ...]
------------------------------------------------------------
Node 194 (call 165) - model.decoder.layers.4.encoder_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-10.347695 max=66.881241 mean=0.166795 std=5.462747
  Preview: [2.160949, 0.494306, 3.886524, 6.372116, 3.229258, 7.102355, 0.948391, 7.721804, -5.444531, -5.357726, 4.072127, -2.461297, -0.840686, 6.186868, 0.122841, 2.546684, ...]
------------------------------------------------------------
Node 195 (call 165) - model.decoder.layers.4.encoder_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.130316 mean=0.001964 std=0.009376
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.026255, 0.025914, 0.025499, 0.024782, 0.023181, 0.020748, 0.018068, 0.016322, 0.015297, 0.014372, 0.014793, 0.017694, 0.018083, 0.021206, 0.020153, 0.026400, ...]
------------------------------------------------------------
Node 196 (call 166) - model.decoder.layers.4.final_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.300158 max=1.949729 mean=0.003100 std=0.368426
  Preview: [-0.053277, 0.084456, 0.083705, 0.178840, 0.200238, 0.245927, -0.037299, 0.228470, -0.120346, -0.061470, 0.155800, 0.070942, -0.079273, -0.314883, -0.121304, 0.138865, ...]
------------------------------------------------------------
Node 197 (call 167) - model.decoder.layers.4.mlp.fc1 :: Linear :: output
  Shape: [1,1,2304]  Precision: FLOAT32
  Stats: min=-2.964643 max=3.048249 mean=-0.002669 std=0.789460
  Note: stats computed on first 1000 of 2304 values
  Preview: [0.391078, -0.627396, -0.257206, 0.523752, 0.614040, 0.286047, -1.646034, -0.539791, -0.039231, 0.914311, -0.163476, 0.647072, 0.380072, 0.146172, -0.579105, -0.056136, ...]
------------------------------------------------------------
Node 198 (call 168) - model.decoder.layers.4.mlp.activation_fn :: SiLUActivation :: output
  Shape: [1,1,1152]  Precision: FLOAT32
  Stats: min=-0.278454 max=8.351476 mean=0.186981 std=0.566675
  Note: stats computed on first 1000 of 1152 values
  Preview: [0.866152, -0.169736, -0.084823, -0.031843, 0.391288, -0.194822, -0.003912, -0.061359, 0.159947, -0.223360, 0.203971, 0.402530, -0.132607, 0.120105, -0.091278, 0.326229, ...]
------------------------------------------------------------
Node 199 (call 169) - model.decoder.layers.4.mlp.fc2 :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-22.658047 max=11.316574 mean=-0.025863 std=2.218218
  Preview: [1.437318, -2.126808, -0.904172, -1.097558, -0.134850, 2.158915, 3.379393, 0.522771, -1.133624, -1.297069, 1.046465, 0.901174, -0.852509, 0.259482, -0.237460, -0.516491, ...]
------------------------------------------------------------
Node 200 (call 170) - model.decoder.layers.4.mlp :: MoonshineDecoderMLP :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-22.658047 max=11.316574 mean=-0.025863 std=2.218218
  Preview: [1.437318, -2.126808, -0.904172, -1.097558, -0.134850, 2.158915, 3.379393, 0.522771, -1.133624, -1.297069, 1.046465, 0.901174, -0.852509, 0.259482, -0.237460, -0.516491, ...]
------------------------------------------------------------
Node 201 (call 171) - model.decoder.layers.4 :: MoonshineDecoderLayer :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-292.786469 max=145.527756 mean=0.099490 std=21.192312
  Preview: [-0.209773, 0.785052, 2.054050, 5.537149, 7.175459, 12.321303, 2.322238, 7.793903, -5.085999, -3.162755, 7.371303, 3.476619, -3.403926, -9.948280, -4.912565, 4.710194, ...]
------------------------------------------------------------
Node 202 (call 172) - model.decoder.layers.5.input_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-7.016648 max=3.798140 mean=0.007211 std=0.552924
  Preview: [-0.011660, 0.025176, 0.069390, 0.207685, 0.227538, 0.348585, 0.090942, 0.251928, -0.193518, -0.106718, 0.245926, 0.101802, -0.121177, -0.392905, -0.188555, 0.158790, ...]
------------------------------------------------------------
Node 203 (call 173) - model.decoder.layers.5.self_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.777845 max=4.614859 mean=-0.005931 std=1.091508
  Preview: [-0.833872, 0.166580, -0.046192, -1.085451, -0.803971, 0.710525, -1.194815, -0.355789, -0.128867, -0.661177, -0.339106, 0.718160, -0.495387, -0.293850, -0.969940, 0.750517, ...]
------------------------------------------------------------
Node 204 (call 174) - model.decoder.layers.5.self_attn.k_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-5.818910 max=5.042671 mean=0.090744 std=1.598065
  Preview: [-0.047225, -0.118471, 0.295123, 0.165441, 0.023007, -0.280409, 0.133093, -0.224763, 0.035539, -0.171781, -0.152943, 0.159049, 0.668024, -0.070356, -0.961180, -0.389784, ...]
------------------------------------------------------------
Node 205 (call 175) - model.decoder.layers.5.self_attn.v_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-2.296765 max=1.570116 mean=0.032456 std=0.483685
  Preview: [-0.068396, 0.068243, 0.208476, 0.242101, -0.269043, 0.025827, 0.086088, 0.097385, -0.145928, 0.061385, 0.126103, -0.027160, -0.212962, -0.064516, -0.166074, -0.013534, ...]
------------------------------------------------------------
Node 206 (call 176) - model.decoder.layers.5.self_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-9.212767 max=58.684540 mean=0.097454 std=4.068989
  Preview: [1.562745, -0.967823, -1.525611, 0.106660, -0.177904, 0.449506, 1.549420, 0.966372, -1.862699, -1.031875, 0.668520, -0.610074, -1.581350, 2.773266, -0.885036, 1.317789, ...]
------------------------------------------------------------
Node 207 (call 177) - model.decoder.layers.5.self_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-9.212767 max=58.684540 mean=0.097454 std=4.068989
  Preview: [1.562745, -0.967823, -1.525611, 0.106660, -0.177904, 0.449506, 1.549420, 0.966372, -1.862699, -1.031875, 0.668520, -0.610074, -1.581350, 2.773266, -0.885036, 1.317789, ...]
------------------------------------------------------------
Node 208 (call 177) - model.decoder.layers.5.self_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,1]  Precision: FLOAT32
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 209 (call 178) - model.decoder.layers.5.post_attention_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-6.335145 max=5.938777 mean=0.012913 std=0.618826
  Preview: [0.040340, -0.015669, 0.015802, 0.237905, 0.387719, 0.537060, 0.160770, 0.342730, -0.326704, -0.245620, 0.427444, 0.095969, -0.236979, -0.321341, -0.223729, 0.253073, ...]
------------------------------------------------------------
Node 210 (call 179) - model.decoder.layers.5.encoder_attn.q_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-3.689582 max=2.768862 mean=-0.039353 std=1.147174
  Preview: [1.045380, -0.032354, -2.200941, 0.119666, -0.993436, 1.284565, -0.118421, 0.214245, 0.469757, 1.721874, 0.068058, -0.896606, -3.689582, 1.472786, 2.301310, 0.194630, ...]
------------------------------------------------------------
Node 211 (call 180) - model.decoder.layers.5.encoder_attn.k_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-13.822814 max=11.878078 mean=0.277508 std=4.596016
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.745197, 2.452033, 0.279947, 3.522686, -3.804674, -0.568849, -7.950832, 1.197015, 10.283094, 7.677633, -0.395080, 1.131051, -0.787304, 3.500204, 0.499386, 1.904269, ...]
------------------------------------------------------------
Node 212 (call 181) - model.decoder.layers.5.encoder_attn.v_proj :: Linear :: output
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-2.853751 max=2.847141 mean=-0.060727 std=0.904415
  Note: stats computed on first 1000 of 239904 values
  Preview: [-0.979656, -0.589914, -0.950026, -0.411975, -2.197617, -0.803769, -0.541201, -0.279490, -0.069801, -0.050932, 0.569789, 0.005585, -0.734873, 0.402280, -1.075054, -0.670145, ...]
------------------------------------------------------------
Node 213 (call 182) - model.decoder.layers.5.encoder_attn.o_proj :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-25.622732 max=108.303841 mean=0.259264 std=11.338031
  Preview: [1.173680, 9.566771, -5.154660, 12.368049, 4.956704, 6.659907, -5.681972, 5.252526, -2.800107, 8.807108, 2.002168, -11.225492, 9.374965, 4.083322, -7.605827, -0.394304, ...]
------------------------------------------------------------
Node 214 (call 183) - model.decoder.layers.5.encoder_attn :: MoonshineAttention :: output[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-25.622732 max=108.303841 mean=0.259264 std=11.338031
  Preview: [1.173680, 9.566771, -5.154660, 12.368049, 4.956704, 6.659907, -5.681972, 5.252526, -2.800107, 8.807108, 2.002168, -11.225492, 9.374965, 4.083322, -7.605827, -0.394304, ...]
------------------------------------------------------------
Node 215 (call 183) - model.decoder.layers.5.encoder_attn :: MoonshineAttention :: output[1]
  Shape: [1,8,1,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.119815 mean=0.001948 std=0.009547
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.006005, 0.005941, 0.005907, 0.005932, 0.005878, 0.005698, 0.005368, 0.005057, 0.004794, 0.004621, 0.004496, 0.003900, 0.004736, 0.005168, 0.004435, 0.003508, ...]
------------------------------------------------------------
Node 216 (call 184) - model.decoder.layers.5.final_layernorm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-2.211678 max=4.766237 mean=-0.007129 std=0.457753
  Preview: [0.061488, 0.296001, -0.162521, 0.532359, 0.349978, 0.587964, -0.076313, 0.408386, -0.316129, 0.131314, 0.271157, -0.234978, 0.112194, -0.112526, -0.397588, 0.146720, ...]
------------------------------------------------------------
Node 217 (call 185) - model.decoder.layers.5.mlp.fc1 :: Linear :: output
  Shape: [1,1,2304]  Precision: FLOAT32
  Stats: min=-4.238381 max=19.582312 mean=0.017591 std=1.494618
  Note: stats computed on first 1000 of 2304 values
  Preview: [2.142191, -0.574787, -4.238381, -1.446915, -1.786880, -0.358324, 0.299628, 1.635725, -1.362127, 0.641640, 0.553881, -1.889983, 0.718290, -1.180731, -1.187899, 1.404282, ...]
------------------------------------------------------------
Node 218 (call 186) - model.decoder.layers.5.mlp.activation_fn :: SiLUActivation :: output
  Shape: [1,1,1152]  Precision: FLOAT32
  Stats: min=-0.278464 max=4.053687 mean=0.144988 std=0.528229
  Note: stats computed on first 1000 of 1152 values
  Preview: [-0.111240, -0.108353, -0.118686, 0.466934, 0.032231, -0.203671, -0.087621, -0.109718, -0.243668, 0.152135, -0.132395, 0.034957, -0.256084, 0.082367, -0.195091, 0.366664, ...]
------------------------------------------------------------
Node 219 (call 187) - model.decoder.layers.5.mlp.fc2 :: Linear :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-13.755094 max=28.773335 mean=0.142903 std=5.120593
  Preview: [-4.597238, 5.087211, -2.871425, -6.333598, -7.944027, 3.790870, 3.183702, -4.808059, -5.588728, -1.207505, 2.817116, 0.530064, -1.106305, -0.519381, 1.759082, 15.527998, ...]
------------------------------------------------------------
Node 220 (call 188) - model.decoder.layers.5.mlp :: MoonshineDecoderMLP :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-13.755094 max=28.773335 mean=0.142903 std=5.120593
  Preview: [-4.597238, 5.087211, -2.871425, -6.333598, -7.944027, 3.790870, 3.183702, -4.808059, -5.588728, -1.207505, 2.817116, 0.530064, -1.106305, -0.519381, 1.759082, 15.527998, ...]
------------------------------------------------------------
Node 221 (call 189) - model.decoder.layers.5 :: MoonshineDecoderLayer :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-97.024750 max=281.266113 mean=0.599111 std=21.093794
  Preview: [-2.070586, 14.471210, -7.497645, 11.678260, 4.010233, 23.221586, 1.373389, 9.204742, -15.337533, 3.404973, 12.859106, -7.828882, 3.283384, -3.611074, -11.644346, 21.161676, ...]
------------------------------------------------------------
Node 222 (call 190) - model.decoder.norm :: LayerNorm :: output
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.735965 max=2.314783 mean=-0.058501 std=0.892523
  Preview: [-0.204960, 1.076454, -0.596379, 0.833116, 0.240400, 1.562286, 0.071194, 0.667390, -1.243413, 0.205377, 0.897278, -0.620338, 0.212628, -0.292663, -0.871694, 1.047183, ...]
------------------------------------------------------------
Node 223 (call 191) - model.decoder :: MoonshineDecoder :: output.last_hidden_state
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.735965 max=2.314783 mean=-0.058501 std=0.892523
  Preview: [-0.204960, 1.076454, -0.596379, 0.833116, 0.240400, 1.562286, 0.071194, 0.667390, -1.243413, 0.205377, 0.897278, -0.620338, 0.212628, -0.292663, -0.871694, 1.047183, ...]
------------------------------------------------------------
Node 224 (call 191) - model.decoder :: MoonshineDecoder :: output.hidden_states[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-1.484618 max=0.408362 mean=0.000947 std=0.120420
  Preview: [0.004872, 0.019829, 0.064762, 0.019214, 0.024987, 0.024814, 0.000693, 0.011946, 0.028443, 0.014216, 0.005537, -0.566699, 0.000872, 0.000878, 0.007687, 0.006083, ...]
------------------------------------------------------------
Node 225 (call 191) - model.decoder :: MoonshineDecoder :: output.hidden_states[1]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-25.851511 max=5.221369 mean=-0.009852 std=1.834134
  Preview: [0.113633, 0.093343, 0.311656, 0.077115, -0.142165, 0.344664, -0.423400, -0.544636, -0.427395, 0.551119, 0.280225, 0.244820, 0.076052, -1.076009, 0.317579, 0.593914, ...]
------------------------------------------------------------
Node 226 (call 191) - model.decoder :: MoonshineDecoder :: output.hidden_states[2]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-249.257706 max=32.474297 mean=-0.123876 std=15.904210
  Preview: [-3.037816, 1.491967, 0.179369, -0.182742, 2.343095, 1.004349, 1.123934, 0.480699, 0.223516, 1.528078, 1.721509, 2.951518, -0.626454, -10.775478, -0.584159, -0.956568, ...]
------------------------------------------------------------
Node 227 (call 191) - model.decoder :: MoonshineDecoder :: output.hidden_states[3]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-277.316498 max=37.373379 mean=-0.107492 std=17.736355
  Preview: [-3.214118, 2.222708, 0.280212, -1.264462, 3.381316, 0.950869, -0.022241, 0.251055, 0.183486, 1.639504, 2.346365, 3.712768, -0.735738, -14.327861, -2.044772, 0.571815, ...]
------------------------------------------------------------
Node 228 (call 191) - model.decoder :: MoonshineDecoder :: output.hidden_states[4]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-285.072845 max=52.406712 mean=-0.049076 std=18.476637
  Preview: [-3.512081, 1.692788, 0.105384, -0.634452, 4.757413, 1.968073, -1.198132, -0.557469, 1.639060, 2.502470, 2.147482, 4.476405, -1.661068, -15.907109, -3.823559, 2.487349, ...]
------------------------------------------------------------
Node 229 (call 191) - model.decoder :: MoonshineDecoder :: output.hidden_states[5]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-292.786469 max=145.527756 mean=0.099490 std=21.192312
  Preview: [-0.209773, 0.785052, 2.054050, 5.537149, 7.175459, 12.321303, 2.322238, 7.793903, -5.085999, -3.162755, 7.371303, 3.476619, -3.403926, -9.948280, -4.912565, 4.710194, ...]
------------------------------------------------------------
Node 230 (call 191) - model.decoder :: MoonshineDecoder :: output.hidden_states[6]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.735965 max=2.314783 mean=-0.058501 std=0.892523
  Preview: [-0.204960, 1.076454, -0.596379, 0.833116, 0.240400, 1.562286, 0.071194, 0.667390, -1.243413, 0.205377, 0.897278, -0.620338, 0.212628, -0.292663, -0.871694, 1.047183, ...]
------------------------------------------------------------
Node 231 (call 191) - model.decoder :: MoonshineDecoder :: output.attentions[0]
  Shape: [1,8,1,1]  Precision: FLOAT32
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 232 (call 191) - model.decoder :: MoonshineDecoder :: output.attentions[1]
  Shape: [1,8,1,1]  Precision: FLOAT32
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 233 (call 191) - model.decoder :: MoonshineDecoder :: output.attentions[2]
  Shape: [1,8,1,1]  Precision: FLOAT32
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 234 (call 191) - model.decoder :: MoonshineDecoder :: output.attentions[3]
  Shape: [1,8,1,1]  Precision: FLOAT32
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 235 (call 191) - model.decoder :: MoonshineDecoder :: output.attentions[4]
  Shape: [1,8,1,1]  Precision: FLOAT32
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 236 (call 191) - model.decoder :: MoonshineDecoder :: output.attentions[5]
  Shape: [1,8,1,1]  Precision: FLOAT32
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 237 (call 191) - model.decoder :: MoonshineDecoder :: output.cross_attentions[0]
  Shape: [1,8,1,833]  Precision: FLOAT32
  Stats: min=0.000018 max=0.011875 mean=0.001284 std=0.001561
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.001383, 0.001402, 0.001390, 0.001331, 0.001240, 0.001087, 0.000923, 0.000793, 0.000708, 0.000626, 0.000574, 0.000704, 0.000816, 0.001388, 0.001546, 0.000903, ...]
------------------------------------------------------------
Node 238 (call 191) - model.decoder :: MoonshineDecoder :: output.cross_attentions[1]
  Shape: [1,8,1,833]  Precision: FLOAT32
  Stats: min=0.000006 max=0.017489 mean=0.001057 std=0.001490
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.006728, 0.006645, 0.006420, 0.005744, 0.004857, 0.004018, 0.003334, 0.002908, 0.002656, 0.002167, 0.001605, 0.000995, 0.002544, 0.007036, 0.007128, 0.000681, ...]
------------------------------------------------------------
Node 239 (call 191) - model.decoder :: MoonshineDecoder :: output.cross_attentions[2]
  Shape: [1,8,1,833]  Precision: FLOAT32
  Stats: min=0.000006 max=0.032000 mean=0.001368 std=0.003782
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.022682, 0.022276, 0.021532, 0.020491, 0.019051, 0.017139, 0.015081, 0.013444, 0.012068, 0.010074, 0.008628, 0.008272, 0.013412, 0.019699, 0.018101, 0.008217, ...]
------------------------------------------------------------
Node 240 (call 191) - model.decoder :: MoonshineDecoder :: output.cross_attentions[3]
  Shape: [1,8,1,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.040810 mean=0.001101 std=0.004786
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.012893, 0.012139, 0.011875, 0.011912, 0.012212, 0.012199, 0.011569, 0.010699, 0.009860, 0.008737, 0.008185, 0.009198, 0.010366, 0.004095, 0.002495, 0.012441, ...]
------------------------------------------------------------
Node 241 (call 191) - model.decoder :: MoonshineDecoder :: output.cross_attentions[4]
  Shape: [1,8,1,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.130316 mean=0.001964 std=0.009376
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.026255, 0.025914, 0.025499, 0.024782, 0.023181, 0.020748, 0.018068, 0.016322, 0.015297, 0.014372, 0.014793, 0.017694, 0.018083, 0.021206, 0.020153, 0.026400, ...]
------------------------------------------------------------
Node 242 (call 191) - model.decoder :: MoonshineDecoder :: output.cross_attentions[5]
  Shape: [1,8,1,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.119815 mean=0.001948 std=0.009547
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.006005, 0.005941, 0.005907, 0.005932, 0.005878, 0.005698, 0.005368, 0.005057, 0.004794, 0.004621, 0.004496, 0.003900, 0.004736, 0.005168, 0.004435, 0.003508, ...]
------------------------------------------------------------
Node 243 (call 192) - model :: MoonshineModel :: output.last_hidden_state
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.735965 max=2.314783 mean=-0.058501 std=0.892523
  Preview: [-0.204960, 1.076454, -0.596379, 0.833116, 0.240400, 1.562286, 0.071194, 0.667390, -1.243413, 0.205377, 0.897278, -0.620338, 0.212628, -0.292663, -0.871694, 1.047183, ...]
------------------------------------------------------------
Node 244 (call 192) - model :: MoonshineModel :: output.decoder_hidden_states[0]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-1.484618 max=0.408362 mean=0.000947 std=0.120420
  Preview: [0.004872, 0.019829, 0.064762, 0.019214, 0.024987, 0.024814, 0.000693, 0.011946, 0.028443, 0.014216, 0.005537, -0.566699, 0.000872, 0.000878, 0.007687, 0.006083, ...]
------------------------------------------------------------
Node 245 (call 192) - model :: MoonshineModel :: output.decoder_hidden_states[1]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-25.851511 max=5.221369 mean=-0.009852 std=1.834134
  Preview: [0.113633, 0.093343, 0.311656, 0.077115, -0.142165, 0.344664, -0.423400, -0.544636, -0.427395, 0.551119, 0.280225, 0.244820, 0.076052, -1.076009, 0.317579, 0.593914, ...]
------------------------------------------------------------
Node 246 (call 192) - model :: MoonshineModel :: output.decoder_hidden_states[2]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-249.257706 max=32.474297 mean=-0.123876 std=15.904210
  Preview: [-3.037816, 1.491967, 0.179369, -0.182742, 2.343095, 1.004349, 1.123934, 0.480699, 0.223516, 1.528078, 1.721509, 2.951518, -0.626454, -10.775478, -0.584159, -0.956568, ...]
------------------------------------------------------------
Node 247 (call 192) - model :: MoonshineModel :: output.decoder_hidden_states[3]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-277.316498 max=37.373379 mean=-0.107492 std=17.736355
  Preview: [-3.214118, 2.222708, 0.280212, -1.264462, 3.381316, 0.950869, -0.022241, 0.251055, 0.183486, 1.639504, 2.346365, 3.712768, -0.735738, -14.327861, -2.044772, 0.571815, ...]
------------------------------------------------------------
Node 248 (call 192) - model :: MoonshineModel :: output.decoder_hidden_states[4]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-285.072845 max=52.406712 mean=-0.049076 std=18.476637
  Preview: [-3.512081, 1.692788, 0.105384, -0.634452, 4.757413, 1.968073, -1.198132, -0.557469, 1.639060, 2.502470, 2.147482, 4.476405, -1.661068, -15.907109, -3.823559, 2.487349, ...]
------------------------------------------------------------
Node 249 (call 192) - model :: MoonshineModel :: output.decoder_hidden_states[5]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-292.786469 max=145.527756 mean=0.099490 std=21.192312
  Preview: [-0.209773, 0.785052, 2.054050, 5.537149, 7.175459, 12.321303, 2.322238, 7.793903, -5.085999, -3.162755, 7.371303, 3.476619, -3.403926, -9.948280, -4.912565, 4.710194, ...]
------------------------------------------------------------
Node 250 (call 192) - model :: MoonshineModel :: output.decoder_hidden_states[6]
  Shape: [1,1,288]  Precision: FLOAT32
  Stats: min=-4.735965 max=2.314783 mean=-0.058501 std=0.892523
  Preview: [-0.204960, 1.076454, -0.596379, 0.833116, 0.240400, 1.562286, 0.071194, 0.667390, -1.243413, 0.205377, 0.897278, -0.620338, 0.212628, -0.292663, -0.871694, 1.047183, ...]
------------------------------------------------------------
Node 251 (call 192) - model :: MoonshineModel :: output.decoder_attentions[0]
  Shape: [1,8,1,1]  Precision: FLOAT32
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 252 (call 192) - model :: MoonshineModel :: output.decoder_attentions[1]
  Shape: [1,8,1,1]  Precision: FLOAT32
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 253 (call 192) - model :: MoonshineModel :: output.decoder_attentions[2]
  Shape: [1,8,1,1]  Precision: FLOAT32
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 254 (call 192) - model :: MoonshineModel :: output.decoder_attentions[3]
  Shape: [1,8,1,1]  Precision: FLOAT32
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 255 (call 192) - model :: MoonshineModel :: output.decoder_attentions[4]
  Shape: [1,8,1,1]  Precision: FLOAT32
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 256 (call 192) - model :: MoonshineModel :: output.decoder_attentions[5]
  Shape: [1,8,1,1]  Precision: FLOAT32
  Stats: min=1.000000 max=1.000000 mean=1.000000 std=0.000000
  Preview: [1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000]
------------------------------------------------------------
Node 257 (call 192) - model :: MoonshineModel :: output.cross_attentions[0]
  Shape: [1,8,1,833]  Precision: FLOAT32
  Stats: min=0.000018 max=0.011875 mean=0.001284 std=0.001561
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.001383, 0.001402, 0.001390, 0.001331, 0.001240, 0.001087, 0.000923, 0.000793, 0.000708, 0.000626, 0.000574, 0.000704, 0.000816, 0.001388, 0.001546, 0.000903, ...]
------------------------------------------------------------
Node 258 (call 192) - model :: MoonshineModel :: output.cross_attentions[1]
  Shape: [1,8,1,833]  Precision: FLOAT32
  Stats: min=0.000006 max=0.017489 mean=0.001057 std=0.001490
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.006728, 0.006645, 0.006420, 0.005744, 0.004857, 0.004018, 0.003334, 0.002908, 0.002656, 0.002167, 0.001605, 0.000995, 0.002544, 0.007036, 0.007128, 0.000681, ...]
------------------------------------------------------------
Node 259 (call 192) - model :: MoonshineModel :: output.cross_attentions[2]
  Shape: [1,8,1,833]  Precision: FLOAT32
  Stats: min=0.000006 max=0.032000 mean=0.001368 std=0.003782
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.022682, 0.022276, 0.021532, 0.020491, 0.019051, 0.017139, 0.015081, 0.013444, 0.012068, 0.010074, 0.008628, 0.008272, 0.013412, 0.019699, 0.018101, 0.008217, ...]
------------------------------------------------------------
Node 260 (call 192) - model :: MoonshineModel :: output.cross_attentions[3]
  Shape: [1,8,1,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.040810 mean=0.001101 std=0.004786
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.012893, 0.012139, 0.011875, 0.011912, 0.012212, 0.012199, 0.011569, 0.010699, 0.009860, 0.008737, 0.008185, 0.009198, 0.010366, 0.004095, 0.002495, 0.012441, ...]
------------------------------------------------------------
Node 261 (call 192) - model :: MoonshineModel :: output.cross_attentions[4]
  Shape: [1,8,1,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.130316 mean=0.001964 std=0.009376
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.026255, 0.025914, 0.025499, 0.024782, 0.023181, 0.020748, 0.018068, 0.016322, 0.015297, 0.014372, 0.014793, 0.017694, 0.018083, 0.021206, 0.020153, 0.026400, ...]
------------------------------------------------------------
Node 262 (call 192) - model :: MoonshineModel :: output.cross_attentions[5]
  Shape: [1,8,1,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.119815 mean=0.001948 std=0.009547
  Note: stats computed on first 1000 of 6664 values
  Preview: [0.006005, 0.005941, 0.005907, 0.005932, 0.005878, 0.005698, 0.005368, 0.005057, 0.004794, 0.004621, 0.004496, 0.003900, 0.004736, 0.005168, 0.004435, 0.003508, ...]
------------------------------------------------------------
Node 263 (call 192) - model :: MoonshineModel :: output.encoder_last_hidden_state
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.701851 max=4.225048 mean=0.029268 std=0.487575
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.180410, -0.194983, 0.163126, 0.162613, -1.183519, 0.366340, -0.022310, 0.021833, -0.057942, 0.387811, -0.012669, -0.082833, 0.152838, -0.176949, 0.085384, 0.049317, ...]
------------------------------------------------------------
Node 264 (call 192) - model :: MoonshineModel :: output.encoder_hidden_states[0]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-0.169931 max=17.944090 mean=0.736144 std=1.061540
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.249802, 1.098746, 0.815923, 0.549584, 0.711983, 0.537123, 0.040814, 0.941720, 0.152236, 1.142886, 0.348016, 1.096730, 0.880705, 1.114160, -0.169931, 0.751759, ...]
------------------------------------------------------------
Node 265 (call 192) - model :: MoonshineModel :: output.encoder_hidden_states[1]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-17.224686 max=44.700859 mean=0.672309 std=3.330945
  Note: stats computed on first 1000 of 239904 values
  Preview: [1.919789, 0.044347, -0.182905, 0.294989, -0.409678, -1.187226, 0.026538, -1.403607, 3.172574, -0.779909, -1.073636, 0.003678, 0.151031, -0.381490, 0.967268, -0.453869, ...]
------------------------------------------------------------
Node 266 (call 192) - model :: MoonshineModel :: output.encoder_hidden_states[2]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-52.424725 max=121.588928 mean=0.623509 std=8.783118
  Note: stats computed on first 1000 of 239904 values
  Preview: [3.954081, 0.175849, -4.568871, 6.564666, -9.646864, -2.427885, 0.165132, -4.236034, -0.675834, 1.266754, -1.268244, 3.563315, 5.568336, -7.807683, 0.259346, -1.616696, ...]
------------------------------------------------------------
Node 267 (call 192) - model :: MoonshineModel :: output.encoder_hidden_states[3]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-90.951782 max=206.255829 mean=0.631912 std=14.625962
  Note: stats computed on first 1000 of 239904 values
  Preview: [8.205666, 0.348083, -4.513798, 13.445332, -20.487366, 1.840679, -2.053669, 1.179065, 3.203657, -0.647474, -1.080943, 0.794459, 5.361691, -16.456264, -1.961069, 2.921722, ...]
------------------------------------------------------------
Node 268 (call 192) - model :: MoonshineModel :: output.encoder_hidden_states[4]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-119.795982 max=237.744293 mean=0.651004 std=19.008587
  Note: stats computed on first 1000 of 239904 values
  Preview: [2.728045, 0.095876, -1.609455, 23.524672, -25.013927, 2.984807, -11.047768, 4.367102, 1.830327, -1.881685, 5.895497, 5.678263, 4.056883, -21.924953, -3.333667, 11.888998, ...]
------------------------------------------------------------
Node 269 (call 192) - model :: MoonshineModel :: output.encoder_hidden_states[5]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-177.130890 max=279.572205 mean=0.379514 std=25.222710
  Note: stats computed on first 1000 of 239904 values
  Preview: [7.295281, -3.896157, 3.397039, 16.844292, -41.323105, 13.485542, -6.974017, 4.415526, 4.013231, 6.786592, -7.227048, 2.539574, 0.499406, -11.958514, 0.078474, 6.914792, ...]
------------------------------------------------------------
Node 270 (call 192) - model :: MoonshineModel :: output.encoder_hidden_states[6]
  Shape: [1,833,288]  Precision: FLOAT32
  Stats: min=-3.701851 max=4.225048 mean=0.029268 std=0.487575
  Note: stats computed on first 1000 of 239904 values
  Preview: [0.180410, -0.194983, 0.163126, 0.162613, -1.183519, 0.366340, -0.022310, 0.021833, -0.057942, 0.387811, -0.012669, -0.082833, 0.152838, -0.176949, 0.085384, 0.049317, ...]
------------------------------------------------------------
Node 271 (call 192) - model :: MoonshineModel :: output.encoder_attentions[0]
  Shape: [1,8,833,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.837600 mean=0.001929 std=0.027456
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.206417, 0.001823, 0.000143, 0.000421, 0.006926, 0.023363, 0.003455, 0.000116, 0.000023, 0.000143, 0.005992, 0.048824, 0.007898, 0.000046, 0.000001, 0.000002, ...]
------------------------------------------------------------
Node 272 (call 192) - model :: MoonshineModel :: output.encoder_attentions[1]
  Shape: [1,8,833,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.088339 mean=0.001842 std=0.006838
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.088339, 0.065523, 0.044084, 0.034436, 0.029763, 0.024873, 0.017679, 0.009979, 0.006033, 0.004165, 0.003947, 0.004073, 0.002893, 0.001316, 0.007295, 0.001946, ...]
------------------------------------------------------------
Node 273 (call 192) - model :: MoonshineModel :: output.encoder_attentions[2]
  Shape: [1,8,833,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.040443 mean=0.001987 std=0.005571
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.017569, 0.021672, 0.026987, 0.031813, 0.036181, 0.039707, 0.040443, 0.036341, 0.028950, 0.022327, 0.019708, 0.023282, 0.027502, 0.013863, 0.022641, 0.018133, ...]
------------------------------------------------------------
Node 274 (call 192) - model :: MoonshineModel :: output.encoder_attentions[3]
  Shape: [1,8,833,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.083949 mean=0.001963 std=0.007324
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.024036, 0.051044, 0.083751, 0.083949, 0.058506, 0.038361, 0.030023, 0.027527, 0.025139, 0.019248, 0.012609, 0.008372, 0.006696, 0.002881, 0.004792, 0.013203, ...]
------------------------------------------------------------
Node 275 (call 192) - model :: MoonshineModel :: output.encoder_attentions[4]
  Shape: [1,8,833,833]  Precision: FLOAT32
  Stats: min=0.000004 max=0.029645 mean=0.001199 std=0.002607
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.021287, 0.016123, 0.014071, 0.013301, 0.012963, 0.013161, 0.014118, 0.014687, 0.013328, 0.009408, 0.005139, 0.002332, 0.001407, 0.002840, 0.004298, 0.001381, ...]
------------------------------------------------------------
Node 276 (call 192) - model :: MoonshineModel :: output.encoder_attentions[5]
  Shape: [1,8,833,833]  Precision: FLOAT32
  Stats: min=0.000000 max=0.036430 mean=0.001003 std=0.004184
  Note: stats computed on first 1000 of 5551112 values
  Preview: [0.000105, 0.000106, 0.000109, 0.000108, 0.000102, 0.000093, 0.000082, 0.000077, 0.000072, 0.000068, 0.000058, 0.000046, 0.000014, 0.000002, 0.000001, 0.000031, ...]
------------------------------------------------------------
Node 277 (call 193) - proj_out :: Linear :: output
  Shape: [1,1,32768]  Precision: FLOAT32
  Stats: min=-16.367403 max=9.771976 mean=-3.532688 std=4.745609
  Note: stats computed on first 1000 of 32768 values
  Preview: [-16.367403, 3.175433, 0.455280, -8.236913, -8.039354, -7.960202, -8.034778, -8.276382, -7.807444, -8.215767, -8.198943, -8.238002, -8.138678, -8.293653, -8.085893, -8.260392, ...]
