[Cactus][layer_148] input_tokens shape=[7] showing 7/7 values: 1, 36309, 1033, 856, 17299, 521, 730
[Cactus][global] embedding_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0.00550461,0,-0.0027523,0.0027523,-0.0192719,0,0.00550461,0.0027523]
  [tok1: 0.0137634,-0.0110092,0.0137634,0.0302887,0.0302887,-0.0247803,0.0110092,-0.0247803]
  [tok2: -0.0137634,0,0.0110092,-0.00550461,0.01651,0.0137634,-0.0192719,0.0027523]
  [tok3: -0.0137634,0.03302,0.0027523,0.0275269,0.0137634,0.03302,0.008255,0.0192719]
  [tok4: -0.0220184,-0.0110092,0.0220184,0.0110092,-0.0220184,0.01651,-0.03302,-0.0220184]
  [tok5: 0.00550461,0.0467834,0.0027523,0.01651,0.0110092,0.008255,-0.0412903,0.03302]
  [tok6: 0.00550461,0.0412903,0.0192719,0.0550537,0.03302,0.0137634,-0.0302887,0.03302]
[Cactus][layer_0] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0.145264,0,-0.0748291,0.0797119,-0.539551,0,0.139893,0.0819092]
  [tok1: 0.180298,-0.147827,0.185791,0.435791,0.421143,-0.312744,0.139038,-0.366211]
  [tok2: -0.192505,0,0.158813,-0.0845337,0.245117,0.185547,-0.259766,0.043457]
  [tok3: -0.20166,0.495361,0.0415344,0.442627,0.213867,0.46582,0.116455,0.318359]
  [tok4: -0.332764,-0.17041,0.343018,0.182617,-0.353027,0.240479,-0.480957,-0.375488]
  [tok5: 0.0665283,0.579102,0.0343018,0.218994,0.141235,0.0961304,-0.480957,0.450195]
  [tok6: 0.0612793,0.470703,0.221069,0.672363,0.389893,0.147583,-0.324951,0.414551]
[Cactus][layer_0] conv_in_proj shape=[7, 3072] per-token (first 8 dims):
  [tok0: -0.0552673,0.144897,0.281982,-0.513672,0.0662842,-0.0332947,0.0428467,0.385986]
  [tok1: 0.0882568,-0.462158,0.529785,-0.0859375,0.198486,0.0499878,-0.538574,0.0443726]
  [tok2: -0.093689,-0.0259552,0.509277,0.1203,0.0387878,0.419922,0.225952,-0.119812]
  [tok3: 0.00492096,0.0384827,0.158813,-0.110535,-0.116333,0.0587463,-0.10553,-0.0303955]
  [tok4: -0.190918,-0.461426,0.127197,0.399902,0.0584717,0.44043,-0.376953,-0.13501]
  [tok5: 0.050354,0.11615,-0.0772095,-0.206665,-0.0194855,0.0427246,0.0548096,-0.0505676]
  [tok6: 0.0748901,0.0572205,-0.176636,-0.000386,-0.083252,0.081543,0.0064621,0.172729]
[Cactus][layer_0] conv_triplet shape=[7, 3, 1024] showing 8/21504 values: -0.0552673, 0.144897, 0.281982, -0.513672, 0.0662842, -0.0332947, 0.0428467, 0.385986 ...
[Cactus][layer_0] conv_B_sliced shape=[7, 1, 1024] showing 8/7168 values: -0.0552673, 0.144897, 0.281982, -0.513672, 0.0662842, -0.0332947, 0.0428467, 0.385986 ...
[Cactus][layer_0] conv_B shape=[7, 1024] per-token (first 8 dims):
  [tok0: -0.0552673,0.144897,0.281982,-0.513672,0.0662842,-0.0332947,0.0428467,0.385986]
  [tok1: 0.0882568,-0.462158,0.529785,-0.0859375,0.198486,0.0499878,-0.538574,0.0443726]
  [tok2: -0.093689,-0.0259552,0.509277,0.1203,0.0387878,0.419922,0.225952,-0.119812]
  [tok3: 0.00492096,0.0384827,0.158813,-0.110535,-0.116333,0.0587463,-0.10553,-0.0303955]
  [tok4: -0.190918,-0.461426,0.127197,0.399902,0.0584717,0.44043,-0.376953,-0.13501]
  [tok5: 0.050354,0.11615,-0.0772095,-0.206665,-0.0194855,0.0427246,0.0548096,-0.0505676]
  [tok6: 0.0748901,0.0572205,-0.176636,-0.000386,-0.083252,0.081543,0.0064621,0.172729]
[Cactus][layer_0] conv_C shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0.0804443,-0.067749,-0.188477,-0.09375,-0.417725,0.175659,0.343262,0.0597229]
  [tok1: 0.226562,-0.0401306,0.335938,0.285645,-0.168701,-0.227905,0.0582581,0.235352]
  [tok2: 0.217407,-0.256104,0.102173,-0.0586548,-0.109985,0.168579,0.638184,0.176514]
  [tok3: 0.271729,0.03125,0.0733032,0.0441895,0.0791016,-0.266113,0.00385857,-0.402832]
  [tok4: 0.0305786,-0.0563354,-0.221191,-0.234985,0.191162,0.23584,-0.0755615,-0.0699463]
  [tok5: 0.0376282,-0.0224762,-0.10553,0.039856,0.0413818,-0.0980225,0.00694656,0.140747]
  [tok6: -0.15918,-0.00868225,-0.127686,0.102539,-0.00260544,-0.177368,0.282959,-0.129883]
[Cactus][layer_0] conv_X shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0.165039,0.129761,0.140137,-0.246704,0.211426,-0.0245056,0.201294,-0.275146]
  [tok1: -0.225342,-0.448242,0.140503,0.177246,0.152344,0.251953,-0.0662842,-0.259277]
  [tok2: -0.168701,-0.0305786,-0.181641,0.257812,0.403076,0.112793,0.154419,0.0684204]
  [tok3: 0.109985,0.0133133,-0.0618286,-0.00086832,-0.0375366,0.0524902,0.143311,-0.029129]
  [tok4: 0.140747,-0.00858307,-0.254883,0.134644,0.498535,-0.0703125,-0.407471,0.0328979]
  [tok5: -0.162964,0.0571289,0.051239,0.106384,0.0437927,-0.119507,0.0430298,-0.0454407]
  [tok6: 0.0211334,0.167358,0.00501633,-0.061554,-0.100159,0.115784,-0.0551758,0.234375]
[Cactus][layer_0] conv_bx shape=[7, 1024] per-token (first 8 dims):
  [tok0: -0.00912476,0.0187988,0.0395203,0.126709,0.0140152,0.000815868,0.00862122,-0.106201]
  [tok1: -0.0198822,0.207153,0.0744629,-0.0152283,0.0302429,0.0125961,0.0357056,-0.0115051]
  [tok2: 0.0158081,0.000793457,-0.0925293,0.0310211,0.0156403,0.0473633,0.0348816,-0.00819397]
  [tok3: 0.00054121,0.000512123,-0.00981903,9.59635e-05,0.00436783,0.00308418,-0.0151215,0.000885487]
  [tok4: -0.0268707,0.00395966,-0.0324097,0.053833,0.0291443,-0.0309753,0.153564,-0.00444031]
  [tok5: -0.00820923,0.00663376,-0.00395584,-0.0219879,-0.000853539,-0.00510406,0.00235939,0.00229836]
  [tok6: 0.0015831,0.00957489,-0.000885963,2.37823e-05,0.00833893,0.00943756,-0.000356436,0.0404968]
[Cactus][layer_0] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.0112574, 0.188561, 0, 0.0197004, 0.166046, -0.00844304, 0.132274, 0.0591013 ...
[Cactus][layer_0] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 4, 67, 0, 7, 59, -3, 47, 21 ...
[Cactus][layer_0] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 4, 67, 0
[Cactus][layer_0] conv_input_concat shape=[7, 1024] per-token (first 8 dims):
  [tok0: -0.00912476,0.0187988,0.0395203,0.126709,0.0140152,0.000815868,0.00862122,-0.106201]
  [tok1: -0.0198822,0.207153,0.0744629,-0.0152283,0.0302429,0.0125961,0.0357056,-0.0115051]
  [tok2: 0.0158081,0.000793457,-0.0925293,0.0310211,0.0156403,0.0473633,0.0348816,-0.00819397]
  [tok3: 0.00054121,0.000512123,-0.00981903,9.59635e-05,0.00436783,0.00308418,-0.0151215,0.000885487]
  [tok4: -0.0268707,0.00395966,-0.0324097,0.053833,0.0291443,-0.0309753,0.153564,-0.00444031]
  [tok5: -0.00820923,0.00663376,-0.00395584,-0.0219879,-0.000853539,-0.00510406,0.00235939,0.00229836]
  [tok6: 0.0015831,0.00957489,-0.000885963,2.37823e-05,0.00833893,0.00943756,-0.000356436,0.0404968]
[Cactus][layer_0] conv_input_NLC shape=[1, 7, 1024] showing 8/7168 values: -0.00912476, 0.0187988, 0.0395203, 0.126709, 0.0140152, 0.000815868, 0.00862122, -0.106201 ...
[Cactus][layer_0] conv_output_NLC shape=[1, 7, 1024] showing 8/7168 values: -nan, 1.07288e-06, 1.66893e-06, -nan, 2.563e-06, 5.96046e-08, 0.00133419, -nan ...
[Cactus][layer_0] conv_output_LC shape=[7, 1024] per-token (first 8 dims):
  [tok0: -nan,1.07288e-06,1.66893e-06,-nan,2.563e-06,5.96046e-08,0.00133419,-nan]
  [tok1: -nan,-nan,3.15905e-06,-nan,5.54323e-06,-nan,-494.25,-nan]
  [tok2: -nan,-nan,-0.196167,-nan,-0.55957,-nan,-2048,-nan]
  [tok3: -nan,-nan,-0.369629,-nan,-1.20801,-nan,-2000,-nan]
  [tok4: -nan,-nan,0.459473,-nan,-0.624512,-nan,867,-nan]
  [tok5: -nan,-nan,0.0487366,-nan,-0.174438,-nan,-8808,-nan]
  [tok6: -nan,-nan,0.160889,-nan,-1.16406,-nan,-136.125,-nan]
[Cactus][layer_0] conv_gated_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: -nan,-5.96046e-08,-2.98023e-07,-nan,-1.07288e-06,0,0.000458002,-nan]
  [tok1: -nan,-nan,1.07288e-06,-nan,-9.53674e-07,-nan,-28.7969,-nan]
  [tok2: -nan,-nan,-0.02005,-nan,0.061554,-nan,-1307,-nan]
  [tok3: -nan,-nan,-0.0270996,-nan,-0.0955811,-nan,-7.71875,-nan]
  [tok4: -nan,-nan,-0.101624,-nan,-0.119385,-nan,-65.5,-nan]
  [tok5: -nan,-nan,-0.00514221,-nan,-0.00721741,-nan,-61.1875,-nan]
  [tok6: -nan,-nan,-0.0205383,-nan,0.00303268,-nan,-38.5312,-nan]
[Cactus][layer_0] conv_out_proj_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_0] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_0] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_0] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_0] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_0] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_0] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_0] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_0] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_0] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_0] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_in_proj shape=[7, 3072] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_triplet shape=[7, 3, 1024] showing 8/21504 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_B_sliced shape=[7, 1, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_B shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_C shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_X shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_bx shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.0507505, 0.115034, -0.0152251, -0.0186085, -0.19116, -0.0101501, -0.0219919, -0.167477 ...
[Cactus][layer_1] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 30, 68, -9, -11, -113, -6, -13, -99 ...
[Cactus][layer_1] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 30, 68, -9
[Cactus][layer_1] conv_input_concat shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_input_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_output_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, -nan, nan, nan, -nan ...
[Cactus][layer_1] conv_output_LC shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,-nan,nan,nan,-nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,-nan,nan,nan,-nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,-nan,nan,nan,-nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,-nan,nan,nan,-nan]
[Cactus][layer_1] conv_gated_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_out_proj_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_1] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_1] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] attn_q_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] attn_k_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] attn_v_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] attn_q_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] attn_k_norm shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] attn_q_pre_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_pre_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_v shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_q_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_scores_output shape=[1, 7, 16, 64] showing 8/7168 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_2] attn_output_flat shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_2] attn_out_proj shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_2] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_2] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_2] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_2] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_in_proj shape=[7, 3072] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_triplet shape=[7, 3, 1024] showing 8/21504 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_B_sliced shape=[7, 1, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_B shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_C shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_X shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_bx shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.0122724, 0.020454, -0.051135, -0.0245448, -0.0634074, 0.0756797, -0.0122724, -0.0245448 ...
[Cactus][layer_3] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 6, 10, -25, -12, -31, 37, -6, -12 ...
[Cactus][layer_3] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 6, 10, -25
[Cactus][layer_3] conv_input_concat shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_input_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_output_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_output_LC shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_gated_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_out_proj_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_3] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_3] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_in_proj shape=[7, 3072] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_triplet shape=[7, 3, 1024] showing 8/21504 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_B_sliced shape=[7, 1, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_B shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_C shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_X shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_bx shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.051135, 0.0756797, 0.0327264, -0.0490896, -0.0818159, -0.0818159, 0.0224994, 0.0777251 ...
[Cactus][layer_4] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 25, 37, 16, -24, -40, -40, 11, 38 ...
[Cactus][layer_4] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 25, 37, 16
[Cactus][layer_4] conv_input_concat shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_input_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_output_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, -nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_output_LC shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,-nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,-nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,-nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,-nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_gated_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_out_proj_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_4] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_4] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] attn_q_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] attn_k_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] attn_v_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] attn_q_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] attn_k_norm shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] attn_q_pre_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_pre_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_v shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_q_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_scores_output shape=[1, 7, 16, 64] showing 8/7168 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_5] attn_output_flat shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_5] attn_out_proj shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_5] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_5] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_5] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_5] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_in_proj shape=[7, 3072] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_triplet shape=[7, 3, 1024] showing 8/21504 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_B_sliced shape=[7, 1, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_B shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_C shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_X shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_bx shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: -0.00650529, -0.015179, -0.160464, 0.0346949, 0.0498739, 0.119264, -0.00867372, -0.0390317 ...
[Cactus][layer_6] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: -3, -7, -74, 16, 23, 55, -4, -18 ...
[Cactus][layer_6] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: -3, -7, -74
[Cactus][layer_6] conv_input_concat shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_input_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_output_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_output_LC shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_gated_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_out_proj_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_6] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_6] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_in_proj shape=[7, 3072] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_triplet shape=[7, 3, 1024] showing 8/21504 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_B_sliced shape=[7, 1, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_B shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_C shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_X shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_bx shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.0226686, 0.0226686, -0.0453371, 0.0494587, 0.0886134, -0.0721272, 0.0226686, 0.055641 ...
[Cactus][layer_7] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 11, 11, -22, 24, 43, -35, 11, 27 ...
[Cactus][layer_7] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 11, 11, -22
[Cactus][layer_7] conv_input_concat shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_input_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_output_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_output_LC shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_gated_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_out_proj_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_7] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_7] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_7] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_7] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] attn_q_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] attn_k_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] attn_v_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] attn_q_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] attn_k_norm shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] attn_q_pre_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_pre_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_v shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_q_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_scores_output shape=[1, 7, 16, 64] showing 8/7168 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_8] attn_output_flat shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_8] attn_out_proj shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_8] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_8] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_8] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_8] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_in_proj shape=[7, 3072] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_triplet shape=[7, 3, 1024] showing 8/21504 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_B_sliced shape=[7, 1, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_B shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_C shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_X shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_bx shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.0183009, 0.0261442, -0.0444451, 0.0209154, 0.0522884, 0.177781, 0.0156865, 0.0261442 ...
[Cactus][layer_9] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 7, 10, -17, 8, 20, 68, 6, 10 ...
[Cactus][layer_9] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 7, 10, -17
[Cactus][layer_9] conv_input_concat shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_input_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_output_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_output_LC shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_gated_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_out_proj_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_9] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_9] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] attn_q_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] attn_k_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] attn_v_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] attn_q_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] attn_k_norm shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] attn_q_pre_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_pre_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_v shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_q_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_scores_output shape=[1, 7, 16, 64] showing 8/7168 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_10] attn_output_flat shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_10] attn_out_proj shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_10] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_10] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_10] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_10] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_in_proj shape=[7, 3072] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_triplet shape=[7, 3, 1024] showing 8/21504 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_B_sliced shape=[7, 1, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_B shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_C shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_X shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_bx shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: -0.0196543, -0.0327571, -0.157234, 0.0131029, 0.04586, -0.0808009, -0.0218381, -0.0436762 ...
[Cactus][layer_11] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: -9, -15, -72, 6, 21, -37, -10, -20 ...
[Cactus][layer_11] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: -9, -15, -72
[Cactus][layer_11] conv_input_concat shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_input_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_output_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_output_LC shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_gated_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_out_proj_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_11] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_11] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] attn_q_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] attn_k_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] attn_v_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] attn_q_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] attn_k_norm shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] attn_q_pre_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_pre_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_v shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_q_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_scores_output shape=[1, 7, 16, 64] showing 8/7168 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_12] attn_output_flat shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_12] attn_out_proj shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_12] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_12] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_12] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_12] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_in_proj shape=[7, 3072] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_triplet shape=[7, 3, 1024] showing 8/21504 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_B_sliced shape=[7, 1, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_B shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_C shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_X shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_bx shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: -0.00799705, 0.00599779, -0.229915, 0.00199926, 0.00399852, 0.163939, -0.0219919, 0.0959646 ...
[Cactus][layer_13] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: -4, 3, -115, 1, 2, 82, -11, 48 ...
[Cactus][layer_13] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: -4, 3, -115
[Cactus][layer_13] conv_input_concat shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_input_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_output_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_output_LC shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_gated_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_out_proj_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_13] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_13] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] attn_q_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] attn_k_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] attn_v_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] attn_q_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] attn_k_norm shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] attn_q_pre_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_pre_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_v shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_q_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_scores_output shape=[1, 7, 16, 64] showing 8/7168 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_14] attn_output_flat shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_14] attn_out_proj shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_14] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_14] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_14] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_14] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_in_proj shape=[7, 3072] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_triplet shape=[7, 3, 1024] showing 8/21504 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_B_sliced shape=[7, 1, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_B shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_C shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_X shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_bx shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.00239911, 0.00959646, 0.194328, -0.00479823, -0.00719734, -0.0863681, -0.00719734, -0.0191929 ...
[Cactus][layer_15] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 1, 4, 81, -2, -3, -36, -3, -8 ...
[Cactus][layer_15] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 1, 4, 81
[Cactus][layer_15] conv_input_concat shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_input_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_output_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_output_LC shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_gated_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_out_proj_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_15] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_15] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][global] final_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
Final hidden tensor has 7168 values (seq_len=7, hidden_dim=1024)
Per-token hidden summaries (showing 7 of 7 prefilling token(s)):
  token 0 id=1 text="<|startoftext|>" | min=nan max=nan mean=nan std=0
    values: nan nan nan nan nan nan nan nan ...
  token 1 id=36309 text="Hello" | min=nan max=nan mean=nan std=0
    values: nan nan nan nan nan nan nan nan ...
  token 2 id=1033 text=" this" | min=nan max=nan mean=nan std=0
    values: nan nan nan nan nan nan nan nan ...
  token 3 id=856 text=" is" | min=nan max=nan mean=nan std=0
    values: nan nan nan nan nan nan nan nan ...
  token 4 id=17299 text=" Patrick" | min=nan max=nan mean=nan std=0
    values: nan nan nan nan nan nan nan nan ...
  token 5 id=521 text="," | min=nan max=nan mean=nan std=0
    values: nan nan nan nan nan nan nan nan ...
  token 6 id=730 text=" " | min=nan max=nan mean=nan std=0
    values: nan nan nan nan nan nan nan nan ...
Layer dumps respect CACTUS_DEBUG_ENABLE, CACTUS_DEBUG_STDOUT, CACTUS_DEBUG_DIR, and related env vars.
Starting autoregressive generation of 6 token(s).
[Cactus][layer_148] input_tokens shape=[7] showing 7/7 values: 1, 36309, 1033, 856, 17299, 521, 730
[Cactus][global] embedding_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0.0055046,0,-0.0027523,0.0027523,-0.019272,0,0.0055046,0.0027523]
  [tok1: 0.013763,-0.011009,0.013763,0.030289,0.030289,-0.02478,0.011009,-0.02478]
  [tok2: -0.013763,0,0.011009,-0.0055046,0.01651,0.013763,-0.019272,0.0027523]
  [tok3: -0.013763,0.03302,0.0027523,0.027527,0.013763,0.03302,0.008255,0.019272]
  [tok4: -0.022018,-0.011009,0.022018,0.011009,-0.022018,0.01651,-0.03302,-0.022018]
  [tok5: 0.0055046,0.046783,0.0027523,0.01651,0.011009,0.008255,-0.04129,0.03302]
  [tok6: 0.0055046,0.04129,0.019272,0.055054,0.03302,0.013763,-0.030289,0.03302]
[Cactus][layer_0] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0.14526,0,-0.074829,0.079712,-0.53955,0,0.13989,0.081909]
  [tok1: 0.1803,-0.14783,0.18579,0.43579,0.42114,-0.31274,0.13904,-0.36621]
  [tok2: -0.1925,0,0.15881,-0.084534,0.24512,0.18555,-0.25977,0.043457]
  [tok3: -0.20166,0.49536,0.041534,0.44263,0.21387,0.46582,0.11646,0.31836]
  [tok4: -0.33276,-0.17041,0.34302,0.18262,-0.35303,0.24048,-0.48096,-0.37549]
  [tok5: 0.066528,0.5791,0.034302,0.21899,0.14124,0.09613,-0.48096,0.4502]
  [tok6: 0.061279,0.4707,0.22107,0.67236,0.38989,0.14758,-0.32495,0.41455]
[Cactus][layer_0] conv_in_proj shape=[7, 3072] per-token (first 8 dims):
  [tok0: -0.055267,0.1449,0.28198,-0.51367,0.066284,-0.033295,0.042847,0.38599]
  [tok1: 0.088257,-0.46216,0.52979,-0.085938,0.19849,0.049988,-0.53857,0.044373]
  [tok2: -0.093689,-0.025955,0.50928,0.1203,0.038788,0.41992,0.22595,-0.11981]
  [tok3: 0.004921,0.038483,0.15881,-0.11053,-0.11633,0.058746,-0.10553,-0.030396]
  [tok4: -0.19092,-0.46143,0.1272,0.3999,0.058472,0.44043,-0.37695,-0.13501]
  [tok5: 0.050354,0.11615,-0.077209,-0.20667,-0.019485,0.042725,0.05481,-0.050568]
  [tok6: 0.07489,0.05722,-0.17664,-0.000386,-0.083252,0.081543,0.0064621,0.17273]
[Cactus][layer_0] conv_triplet shape=[7, 3, 1024] showing 8/21504 values: -0.055267, 0.1449, 0.28198, -0.51367, 0.066284, -0.033295, 0.042847, 0.38599 ...
[Cactus][layer_0] conv_B_sliced shape=[7, 1, 1024] showing 8/7168 values: -0.055267, 0.1449, 0.28198, -0.51367, 0.066284, -0.033295, 0.042847, 0.38599 ...
[Cactus][layer_0] conv_B shape=[7, 1024] per-token (first 8 dims):
  [tok0: -0.055267,0.1449,0.28198,-0.51367,0.066284,-0.033295,0.042847,0.38599]
  [tok1: 0.088257,-0.46216,0.52979,-0.085938,0.19849,0.049988,-0.53857,0.044373]
  [tok2: -0.093689,-0.025955,0.50928,0.1203,0.038788,0.41992,0.22595,-0.11981]
  [tok3: 0.004921,0.038483,0.15881,-0.11053,-0.11633,0.058746,-0.10553,-0.030396]
  [tok4: -0.19092,-0.46143,0.1272,0.3999,0.058472,0.44043,-0.37695,-0.13501]
  [tok5: 0.050354,0.11615,-0.077209,-0.20667,-0.019485,0.042725,0.05481,-0.050568]
  [tok6: 0.07489,0.05722,-0.17664,-0.000386,-0.083252,0.081543,0.0064621,0.17273]
[Cactus][layer_0] conv_C shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0.080444,-0.067749,-0.18848,-0.09375,-0.41772,0.17566,0.34326,0.059723]
  [tok1: 0.22656,-0.040131,0.33594,0.28564,-0.1687,-0.22791,0.058258,0.23535]
  [tok2: 0.21741,-0.2561,0.10217,-0.058655,-0.10999,0.16858,0.63818,0.17651]
  [tok3: 0.27173,0.03125,0.073303,0.044189,0.079102,-0.26611,0.0038586,-0.40283]
  [tok4: 0.030579,-0.056335,-0.22119,-0.23499,0.19116,0.23584,-0.075562,-0.069946]
  [tok5: 0.037628,-0.022476,-0.10553,0.039856,0.041382,-0.098022,0.0069466,0.14075]
  [tok6: -0.15918,-0.0086823,-0.12769,0.10254,-0.0026054,-0.17737,0.28296,-0.12988]
[Cactus][layer_0] conv_X shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0.16504,0.12976,0.14014,-0.2467,0.21143,-0.024506,0.20129,-0.27515]
  [tok1: -0.22534,-0.44824,0.1405,0.17725,0.15234,0.25195,-0.066284,-0.25928]
  [tok2: -0.1687,-0.030579,-0.18164,0.25781,0.40308,0.11279,0.15442,0.06842]
  [tok3: 0.10999,0.013313,-0.061829,-0.00086832,-0.037537,0.05249,0.14331,-0.029129]
  [tok4: 0.14075,-0.0085831,-0.25488,0.13464,0.49854,-0.070312,-0.40747,0.032898]
  [tok5: -0.16296,0.057129,0.051239,0.10638,0.043793,-0.11951,0.04303,-0.045441]
  [tok6: 0.021133,0.16736,0.0050163,-0.061554,-0.10016,0.11578,-0.055176,0.23438]
[Cactus][layer_0] conv_bx shape=[7, 1024] per-token (first 8 dims):
  [tok0: -0.0091248,0.018799,0.03952,0.12671,0.014015,0.00081587,0.0086212,-0.1062]
  [tok1: -0.019882,0.20715,0.074463,-0.015228,0.030243,0.012596,0.035706,-0.011505]
  [tok2: 0.015808,0.00079346,-0.092529,0.031021,0.01564,0.047363,0.034882,-0.008194]
  [tok3: 0.00054121,0.00051212,-0.009819,9.5963e-05,0.0043678,0.0030842,-0.015121,0.00088549]
  [tok4: -0.026871,0.0039597,-0.03241,0.053833,0.029144,-0.030975,0.15356,-0.0044403]
  [tok5: -0.0082092,0.0066338,-0.0039558,-0.021988,-0.00085354,-0.0051041,0.0023594,0.0022984]
  [tok6: 0.0015831,0.0095749,-0.00088596,2.3782e-05,0.0083389,0.0094376,-0.00035644,0.040497]
[Cactus][layer_0] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.011257, 0.18856, 0, 0.0197, 0.16605, -0.008443, 0.13227, 0.059101 ...
[Cactus][layer_0] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 4, 67, 0, 7, 59, -3, 47, 21 ...
[Cactus][layer_0] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 4, 67, 0
[Cactus][layer_0] conv_input_concat shape=[7, 1024] per-token (first 8 dims):
  [tok0: -0.0091248,0.018799,0.03952,0.12671,0.014015,0.00081587,0.0086212,-0.1062]
  [tok1: -0.019882,0.20715,0.074463,-0.015228,0.030243,0.012596,0.035706,-0.011505]
  [tok2: 0.015808,0.00079346,-0.092529,0.031021,0.01564,0.047363,0.034882,-0.008194]
  [tok3: 0.00054121,0.00051212,-0.009819,9.5963e-05,0.0043678,0.0030842,-0.015121,0.00088549]
  [tok4: -0.026871,0.0039597,-0.03241,0.053833,0.029144,-0.030975,0.15356,-0.0044403]
  [tok5: -0.0082092,0.0066338,-0.0039558,-0.021988,-0.00085354,-0.0051041,0.0023594,0.0022984]
  [tok6: 0.0015831,0.0095749,-0.00088596,2.3782e-05,0.0083389,0.0094376,-0.00035644,0.040497]
[Cactus][layer_0] conv_input_NLC shape=[1, 7, 1024] showing 8/7168 values: -0.0091248, 0.018799, 0.03952, 0.12671, 0.014015, 0.00081587, 0.0086212, -0.1062 ...
[Cactus][layer_0] conv_output_NLC shape=[1, 7, 1024] showing 8/7168 values: -nan, 1.0729e-06, 1.6689e-06, -nan, 2.563e-06, 5.9605e-08, 0.0013342, -nan ...
[Cactus][layer_0] conv_output_LC shape=[7, 1024] per-token (first 8 dims):
  [tok0: -nan,1.0729e-06,1.6689e-06,-nan,2.563e-06,5.9605e-08,0.0013342,-nan]
  [tok1: -nan,-nan,3.159e-06,-nan,5.5432e-06,-nan,-494.25,-nan]
  [tok2: -nan,-nan,-0.19617,-nan,-0.55957,-nan,-2048,-nan]
  [tok3: -nan,-nan,-0.36963,-nan,-1.208,-nan,-2000,-nan]
  [tok4: -nan,-nan,0.45947,-nan,-0.62451,-nan,867,-nan]
  [tok5: -nan,-nan,0.048737,-nan,-0.17444,-nan,-8808,-nan]
  [tok6: -nan,-nan,0.16089,-nan,-1.1641,-nan,-136.12,-nan]
[Cactus][layer_0] conv_gated_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: -nan,-5.9605e-08,-2.9802e-07,-nan,-1.0729e-06,0,0.000458,-nan]
  [tok1: -nan,-nan,1.0729e-06,-nan,-9.5367e-07,-nan,-28.797,-nan]
  [tok2: -nan,-nan,-0.02005,-nan,0.061554,-nan,-1307,-nan]
  [tok3: -nan,-nan,-0.0271,-nan,-0.095581,-nan,-7.7188,-nan]
  [tok4: -nan,-nan,-0.10162,-nan,-0.11938,-nan,-65.5,-nan]
  [tok5: -nan,-nan,-0.0051422,-nan,-0.0072174,-nan,-61.188,-nan]
  [tok6: -nan,-nan,-0.020538,-nan,0.0030327,-nan,-38.531,-nan]
[Cactus][layer_0] conv_out_proj_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_0] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_0] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_0] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_0] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_0] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_0] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_0] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_0] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_0] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_0] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_in_proj shape=[7, 3072] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_triplet shape=[7, 3, 1024] showing 8/21504 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_B_sliced shape=[7, 1, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_B shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_C shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_X shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_bx shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.05075, 0.11503, -0.015225, -0.018609, -0.19116, -0.01015, -0.021992, -0.16748 ...
[Cactus][layer_1] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 30, 68, -9, -11, -113, -6, -13, -99 ...
[Cactus][layer_1] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 30, 68, -9
[Cactus][layer_1] conv_input_concat shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_input_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_output_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, -nan, nan, nan, -nan ...
[Cactus][layer_1] conv_output_LC shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,-nan,nan,nan,-nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,-nan,nan,nan,-nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,-nan,nan,nan,-nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,-nan,nan,nan,-nan]
[Cactus][layer_1] conv_gated_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_out_proj_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_1] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_1] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] attn_q_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] attn_k_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] attn_v_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] attn_q_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] attn_k_norm shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] attn_q_pre_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_pre_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_v shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_q_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_scores_output shape=[1, 7, 16, 64] showing 8/7168 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_2] attn_output_flat shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_2] attn_out_proj shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_2] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_2] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_2] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_2] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_2] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_in_proj shape=[7, 3072] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_triplet shape=[7, 3, 1024] showing 8/21504 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_B_sliced shape=[7, 1, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_B shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_C shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_X shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_bx shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.012272, 0.020454, -0.051135, -0.024545, -0.063407, 0.07568, -0.012272, -0.024545 ...
[Cactus][layer_3] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 6, 10, -25, -12, -31, 37, -6, -12 ...
[Cactus][layer_3] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 6, 10, -25
[Cactus][layer_3] conv_input_concat shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_input_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_output_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_output_LC shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_gated_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_out_proj_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_3] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_3] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_in_proj shape=[7, 3072] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_triplet shape=[7, 3, 1024] showing 8/21504 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_B_sliced shape=[7, 1, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_B shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_C shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_X shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_bx shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.051135, 0.07568, 0.032726, -0.04909, -0.081816, -0.081816, 0.022499, 0.077725 ...
[Cactus][layer_4] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 25, 37, 16, -24, -40, -40, 11, 38 ...
[Cactus][layer_4] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 25, 37, 16
[Cactus][layer_4] conv_input_concat shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_input_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_output_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, -nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_output_LC shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,-nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,-nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,-nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,-nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_gated_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_out_proj_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_4] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_4] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] attn_q_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] attn_k_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] attn_v_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] attn_q_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] attn_k_norm shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] attn_q_pre_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_pre_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_v shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_q_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_scores_output shape=[1, 7, 16, 64] showing 8/7168 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_5] attn_output_flat shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_5] attn_out_proj shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_5] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_5] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_5] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_5] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_5] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_in_proj shape=[7, 3072] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_triplet shape=[7, 3, 1024] showing 8/21504 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_B_sliced shape=[7, 1, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_B shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_C shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_X shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_bx shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: -0.0065053, -0.015179, -0.16046, 0.034695, 0.049874, 0.11926, -0.0086737, -0.039032 ...
[Cactus][layer_6] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: -3, -7, -74, 16, 23, 55, -4, -18 ...
[Cactus][layer_6] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: -3, -7, -74
[Cactus][layer_6] conv_input_concat shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_input_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_output_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_output_LC shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_gated_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_out_proj_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_6] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_6] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_in_proj shape=[7, 3072] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_triplet shape=[7, 3, 1024] showing 8/21504 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_B_sliced shape=[7, 1, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_B shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_C shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_X shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_bx shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.022669, 0.022669, -0.045337, 0.049459, 0.088613, -0.072127, 0.022669, 0.055641 ...
[Cactus][layer_7] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 11, 11, -22, 24, 43, -35, 11, 27 ...
[Cactus][layer_7] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 11, 11, -22
[Cactus][layer_7] conv_input_concat shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_input_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_output_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_output_LC shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_gated_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_out_proj_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_7] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_7] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_7] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_7] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] attn_q_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] attn_k_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] attn_v_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] attn_q_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] attn_k_norm shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] attn_q_pre_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_pre_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_v shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_q_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_scores_output shape=[1, 7, 16, 64] showing 8/7168 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_8] attn_output_flat shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_8] attn_out_proj shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_8] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_8] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_8] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_8] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_8] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_in_proj shape=[7, 3072] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_triplet shape=[7, 3, 1024] showing 8/21504 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_B_sliced shape=[7, 1, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_B shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_C shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_X shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_bx shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.018301, 0.026144, -0.044445, 0.020915, 0.052288, 0.17778, 0.015687, 0.026144 ...
[Cactus][layer_9] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 7, 10, -17, 8, 20, 68, 6, 10 ...
[Cactus][layer_9] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 7, 10, -17
[Cactus][layer_9] conv_input_concat shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_input_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_output_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_output_LC shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_gated_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_out_proj_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_9] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_9] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] attn_q_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] attn_k_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] attn_v_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] attn_q_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] attn_k_norm shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] attn_q_pre_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_pre_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_v shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_q_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_scores_output shape=[1, 7, 16, 64] showing 8/7168 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_10] attn_output_flat shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_10] attn_out_proj shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_10] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_10] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_10] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_10] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_10] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_in_proj shape=[7, 3072] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_triplet shape=[7, 3, 1024] showing 8/21504 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_B_sliced shape=[7, 1, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_B shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_C shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_X shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_bx shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: -0.019654, -0.032757, -0.15723, 0.013103, 0.04586, -0.080801, -0.021838, -0.043676 ...
[Cactus][layer_11] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: -9, -15, -72, 6, 21, -37, -10, -20 ...
[Cactus][layer_11] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: -9, -15, -72
[Cactus][layer_11] conv_input_concat shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_input_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_output_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_output_LC shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_gated_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_out_proj_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_11] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_11] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] attn_q_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] attn_k_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] attn_v_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] attn_q_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] attn_k_norm shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] attn_q_pre_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_pre_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_v shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_q_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_scores_output shape=[1, 7, 16, 64] showing 8/7168 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_12] attn_output_flat shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_12] attn_out_proj shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_12] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_12] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_12] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_12] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_12] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_in_proj shape=[7, 3072] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_triplet shape=[7, 3, 1024] showing 8/21504 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_B_sliced shape=[7, 1, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_B shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_C shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_X shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_bx shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: -0.007997, 0.0059978, -0.22992, 0.0019993, 0.0039985, 0.16394, -0.021992, 0.095965 ...
[Cactus][layer_13] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: -4, 3, -115, 1, 2, 82, -11, 48 ...
[Cactus][layer_13] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: -4, 3, -115
[Cactus][layer_13] conv_input_concat shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_input_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_output_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_output_LC shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_gated_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_out_proj_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_13] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_13] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] attn_q_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] attn_k_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] attn_v_linear shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] attn_q_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] attn_k_norm shape=[7, 512] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] attn_q_pre_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_pre_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_v shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_q_rope shape=[1, 7, 16, 64] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_rope shape=[1, 7, 8, 64] showing 8/3584 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_scores_output shape=[1, 7, 16, 64] showing 8/7168 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_14] attn_output_flat shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_14] attn_out_proj shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_14] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: 0,0,0,0,0,0,0,0]
  [tok1: 0,0,0,0,0,0,0,0]
  [tok2: 0,0,0,0,0,0,0,0]
  [tok3: 0,0,0,0,0,0,0,0]
  [tok4: 0,0,0,0,0,0,0,0]
  [tok5: 0,0,0,0,0,0,0,0]
  [tok6: 0,0,0,0,0,0,0,0]
[Cactus][layer_14] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_14] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_14] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_14] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] input_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_in_proj shape=[7, 3072] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_triplet shape=[7, 3, 1024] showing 8/21504 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_B_sliced shape=[7, 1, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_B shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_C shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_X shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_bx shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.0023991, 0.0095965, 0.19433, -0.0047982, -0.0071973, -0.086368, -0.0071973, -0.019193 ...
[Cactus][layer_15] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 1, 4, 81, -2, -3, -36, -3, -8 ...
[Cactus][layer_15] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 1, 4, 81
[Cactus][layer_15] conv_input_concat shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_input_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_output_NLC shape=[1, 7, 1024] showing 8/7168 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_output_LC shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_gated_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_out_proj_[L,C] shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] block_main_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] post_block_residual shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] post_block_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] mlp_gate_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] mlp_up_linear shape=[7, 4608] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] mlp_gate_silu shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_15] mlp_gated shape=[7, 4608] per-token (first 8 dims):
  [tok0: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok1: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok2: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok3: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok4: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok5: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
  [tok6: -nan,-nan,-nan,-nan,-nan,-nan,-nan,-nan]
[Cactus][layer_15] mlp_down_linear shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] mlp_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] block_output shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][global] final_norm shape=[7, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok3: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok4: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok5: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok6: nan,nan,nan,nan,nan,nan,nan,nan]
  [gen 0] token_id=0 text="<|pad|>"
[Cactus][layer_148] input_tokens shape=[1] showing 1/1 values: 0
[Cactus][global] embedding_output shape=[1, 1024] showing 8/1024 values: 0.0027523, -0.013763, -0.0055046, -0.019272, -0.0027523, -0.0027523, 0.0055046, -0.0027523 ...
[Cactus][layer_0] input_norm shape=[1, 1024] showing 8/1024 values: 0.060577, -0.31006, -0.12482, -0.46558, -0.06427, -0.05835, 0.1167, -0.068298 ...
[Cactus][layer_0] conv_in_proj shape=[1, 3072] showing 8/3072 values: 0.097168, -0.045197, -0.03125, 0.10559, -0.30322, 0.17029, 0.49585, 0.22119 ...
[Cactus][layer_0] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: 0.097168, -0.045197, -0.03125, 0.10559, -0.30322, 0.17029, 0.49585, 0.22119 ...
[Cactus][layer_0] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: 0.097168, -0.045197, -0.03125, 0.10559, -0.30322, 0.17029, 0.49585, 0.22119 ...
[Cactus][layer_0] conv_B shape=[1, 1024] showing 8/1024 values: 0.097168, -0.045197, -0.03125, 0.10559, -0.30322, 0.17029, 0.49585, 0.22119 ...
[Cactus][layer_0] conv_C shape=[1, 1024] showing 8/1024 values: 0.12695, 0.082703, -0.06781, -0.0069733, 0.1236, 0.12524, 0.1842, -0.28149 ...
[Cactus][layer_0] conv_X shape=[1, 1024] showing 8/1024 values: -0.19641, 0.28052, 0.22241, -0.22046, 0.062744, -0.5293, -0.1698, 0.34937 ...
[Cactus][layer_0] conv_bx shape=[1, 1024] showing 8/1024 values: -0.019089, -0.01268, -0.0069504, -0.023285, -0.019028, -0.090149, -0.084167, 0.077271 ...
[Cactus][layer_0] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.011257, 0.18856, 0, 0.0197, 0.16605, -0.008443, 0.13227, 0.059101 ...
[Cactus][layer_0] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 4, 67, 0, 7, 59, -3, 47, 21 ...
[Cactus][layer_0] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 4, 67, 0
[Cactus][layer_0] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: -0.0082092,0.0066338,-0.0039558,-0.021988,-0.00085354,-0.0051041,0.0023594,0.0022984]
  [tok1: 0.0015831,0.0095749,-0.00088596,2.3782e-05,0.0083389,0.0094376,-0.00035644,0.040497]
[Cactus][layer_0] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: -0.0082092,0.0066338,-0.0039558,-0.021988,-0.00085354,-0.0051041,0.0023594,0.0022984]
  [tok1: 0.0015831,0.0095749,-0.00088596,2.3782e-05,0.0083389,0.0094376,-0.00035644,0.040497]
  [tok2: -0.019089,-0.01268,-0.0069504,-0.023285,-0.019028,-0.090149,-0.084167,0.077271]
[Cactus][layer_0] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: -0.0082092, 0.0066338, -0.0039558, -0.021988, -0.00085354, -0.0051041, 0.0023594, 0.0022984 ...
[Cactus][layer_0] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: -nan, 3.5763e-07, -1.7881e-07, -nan, -1.7881e-07, -3.5763e-07, 0.00036526, -nan ...
[Cactus][layer_0] conv_output_LC shape=[1, 1024] showing 8/1024 values: -nan, -nan, 0.019638, -nan, 0.034088, -nan, 20.422, -nan ...
[Cactus][layer_0] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: -nan, -nan, -0.0013313, -nan, 0.0042114, -nan, 3.7617, -nan ...
[Cactus][layer_0] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_0] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_0] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.05075, 0.11503, -0.015225, -0.018609, -0.19116, -0.01015, -0.021992, -0.16748 ...
[Cactus][layer_1] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 30, 68, -9, -11, -113, -6, -13, -99 ...
[Cactus][layer_1] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 30, 68, -9
[Cactus][layer_1] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, -nan, nan, nan, -nan ...
[Cactus][layer_1] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, -nan, nan, nan, -nan ...
[Cactus][layer_1] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_1] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_1] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_2] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_2] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_2] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_2] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_2] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_2] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.012272, 0.020454, -0.051135, -0.024545, -0.063407, 0.07568, -0.012272, -0.024545 ...
[Cactus][layer_3] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 6, 10, -25, -12, -31, 37, -6, -12 ...
[Cactus][layer_3] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 6, 10, -25
[Cactus][layer_3] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_3] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_3] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.051135, 0.07568, 0.032726, -0.04909, -0.081816, -0.081816, 0.022499, 0.077725 ...
[Cactus][layer_4] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 25, 37, 16, -24, -40, -40, 11, 38 ...
[Cactus][layer_4] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 25, 37, 16
[Cactus][layer_4] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, -nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, -nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_4] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_4] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_5] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_5] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_5] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_5] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_5] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_5] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: -0.0065053, -0.015179, -0.16046, 0.034695, 0.049874, 0.11926, -0.0086737, -0.039032 ...
[Cactus][layer_6] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: -3, -7, -74, 16, 23, 55, -4, -18 ...
[Cactus][layer_6] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: -3, -7, -74
[Cactus][layer_6] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_6] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_6] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.022669, 0.022669, -0.045337, 0.049459, 0.088613, -0.072127, 0.022669, 0.055641 ...
[Cactus][layer_7] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 11, 11, -22, 24, 43, -35, 11, 27 ...
[Cactus][layer_7] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 11, 11, -22
[Cactus][layer_7] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_7] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_7] mlp_down_linear shape=[1, 1024] showing 8/1024 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_7] mlp_output shape=[1, 1024] showing 8/1024 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_7] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_8] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_8] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_8] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_8] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_8] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_8] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.018301, 0.026144, -0.044445, 0.020915, 0.052288, 0.17778, 0.015687, 0.026144 ...
[Cactus][layer_9] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 7, 10, -17, 8, 20, 68, 6, 10 ...
[Cactus][layer_9] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 7, 10, -17
[Cactus][layer_9] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_9] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_9] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_10] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_10] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_10] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_10] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_10] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_10] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: -0.019654, -0.032757, -0.15723, 0.013103, 0.04586, -0.080801, -0.021838, -0.043676 ...
[Cactus][layer_11] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: -9, -15, -72, 6, 21, -37, -10, -20 ...
[Cactus][layer_11] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: -9, -15, -72
[Cactus][layer_11] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_11] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_11] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_12] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_12] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_12] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_12] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_12] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_12] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: -0.007997, 0.0059978, -0.22992, 0.0019993, 0.0039985, 0.16394, -0.021992, 0.095965 ...
[Cactus][layer_13] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: -4, 3, -115, 1, 2, 82, -11, 48 ...
[Cactus][layer_13] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: -4, 3, -115
[Cactus][layer_13] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_13] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_13] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_14] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_14] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_14] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_14] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_14] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_14] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.0023991, 0.0095965, 0.19433, -0.0047982, -0.0071973, -0.086368, -0.0071973, -0.019193 ...
[Cactus][layer_15] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 1, 4, 81, -2, -3, -36, -3, -8 ...
[Cactus][layer_15] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 1, 4, 81
[Cactus][layer_15] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_15] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_15] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][global] final_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
  [gen 1] token_id=0 text="<|pad|>"
[Cactus][layer_148] input_tokens shape=[1] showing 1/1 values: 0
[Cactus][global] embedding_output shape=[1, 1024] showing 8/1024 values: 0.0027523, -0.013763, -0.0055046, -0.019272, -0.0027523, -0.0027523, 0.0055046, -0.0027523 ...
[Cactus][layer_0] input_norm shape=[1, 1024] showing 8/1024 values: 0.060577, -0.31006, -0.12482, -0.46558, -0.06427, -0.05835, 0.1167, -0.068298 ...
[Cactus][layer_0] conv_in_proj shape=[1, 3072] showing 8/3072 values: 0.097168, -0.045197, -0.03125, 0.10559, -0.30322, 0.17029, 0.49585, 0.22119 ...
[Cactus][layer_0] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: 0.097168, -0.045197, -0.03125, 0.10559, -0.30322, 0.17029, 0.49585, 0.22119 ...
[Cactus][layer_0] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: 0.097168, -0.045197, -0.03125, 0.10559, -0.30322, 0.17029, 0.49585, 0.22119 ...
[Cactus][layer_0] conv_B shape=[1, 1024] showing 8/1024 values: 0.097168, -0.045197, -0.03125, 0.10559, -0.30322, 0.17029, 0.49585, 0.22119 ...
[Cactus][layer_0] conv_C shape=[1, 1024] showing 8/1024 values: 0.12695, 0.082703, -0.06781, -0.0069733, 0.1236, 0.12524, 0.1842, -0.28149 ...
[Cactus][layer_0] conv_X shape=[1, 1024] showing 8/1024 values: -0.19641, 0.28052, 0.22241, -0.22046, 0.062744, -0.5293, -0.1698, 0.34937 ...
[Cactus][layer_0] conv_bx shape=[1, 1024] showing 8/1024 values: -0.019089, -0.01268, -0.0069504, -0.023285, -0.019028, -0.090149, -0.084167, 0.077271 ...
[Cactus][layer_0] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.011257, 0.18856, 0, 0.0197, 0.16605, -0.008443, 0.13227, 0.059101 ...
[Cactus][layer_0] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 4, 67, 0, 7, 59, -3, 47, 21 ...
[Cactus][layer_0] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 4, 67, 0
[Cactus][layer_0] conv_cache_left shape=[1, 1024] showing 8/1024 values: 0.0015831, 0.0095749, -0.00088596, 2.3782e-05, 0.0083389, 0.0094376, -0.00035644, 0.040497 ...
[Cactus][layer_0] conv_cache_right shape=[1, 1024] showing 8/1024 values: -0.019089, -0.01268, -0.0069504, -0.023285, -0.019028, -0.090149, -0.084167, 0.077271 ...
[Cactus][layer_0] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: 0.0015831,0.0095749,-0.00088596,2.3782e-05,0.0083389,0.0094376,-0.00035644,0.040497]
  [tok1: -0.019089,-0.01268,-0.0069504,-0.023285,-0.019028,-0.090149,-0.084167,0.077271]
  [tok2: -0.019089,-0.01268,-0.0069504,-0.023285,-0.019028,-0.090149,-0.084167,0.077271]
[Cactus][layer_0] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: 0.0015831, 0.0095749, -0.00088596, 2.3782e-05, 0.0083389, 0.0094376, -0.00035644, 0.040497 ...
[Cactus][layer_0] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: -nan, 5.3644e-07, -5.9605e-08, -nan, 1.5497e-06, 6.5565e-07, -5.5194e-05, -nan ...
[Cactus][layer_0] conv_output_LC shape=[1, 1024] showing 8/1024 values: -nan, -nan, 0.0043983, -nan, -0.33301, -nan, 4828, -nan ...
[Cactus][layer_0] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: -nan, -nan, -0.00029826, -nan, -0.041168, -nan, 889.5, -nan ...
[Cactus][layer_0] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_0] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_0] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.05075, 0.11503, -0.015225, -0.018609, -0.19116, -0.01015, -0.021992, -0.16748 ...
[Cactus][layer_1] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 30, 68, -9, -11, -113, -6, -13, -99 ...
[Cactus][layer_1] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 30, 68, -9
[Cactus][layer_1] conv_cache_left shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_cache_right shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, -nan, nan, nan, -nan ...
[Cactus][layer_1] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, -nan, nan, nan, -nan ...
[Cactus][layer_1] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_1] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_1] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_2] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_2] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_2] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_2] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_2] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_2] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.012272, 0.020454, -0.051135, -0.024545, -0.063407, 0.07568, -0.012272, -0.024545 ...
[Cactus][layer_3] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 6, 10, -25, -12, -31, 37, -6, -12 ...
[Cactus][layer_3] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 6, 10, -25
[Cactus][layer_3] conv_cache_left shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_cache_right shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_3] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_3] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.051135, 0.07568, 0.032726, -0.04909, -0.081816, -0.081816, 0.022499, 0.077725 ...
[Cactus][layer_4] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 25, 37, 16, -24, -40, -40, 11, 38 ...
[Cactus][layer_4] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 25, 37, 16
[Cactus][layer_4] conv_cache_left shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_cache_right shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, -nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, -nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_4] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_4] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_5] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_5] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_5] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_5] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_5] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_5] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: -0.0065053, -0.015179, -0.16046, 0.034695, 0.049874, 0.11926, -0.0086737, -0.039032 ...
[Cactus][layer_6] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: -3, -7, -74, 16, 23, 55, -4, -18 ...
[Cactus][layer_6] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: -3, -7, -74
[Cactus][layer_6] conv_cache_left shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_cache_right shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_6] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_6] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.022669, 0.022669, -0.045337, 0.049459, 0.088613, -0.072127, 0.022669, 0.055641 ...
[Cactus][layer_7] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 11, 11, -22, 24, 43, -35, 11, 27 ...
[Cactus][layer_7] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 11, 11, -22
[Cactus][layer_7] conv_cache_left shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_cache_right shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_7] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_7] mlp_down_linear shape=[1, 1024] showing 8/1024 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_7] mlp_output shape=[1, 1024] showing 8/1024 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_7] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_8] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_8] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_8] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_8] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_8] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_8] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.018301, 0.026144, -0.044445, 0.020915, 0.052288, 0.17778, 0.015687, 0.026144 ...
[Cactus][layer_9] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 7, 10, -17, 8, 20, 68, 6, 10 ...
[Cactus][layer_9] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 7, 10, -17
[Cactus][layer_9] conv_cache_left shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_cache_right shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_9] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_9] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_10] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_10] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_10] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_10] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_10] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_10] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: -0.019654, -0.032757, -0.15723, 0.013103, 0.04586, -0.080801, -0.021838, -0.043676 ...
[Cactus][layer_11] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: -9, -15, -72, 6, 21, -37, -10, -20 ...
[Cactus][layer_11] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: -9, -15, -72
[Cactus][layer_11] conv_cache_left shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_cache_right shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_11] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_11] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_12] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_12] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_12] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_12] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_12] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_12] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: -0.007997, 0.0059978, -0.22992, 0.0019993, 0.0039985, 0.16394, -0.021992, 0.095965 ...
[Cactus][layer_13] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: -4, 3, -115, 1, 2, 82, -11, 48 ...
[Cactus][layer_13] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: -4, 3, -115
[Cactus][layer_13] conv_cache_left shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_cache_right shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_13] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_13] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_14] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_14] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_14] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_14] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_14] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_14] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.0023991, 0.0095965, 0.19433, -0.0047982, -0.0071973, -0.086368, -0.0071973, -0.019193 ...
[Cactus][layer_15] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 1, 4, 81, -2, -3, -36, -3, -8 ...
[Cactus][layer_15] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 1, 4, 81
[Cactus][layer_15] conv_cache_left shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_cache_right shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_15] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_15] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][global] final_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
  [gen 2] token_id=0 text="<|pad|>"
[Cactus][layer_148] input_tokens shape=[1] showing 1/1 values: 0
[Cactus][global] embedding_output shape=[1, 1024] showing 8/1024 values: 0.0027523, -0.013763, -0.0055046, -0.019272, -0.0027523, -0.0027523, 0.0055046, -0.0027523 ...
[Cactus][layer_0] input_norm shape=[1, 1024] showing 8/1024 values: 0.060577, -0.31006, -0.12482, -0.46558, -0.06427, -0.05835, 0.1167, -0.068298 ...
[Cactus][layer_0] conv_in_proj shape=[1, 3072] showing 8/3072 values: 0.097168, -0.045197, -0.03125, 0.10559, -0.30322, 0.17029, 0.49585, 0.22119 ...
[Cactus][layer_0] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: 0.097168, -0.045197, -0.03125, 0.10559, -0.30322, 0.17029, 0.49585, 0.22119 ...
[Cactus][layer_0] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: 0.097168, -0.045197, -0.03125, 0.10559, -0.30322, 0.17029, 0.49585, 0.22119 ...
[Cactus][layer_0] conv_B shape=[1, 1024] showing 8/1024 values: 0.097168, -0.045197, -0.03125, 0.10559, -0.30322, 0.17029, 0.49585, 0.22119 ...
[Cactus][layer_0] conv_C shape=[1, 1024] showing 8/1024 values: 0.12695, 0.082703, -0.06781, -0.0069733, 0.1236, 0.12524, 0.1842, -0.28149 ...
[Cactus][layer_0] conv_X shape=[1, 1024] showing 8/1024 values: -0.19641, 0.28052, 0.22241, -0.22046, 0.062744, -0.5293, -0.1698, 0.34937 ...
[Cactus][layer_0] conv_bx shape=[1, 1024] showing 8/1024 values: -0.019089, -0.01268, -0.0069504, -0.023285, -0.019028, -0.090149, -0.084167, 0.077271 ...
[Cactus][layer_0] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.011257, 0.18856, 0, 0.0197, 0.16605, -0.008443, 0.13227, 0.059101 ...
[Cactus][layer_0] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 4, 67, 0, 7, 59, -3, 47, 21 ...
[Cactus][layer_0] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 4, 67, 0
[Cactus][layer_0] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: -0.019089,-0.01268,-0.0069504,-0.023285,-0.019028,-0.090149,-0.084167,0.077271]
  [tok1: -0.019089,-0.01268,-0.0069504,-0.023285,-0.019028,-0.090149,-0.084167,0.077271]
[Cactus][layer_0] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: -0.019089,-0.01268,-0.0069504,-0.023285,-0.019028,-0.090149,-0.084167,0.077271]
  [tok1: -0.019089,-0.01268,-0.0069504,-0.023285,-0.019028,-0.090149,-0.084167,0.077271]
  [tok2: -0.019089,-0.01268,-0.0069504,-0.023285,-0.019028,-0.090149,-0.084167,0.077271]
[Cactus][layer_0] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: -0.019089, -0.01268, -0.0069504, -0.023285, -0.019028, -0.090149, -0.084167, 0.077271 ...
[Cactus][layer_0] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: -nan, -7.1526e-07, -2.9802e-07, -nan, -3.4571e-06, -6.4969e-06, -0.013031, -nan ...
[Cactus][layer_0] conv_output_LC shape=[1, 1024] showing 8/1024 values: -nan, -nan, 0.034515, -nan, 0.75977, -nan, 4828, -nan ...
[Cactus][layer_0] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: -nan, -nan, -0.0023403, -nan, 0.093933, -nan, 889.5, -nan ...
[Cactus][layer_0] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_0] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_0] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.05075, 0.11503, -0.015225, -0.018609, -0.19116, -0.01015, -0.021992, -0.16748 ...
[Cactus][layer_1] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 30, 68, -9, -11, -113, -6, -13, -99 ...
[Cactus][layer_1] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 30, 68, -9
[Cactus][layer_1] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, -nan, nan, nan, -nan ...
[Cactus][layer_1] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, -nan, nan, nan, -nan ...
[Cactus][layer_1] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_1] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_1] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_2] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_2] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_2] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_2] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_2] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_2] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.012272, 0.020454, -0.051135, -0.024545, -0.063407, 0.07568, -0.012272, -0.024545 ...
[Cactus][layer_3] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 6, 10, -25, -12, -31, 37, -6, -12 ...
[Cactus][layer_3] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 6, 10, -25
[Cactus][layer_3] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_3] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_3] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.051135, 0.07568, 0.032726, -0.04909, -0.081816, -0.081816, 0.022499, 0.077725 ...
[Cactus][layer_4] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 25, 37, 16, -24, -40, -40, 11, 38 ...
[Cactus][layer_4] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 25, 37, 16
[Cactus][layer_4] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, -nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, -nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_4] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_4] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_5] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_5] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_5] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_5] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_5] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_5] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: -0.0065053, -0.015179, -0.16046, 0.034695, 0.049874, 0.11926, -0.0086737, -0.039032 ...
[Cactus][layer_6] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: -3, -7, -74, 16, 23, 55, -4, -18 ...
[Cactus][layer_6] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: -3, -7, -74
[Cactus][layer_6] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_6] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_6] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.022669, 0.022669, -0.045337, 0.049459, 0.088613, -0.072127, 0.022669, 0.055641 ...
[Cactus][layer_7] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 11, 11, -22, 24, 43, -35, 11, 27 ...
[Cactus][layer_7] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 11, 11, -22
[Cactus][layer_7] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_7] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_7] mlp_down_linear shape=[1, 1024] showing 8/1024 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_7] mlp_output shape=[1, 1024] showing 8/1024 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_7] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_8] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_8] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_8] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_8] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_8] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_8] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.018301, 0.026144, -0.044445, 0.020915, 0.052288, 0.17778, 0.015687, 0.026144 ...
[Cactus][layer_9] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 7, 10, -17, 8, 20, 68, 6, 10 ...
[Cactus][layer_9] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 7, 10, -17
[Cactus][layer_9] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_9] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_9] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_10] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_10] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_10] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_10] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_10] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_10] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: -0.019654, -0.032757, -0.15723, 0.013103, 0.04586, -0.080801, -0.021838, -0.043676 ...
[Cactus][layer_11] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: -9, -15, -72, 6, 21, -37, -10, -20 ...
[Cactus][layer_11] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: -9, -15, -72
[Cactus][layer_11] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_11] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_11] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_12] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_12] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_12] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_12] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_12] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_12] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: -0.007997, 0.0059978, -0.22992, 0.0019993, 0.0039985, 0.16394, -0.021992, 0.095965 ...
[Cactus][layer_13] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: -4, 3, -115, 1, 2, 82, -11, 48 ...
[Cactus][layer_13] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: -4, 3, -115
[Cactus][layer_13] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_13] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_13] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_14] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_14] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_14] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_14] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_14] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_14] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.0023991, 0.0095965, 0.19433, -0.0047982, -0.0071973, -0.086368, -0.0071973, -0.019193 ...
[Cactus][layer_15] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 1, 4, 81, -2, -3, -36, -3, -8 ...
[Cactus][layer_15] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 1, 4, 81
[Cactus][layer_15] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_15] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_15] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][global] final_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
  [gen 3] token_id=0 text="<|pad|>"
[Cactus][layer_148] input_tokens shape=[1] showing 1/1 values: 0
[Cactus][global] embedding_output shape=[1, 1024] showing 8/1024 values: 0.0027523, -0.013763, -0.0055046, -0.019272, -0.0027523, -0.0027523, 0.0055046, -0.0027523 ...
[Cactus][layer_0] input_norm shape=[1, 1024] showing 8/1024 values: 0.060577, -0.31006, -0.12482, -0.46558, -0.06427, -0.05835, 0.1167, -0.068298 ...
[Cactus][layer_0] conv_in_proj shape=[1, 3072] showing 8/3072 values: 0.097168, -0.045197, -0.03125, 0.10559, -0.30322, 0.17029, 0.49585, 0.22119 ...
[Cactus][layer_0] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: 0.097168, -0.045197, -0.03125, 0.10559, -0.30322, 0.17029, 0.49585, 0.22119 ...
[Cactus][layer_0] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: 0.097168, -0.045197, -0.03125, 0.10559, -0.30322, 0.17029, 0.49585, 0.22119 ...
[Cactus][layer_0] conv_B shape=[1, 1024] showing 8/1024 values: 0.097168, -0.045197, -0.03125, 0.10559, -0.30322, 0.17029, 0.49585, 0.22119 ...
[Cactus][layer_0] conv_C shape=[1, 1024] showing 8/1024 values: 0.12695, 0.082703, -0.06781, -0.0069733, 0.1236, 0.12524, 0.1842, -0.28149 ...
[Cactus][layer_0] conv_X shape=[1, 1024] showing 8/1024 values: -0.19641, 0.28052, 0.22241, -0.22046, 0.062744, -0.5293, -0.1698, 0.34937 ...
[Cactus][layer_0] conv_bx shape=[1, 1024] showing 8/1024 values: -0.019089, -0.01268, -0.0069504, -0.023285, -0.019028, -0.090149, -0.084167, 0.077271 ...
[Cactus][layer_0] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.011257, 0.18856, 0, 0.0197, 0.16605, -0.008443, 0.13227, 0.059101 ...
[Cactus][layer_0] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 4, 67, 0, 7, 59, -3, 47, 21 ...
[Cactus][layer_0] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 4, 67, 0
[Cactus][layer_0] conv_cache_left shape=[1, 1024] showing 8/1024 values: -0.019089, -0.01268, -0.0069504, -0.023285, -0.019028, -0.090149, -0.084167, 0.077271 ...
[Cactus][layer_0] conv_cache_right shape=[1, 1024] showing 8/1024 values: -0.019089, -0.01268, -0.0069504, -0.023285, -0.019028, -0.090149, -0.084167, 0.077271 ...
[Cactus][layer_0] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: -0.019089,-0.01268,-0.0069504,-0.023285,-0.019028,-0.090149,-0.084167,0.077271]
  [tok1: -0.019089,-0.01268,-0.0069504,-0.023285,-0.019028,-0.090149,-0.084167,0.077271]
  [tok2: -0.019089,-0.01268,-0.0069504,-0.023285,-0.019028,-0.090149,-0.084167,0.077271]
[Cactus][layer_0] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: -0.019089, -0.01268, -0.0069504, -0.023285, -0.019028, -0.090149, -0.084167, 0.077271 ...
[Cactus][layer_0] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: -nan, -7.1526e-07, -2.9802e-07, -nan, -3.4571e-06, -6.4969e-06, -0.013031, -nan ...
[Cactus][layer_0] conv_output_LC shape=[1, 1024] showing 8/1024 values: -nan, -nan, 0.034515, -nan, 0.75977, -nan, 4828, -nan ...
[Cactus][layer_0] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: -nan, -nan, -0.0023403, -nan, 0.093933, -nan, 889.5, -nan ...
[Cactus][layer_0] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_0] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_0] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.05075, 0.11503, -0.015225, -0.018609, -0.19116, -0.01015, -0.021992, -0.16748 ...
[Cactus][layer_1] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 30, 68, -9, -11, -113, -6, -13, -99 ...
[Cactus][layer_1] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 30, 68, -9
[Cactus][layer_1] conv_cache_left shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_cache_right shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, -nan, nan, nan, -nan ...
[Cactus][layer_1] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, -nan, nan, nan, -nan ...
[Cactus][layer_1] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_1] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_1] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_2] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_2] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_2] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_2] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_2] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_2] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.012272, 0.020454, -0.051135, -0.024545, -0.063407, 0.07568, -0.012272, -0.024545 ...
[Cactus][layer_3] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 6, 10, -25, -12, -31, 37, -6, -12 ...
[Cactus][layer_3] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 6, 10, -25
[Cactus][layer_3] conv_cache_left shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_cache_right shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_3] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_3] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.051135, 0.07568, 0.032726, -0.04909, -0.081816, -0.081816, 0.022499, 0.077725 ...
[Cactus][layer_4] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 25, 37, 16, -24, -40, -40, 11, 38 ...
[Cactus][layer_4] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 25, 37, 16
[Cactus][layer_4] conv_cache_left shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_cache_right shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, -nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, -nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_4] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_4] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_5] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_5] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_5] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_5] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_5] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_5] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: -0.0065053, -0.015179, -0.16046, 0.034695, 0.049874, 0.11926, -0.0086737, -0.039032 ...
[Cactus][layer_6] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: -3, -7, -74, 16, 23, 55, -4, -18 ...
[Cactus][layer_6] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: -3, -7, -74
[Cactus][layer_6] conv_cache_left shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_cache_right shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_6] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_6] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.022669, 0.022669, -0.045337, 0.049459, 0.088613, -0.072127, 0.022669, 0.055641 ...
[Cactus][layer_7] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 11, 11, -22, 24, 43, -35, 11, 27 ...
[Cactus][layer_7] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 11, 11, -22
[Cactus][layer_7] conv_cache_left shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_cache_right shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_7] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_7] mlp_down_linear shape=[1, 1024] showing 8/1024 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_7] mlp_output shape=[1, 1024] showing 8/1024 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_7] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_8] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_8] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_8] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_8] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_8] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_8] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.018301, 0.026144, -0.044445, 0.020915, 0.052288, 0.17778, 0.015687, 0.026144 ...
[Cactus][layer_9] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 7, 10, -17, 8, 20, 68, 6, 10 ...
[Cactus][layer_9] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 7, 10, -17
[Cactus][layer_9] conv_cache_left shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_cache_right shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_9] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_9] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_10] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_10] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_10] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_10] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_10] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_10] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: -0.019654, -0.032757, -0.15723, 0.013103, 0.04586, -0.080801, -0.021838, -0.043676 ...
[Cactus][layer_11] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: -9, -15, -72, 6, 21, -37, -10, -20 ...
[Cactus][layer_11] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: -9, -15, -72
[Cactus][layer_11] conv_cache_left shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_cache_right shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_11] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_11] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_12] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_12] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_12] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_12] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_12] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_12] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: -0.007997, 0.0059978, -0.22992, 0.0019993, 0.0039985, 0.16394, -0.021992, 0.095965 ...
[Cactus][layer_13] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: -4, 3, -115, 1, 2, 82, -11, 48 ...
[Cactus][layer_13] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: -4, 3, -115
[Cactus][layer_13] conv_cache_left shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_cache_right shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_13] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_13] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_14] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_14] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_14] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_14] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_14] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_14] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.0023991, 0.0095965, 0.19433, -0.0047982, -0.0071973, -0.086368, -0.0071973, -0.019193 ...
[Cactus][layer_15] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 1, 4, 81, -2, -3, -36, -3, -8 ...
[Cactus][layer_15] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 1, 4, 81
[Cactus][layer_15] conv_cache_left shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_cache_right shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_15] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_15] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][global] final_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
  [gen 4] token_id=0 text="<|pad|>"
[Cactus][layer_148] input_tokens shape=[1] showing 1/1 values: 0
[Cactus][global] embedding_output shape=[1, 1024] showing 8/1024 values: 0.0027523, -0.013763, -0.0055046, -0.019272, -0.0027523, -0.0027523, 0.0055046, -0.0027523 ...
[Cactus][layer_0] input_norm shape=[1, 1024] showing 8/1024 values: 0.060577, -0.31006, -0.12482, -0.46558, -0.06427, -0.05835, 0.1167, -0.068298 ...
[Cactus][layer_0] conv_in_proj shape=[1, 3072] showing 8/3072 values: 0.097168, -0.045197, -0.03125, 0.10559, -0.30322, 0.17029, 0.49585, 0.22119 ...
[Cactus][layer_0] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: 0.097168, -0.045197, -0.03125, 0.10559, -0.30322, 0.17029, 0.49585, 0.22119 ...
[Cactus][layer_0] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: 0.097168, -0.045197, -0.03125, 0.10559, -0.30322, 0.17029, 0.49585, 0.22119 ...
[Cactus][layer_0] conv_B shape=[1, 1024] showing 8/1024 values: 0.097168, -0.045197, -0.03125, 0.10559, -0.30322, 0.17029, 0.49585, 0.22119 ...
[Cactus][layer_0] conv_C shape=[1, 1024] showing 8/1024 values: 0.12695, 0.082703, -0.06781, -0.0069733, 0.1236, 0.12524, 0.1842, -0.28149 ...
[Cactus][layer_0] conv_X shape=[1, 1024] showing 8/1024 values: -0.19641, 0.28052, 0.22241, -0.22046, 0.062744, -0.5293, -0.1698, 0.34937 ...
[Cactus][layer_0] conv_bx shape=[1, 1024] showing 8/1024 values: -0.019089, -0.01268, -0.0069504, -0.023285, -0.019028, -0.090149, -0.084167, 0.077271 ...
[Cactus][layer_0] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.011257, 0.18856, 0, 0.0197, 0.16605, -0.008443, 0.13227, 0.059101 ...
[Cactus][layer_0] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 4, 67, 0, 7, 59, -3, 47, 21 ...
[Cactus][layer_0] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 4, 67, 0
[Cactus][layer_0] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: -0.019089,-0.01268,-0.0069504,-0.023285,-0.019028,-0.090149,-0.084167,0.077271]
  [tok1: -0.019089,-0.01268,-0.0069504,-0.023285,-0.019028,-0.090149,-0.084167,0.077271]
[Cactus][layer_0] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: -0.019089,-0.01268,-0.0069504,-0.023285,-0.019028,-0.090149,-0.084167,0.077271]
  [tok1: -0.019089,-0.01268,-0.0069504,-0.023285,-0.019028,-0.090149,-0.084167,0.077271]
  [tok2: -0.019089,-0.01268,-0.0069504,-0.023285,-0.019028,-0.090149,-0.084167,0.077271]
[Cactus][layer_0] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: -0.019089, -0.01268, -0.0069504, -0.023285, -0.019028, -0.090149, -0.084167, 0.077271 ...
[Cactus][layer_0] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: -nan, -7.1526e-07, -2.9802e-07, -nan, -3.4571e-06, -6.4969e-06, -0.013031, -nan ...
[Cactus][layer_0] conv_output_LC shape=[1, 1024] showing 8/1024 values: -nan, -nan, 0.034515, -nan, 0.75977, -nan, 4828, -nan ...
[Cactus][layer_0] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: -nan, -nan, -0.0023403, -nan, 0.093933, -nan, 889.5, -nan ...
[Cactus][layer_0] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_0] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_0] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_0] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.05075, 0.11503, -0.015225, -0.018609, -0.19116, -0.01015, -0.021992, -0.16748 ...
[Cactus][layer_1] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 30, 68, -9, -11, -113, -6, -13, -99 ...
[Cactus][layer_1] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 30, 68, -9
[Cactus][layer_1] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_1] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, -nan, nan, nan, -nan ...
[Cactus][layer_1] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, -nan, nan, nan, -nan ...
[Cactus][layer_1] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_1] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_1] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_1] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_2] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_2] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_2] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_2] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_2] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_2] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_2] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.012272, 0.020454, -0.051135, -0.024545, -0.063407, 0.07568, -0.012272, -0.024545 ...
[Cactus][layer_3] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 6, 10, -25, -12, -31, 37, -6, -12 ...
[Cactus][layer_3] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 6, 10, -25
[Cactus][layer_3] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_3] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_3] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_3] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_3] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.051135, 0.07568, 0.032726, -0.04909, -0.081816, -0.081816, 0.022499, 0.077725 ...
[Cactus][layer_4] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 25, 37, 16, -24, -40, -40, 11, 38 ...
[Cactus][layer_4] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 25, 37, 16
[Cactus][layer_4] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_4] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, -nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, -nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_4] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_4] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_4] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_5] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_5] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_5] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_5] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_5] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_5] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_5] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: -0.0065053, -0.015179, -0.16046, 0.034695, 0.049874, 0.11926, -0.0086737, -0.039032 ...
[Cactus][layer_6] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: -3, -7, -74, 16, 23, 55, -4, -18 ...
[Cactus][layer_6] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: -3, -7, -74
[Cactus][layer_6] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_6] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_6] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_6] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_6] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.022669, 0.022669, -0.045337, 0.049459, 0.088613, -0.072127, 0.022669, 0.055641 ...
[Cactus][layer_7] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 11, 11, -22, 24, 43, -35, 11, 27 ...
[Cactus][layer_7] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 11, 11, -22
[Cactus][layer_7] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_7] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_7] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_7] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_7] mlp_down_linear shape=[1, 1024] showing 8/1024 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_7] mlp_output shape=[1, 1024] showing 8/1024 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_7] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_8] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_8] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_8] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_8] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_8] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_8] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_8] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.018301, 0.026144, -0.044445, 0.020915, 0.052288, 0.17778, 0.015687, 0.026144 ...
[Cactus][layer_9] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 7, 10, -17, 8, 20, 68, 6, 10 ...
[Cactus][layer_9] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 7, 10, -17
[Cactus][layer_9] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_9] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_9] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_9] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_9] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_10] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_10] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_10] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_10] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_10] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_10] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_10] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: -0.019654, -0.032757, -0.15723, 0.013103, 0.04586, -0.080801, -0.021838, -0.043676 ...
[Cactus][layer_11] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: -9, -15, -72, 6, 21, -37, -10, -20 ...
[Cactus][layer_11] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: -9, -15, -72
[Cactus][layer_11] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_11] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_11] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_11] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_11] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_12] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_12] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_12] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_12] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_12] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_12] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_12] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: -0.007997, 0.0059978, -0.22992, 0.0019993, 0.0039985, 0.16394, -0.021992, 0.095965 ...
[Cactus][layer_13] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: -4, 3, -115, 1, 2, 82, -11, 48 ...
[Cactus][layer_13] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: -4, 3, -115
[Cactus][layer_13] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_13] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_13] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_13] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_13] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_q_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_v_linear shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_q_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_norm shape=[1, 512] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_q_pre_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_pre_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_v shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_q_rope shape=[1, 1, 16, 64] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_k_rope shape=[1, 1, 8, 64] showing 8/512 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] attn_scores_output shape=[1, 1, 16, 64] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_14] attn_output_flat shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_14] attn_out_proj shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_14] block_main_output shape=[1, 1024] showing 8/1024 values: 0, 0, 0, 0, 0, 0, 0, 0 ...
[Cactus][layer_14] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_14] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_14] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_14] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] input_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_in_proj shape=[1, 3072] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_triplet shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_B_sliced shape=[1, 1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_B shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_C shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_X shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_bx shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_depthwise_weight_raw shape=[1024, 1, 3] showing 8/3072 values: 0.0023991, 0.0095965, 0.19433, -0.0047982, -0.0071973, -0.086368, -0.0071973, -0.019193 ...
[Cactus][layer_15] conv_depthwise_weight_reshaped_(Cout,1,K) shape=[1024, 1, 3] showing 8/3072 values: 1, 4, 81, -2, -3, -36, -3, -8 ...
[Cactus][layer_15] conv_depthwise_weight_c0_kernel_[K] shape=[3] showing 3/3 values: 1, 4, 81
[Cactus][layer_15] conv_cache_left shape=[2, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_input_concat shape=[3, 1024] per-token (first 8 dims):
  [tok0: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok1: nan,nan,nan,nan,nan,nan,nan,nan]
  [tok2: nan,nan,nan,nan,nan,nan,nan,nan]
[Cactus][layer_15] conv_input_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_output_NLC shape=[1, 3, 1024] showing 8/3072 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_output_LC shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_gated_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] conv_out_proj_[L,C] shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] block_main_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] post_block_residual shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] post_block_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] mlp_gate_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] mlp_up_linear shape=[1, 4608] showing 8/4608 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] mlp_gate_silu shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_15] mlp_gated shape=[1, 4608] showing 8/4608 values: -nan, -nan, -nan, -nan, -nan, -nan, -nan, -nan ...
[Cactus][layer_15] mlp_down_linear shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] mlp_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][layer_15] block_output shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
[Cactus][global] final_norm shape=[1, 1024] showing 8/1024 values: nan, nan, nan, nan, nan, nan, nan, nan ...
  [gen 5] token_id=0 text="<|pad|>"
Generated tokens: 0, 0, 0, 0, 0, 0
Generated text: <|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>
