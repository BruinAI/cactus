{
  "description": "Hyperparameter search configuration for Qwen 3 tool calling training",
  "search_space": {
    "learning_rate": [3e-6, 1e-5, 3e-5, 1e-4, 3e-4],
    "num_epochs": [1, 3, 5, 10],
    "lora_rank": [8, 16, 32, 64],
    "alpha_rank_ratio": [1.0, 2.0],
    "batch_size": [4, 8, 16]
  },
  "fixed_params": {
    "gradient_accumulation_steps": 1,
    "max_target_length": 1500,
    "eval_every_n_steps": 5
  },
  "notes": [
    "Learning rates: Typical range for LoRA fine-tuning (3e-6 to 3e-4)",
    "Epochs: Balance between overfitting and underfitting",
    "LoRA rank: Higher rank = more parameters but better expressiveness",
    "alpha_rank_ratio: Ratio of alpha to rank (1.0 means alpha=rank, 2.0 means alpha=2*rank)",
    "This creates sensible combinations: rank=8 with alpha=8 or 16, rank=16 with alpha=16 or 32, etc.",
    "Batch size: Constrained by TPU memory"
  ]
}
