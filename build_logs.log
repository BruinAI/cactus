Building Cactus library...
-- The CXX compiler identification is GNU 14.2.0
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- CMAKE_BUILD_TYPE was not set, defaulting to Release.
-- Configuring done (0.3s)
-- Generating done (0.0s)
-- Build files have been written to: /home/karen/Documents/cactus/cactus/build
[ 14%] Building CXX object CMakeFiles/cactus.dir/kernel/kernel_attention.cpp.o
[ 14%] Building CXX object CMakeFiles/cactus.dir/kernel/kernel_gemm.cpp.o
[ 14%] Building CXX object CMakeFiles/cactus.dir/kernel/kernel_conv.cpp.o
[ 14%] Building CXX object CMakeFiles/cactus.dir/kernel/kernel_blas.cpp.o
[ 18%] Building CXX object CMakeFiles/cactus.dir/kernel/kernel_nn.cpp.o
[ 22%] Building CXX object CMakeFiles/cactus.dir/kernel/kernel_quants.cpp.o
[ 25%] Building CXX object CMakeFiles/cactus.dir/kernel/kernel_reduce.cpp.o
[ 29%] Building CXX object CMakeFiles/cactus.dir/kernel/kernel_scalar.cpp.o
[ 33%] Building CXX object CMakeFiles/cactus.dir/graph/graph_builder.cpp.o
[ 37%] Building CXX object CMakeFiles/cactus.dir/graph/graph_core.cpp.o
[ 40%] Building CXX object CMakeFiles/cactus.dir/graph/graph_file.cpp.o
[ 44%] Building CXX object CMakeFiles/cactus.dir/graph/graph_ops.cpp.o
[ 48%] Building CXX object CMakeFiles/cactus.dir/engine/engine_bpe.cpp.o
[ 51%] Building CXX object CMakeFiles/cactus.dir/engine/engine_cache.cpp.o
[ 55%] Building CXX object CMakeFiles/cactus.dir/engine/engine_lfm2_vl_preprocessor.cpp.o
[ 59%] Building CXX object CMakeFiles/cactus.dir/engine/engine_model.cpp.o
[ 62%] Building CXX object CMakeFiles/cactus.dir/engine/engine_sp.cpp.o
[ 66%] Building CXX object CMakeFiles/cactus.dir/engine/engine_tokenizer.cpp.o
[ 70%] Building CXX object CMakeFiles/cactus.dir/ffi/cactus_ffi.cpp.o
[ 74%] Building CXX object CMakeFiles/cactus.dir/models/model_gemma.cpp.o
[ 77%] Building CXX object CMakeFiles/cactus.dir/models/model_lfm2.cpp.o
[ 81%] Building CXX object CMakeFiles/cactus.dir/models/model_lfm2vl.cpp.o
[ 85%] Building CXX object CMakeFiles/cactus.dir/models/model_nomic.cpp.o
[ 88%] Building CXX object CMakeFiles/cactus.dir/models/model_qwen.cpp.o
[ 92%] Building CXX object CMakeFiles/cactus.dir/models/model_siglip2.cpp.o
[ 96%] Building CXX object CMakeFiles/cactus.dir/models/model_smol.cpp.o
[100%] Linking CXX static library libcactus.a
[100%] Built target cactus
Cactus library built successfully!
Library location: /home/karen/Documents/cactus/cactus/build/lib/libcactus.a
-- The CXX compiler identification is GNU 14.2.0
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Added test: test_engine
-- Added test: test_graph
-- Added test: test_kernel
-- Added test: test_lfm2_preprocessor
-- Added test: test_lfm2vl_model
-- Added test: test_performance
-- Added test: test_siglip2_model
-- Added test: test_siglip2_preprocessor
-- Added test: test_siglip2_vision_model
-- Added test: test_vlm_pipeline
-- Added runner: siglip2_debug_runner
-- Configuring done (0.2s)
-- Generating done (0.0s)
-- Build files have been written to: /home/karen/Documents/cactus/tests/build
[ 12%] Building CXX object CMakeFiles/test_graph.dir/test_graph.cpp.o
[ 12%] Building CXX object CMakeFiles/test_kernel.dir/test_kernel.cpp.o
[ 12%] Building CXX object CMakeFiles/test_lfm2_preprocessor.dir/test_lfm2_preprocessor.cpp.o
[ 12%] Building CXX object CMakeFiles/test_engine.dir/test_engine.cpp.o
[ 15%] Building CXX object CMakeFiles/test_lfm2_preprocessor.dir/test_utils.cpp.o
[ 18%] Building CXX object CMakeFiles/test_kernel.dir/test_utils.cpp.o
[ 21%] Building CXX object CMakeFiles/test_engine.dir/test_utils.cpp.o
[ 25%] Building CXX object CMakeFiles/test_graph.dir/test_utils.cpp.o
[ 28%] Linking CXX executable test_lfm2_preprocessor
[ 28%] Built target test_lfm2_preprocessor
[ 31%] Building CXX object CMakeFiles/test_lfm2vl_model.dir/test_lfm2vl_model.cpp.o
[ 34%] Linking CXX executable test_kernel
[ 34%] Built target test_kernel
[ 37%] Building CXX object CMakeFiles/test_performance.dir/test_performance.cpp.o
[ 40%] Linking CXX executable test_engine
[ 40%] Built target test_engine
[ 43%] Building CXX object CMakeFiles/test_siglip2_model.dir/test_siglip2_model.cpp.o
[ 46%] Linking CXX executable test_graph
[ 46%] Built target test_graph
[ 50%] Building CXX object CMakeFiles/test_siglip2_preprocessor.dir/test_siglip2_preprocessor.cpp.o
[ 53%] Building CXX object CMakeFiles/test_lfm2vl_model.dir/test_utils.cpp.o
[ 56%] Building CXX object CMakeFiles/test_siglip2_model.dir/test_utils.cpp.o
[ 59%] Building CXX object CMakeFiles/test_siglip2_preprocessor.dir/test_utils.cpp.o
[ 62%] Linking CXX executable test_lfm2vl_model
[ 62%] Built target test_lfm2vl_model
[ 65%] Building CXX object CMakeFiles/test_performance.dir/test_utils.cpp.o
[ 68%] Linking CXX executable test_siglip2_model
[ 68%] Built target test_siglip2_model
[ 71%] Building CXX object CMakeFiles/test_siglip2_vision_model.dir/test_siglip2_vision_model.cpp.o
[ 75%] Linking CXX executable test_siglip2_preprocessor
[ 75%] Built target test_siglip2_preprocessor
[ 78%] Building CXX object CMakeFiles/test_vlm_pipeline.dir/test_vlm_pipeline.cpp.o
[ 81%] Building CXX object CMakeFiles/test_vlm_pipeline.dir/test_utils.cpp.o
[ 84%] Building CXX object CMakeFiles/test_siglip2_vision_model.dir/test_utils.cpp.o
[ 87%] Linking CXX executable test_performance
[ 87%] Built target test_performance
[ 90%] Building CXX object CMakeFiles/siglip2_debug_runner.dir/siglip2_debug_runner.cpp.o
[ 93%] Linking CXX executable test_vlm_pipeline
[ 93%] Built target test_vlm_pipeline
[ 96%] Linking CXX executable test_siglip2_vision_model
[ 96%] Built target test_siglip2_vision_model
[100%] Linking CXX executable siglip2_debug_runner
[100%] Built target siglip2_debug_runner
=== LFM2-VL Model Integration Test ===
Model folder : "/home/karen/Documents/cactus/tests/build/../../weights/lfm2-vl-350m-fp16"
Image path   : "/home/karen/Documents/cactus/tests/monkey-nose-muzzle-wallpaper.png"
Prompt       : Describe this image

[Siglip2VisionModel::ctor] config-based constructor called
[Siglip2VisionModel::ctor] preprocessor initialized
Initializing LFM2-VL model...
[Model::init] invoking load_weights_to_graph for folder=/home/karen/Documents/cactus/tests/build/../../weights/lfm2-vl-350m-fp16 embedding_file_path=/home/karen/Documents/cactus/tests/build/../../weights/lfm2-vl-350m-fp16/token_embeddings.weights
[Lfm2VlModel::load_weights_to_graph] projector_linear_1_weight mapped
[Lfm2VlModel::load_weights_to_graph] projector_linear_1_bias mapped
[Lfm2VlModel::load_weights_to_graph] projector_linear_2_weight mapped
[Lfm2VlModel::load_weights_to_graph] projector_linear_2_bias mapped
[Model::init] load_weights_to_graph completed
[Lfm2VlModel::init] Base model initialized for folder=/home/karen/Documents/cactus/tests/build/../../weights/lfm2-vl-350m-fp16
[Model::init] invoking load_weights_to_graph for folder=/home/karen/Documents/cactus/tests/build/../../weights/lfm2-vl-350m-fp16 embedding_file_path=/home/karen/Documents/cactus/tests/build/../../weights/lfm2-vl-350m-fp16/token_embeddings.weights
[Siglip2VisionModel::load_weights_to_graph] vision layers count=12
[Siglip2VisionModel::load_weights_to_graph] patch embedding nodes weight=6 bias=7
[Siglip2VisionModel::load_weights_to_graph] patch embedding weight shape: [Siglip2VisionModel::build_vision_embeddings] patch embedding weight node=6 precision=FP16 shape=[768, 768] total_size=589824
[Siglip2VisionModel::load_weights_to_graph] patch embedding bias shape: [Siglip2VisionModel::build_vision_embeddings] patch embedding bias node=7 precision=FP16 shape=[768] total_size=768
[Siglip2VisionModel::load_weights_to_graph] position_embedding node=8
[Siglip2VisionModel::load_weights_to_graph] post layernorm nodes weight=9 bias=10
[Siglip2VisionModel::load_weights_to_graph] loading vision layer=0
[Siglip2VisionModel::load_weights_to_graph] attention weights loaded for layer=0
[Siglip2VisionModel::load_weights_to_graph] layer norms loaded for layer=0
[Siglip2VisionModel::load_weights_to_graph] MLP weights loaded for layer=0
[Siglip2VisionModel::load_weights_to_graph] loading vision layer=1
[Siglip2VisionModel::load_weights_to_graph] attention weights loaded for layer=1
[Siglip2VisionModel::load_weights_to_graph] layer norms loaded for layer=1
[Siglip2VisionModel::load_weights_to_graph] MLP weights loaded for layer=1
[Siglip2VisionModel::load_weights_to_graph] loading vision layer=2
[Siglip2VisionModel::load_weights_to_graph] attention weights loaded for layer=2
[Siglip2VisionModel::load_weights_to_graph] layer norms loaded for layer=2
[Siglip2VisionModel::load_weights_to_graph] MLP weights loaded for layer=2
[Siglip2VisionModel::load_weights_to_graph] loading vision layer=3
[Siglip2VisionModel::load_weights_to_graph] attention weights loaded for layer=3
[Siglip2VisionModel::load_weights_to_graph] layer norms loaded for layer=3
[Siglip2VisionModel::load_weights_to_graph] MLP weights loaded for layer=3
[Siglip2VisionModel::load_weights_to_graph] loading vision layer=4
[Siglip2VisionModel::load_weights_to_graph] attention weights loaded for layer=4
[Siglip2VisionModel::load_weights_to_graph] layer norms loaded for layer=4
[Siglip2VisionModel::load_weights_to_graph] MLP weights loaded for layer=4
[Siglip2VisionModel::load_weights_to_graph] loading vision layer=5
[Siglip2VisionModel::load_weights_to_graph] attention weights loaded for layer=5
[Siglip2VisionModel::load_weights_to_graph] layer norms loaded for layer=5
[Siglip2VisionModel::load_weights_to_graph] MLP weights loaded for layer=5
[Siglip2VisionModel::load_weights_to_graph] loading vision layer=6
[Siglip2VisionModel::load_weights_to_graph] attention weights loaded for layer=6
[Siglip2VisionModel::load_weights_to_graph] layer norms loaded for layer=6
[Siglip2VisionModel::load_weights_to_graph] MLP weights loaded for layer=6
[Siglip2VisionModel::load_weights_to_graph] loading vision layer=7
[Siglip2VisionModel::load_weights_to_graph] attention weights loaded for layer=7
[Siglip2VisionModel::load_weights_to_graph] layer norms loaded for layer=7
[Siglip2VisionModel::load_weights_to_graph] MLP weights loaded for layer=7
[Siglip2VisionModel::load_weights_to_graph] loading vision layer=8
[Siglip2VisionModel::load_weights_to_graph] attention weights loaded for layer=8
[Siglip2VisionModel::load_weights_to_graph] layer norms loaded for layer=8
[Siglip2VisionModel::load_weights_to_graph] MLP weights loaded for layer=8
[Siglip2VisionModel::load_weights_to_graph] loading vision layer=9
[Siglip2VisionModel::load_weights_to_graph] attention weights loaded for layer=9
[Siglip2VisionModel::load_weights_to_graph] layer norms loaded for layer=9
[Siglip2VisionModel::load_weights_to_graph] MLP weights loaded for layer=9
[Siglip2VisionModel::load_weights_to_graph] loading vision layer=10
[Siglip2VisionModel::load_weights_to_graph] attention weights loaded for layer=10
[Siglip2VisionModel::load_weights_to_graph] layer norms loaded for layer=10
[Siglip2VisionModel::load_weights_to_graph] MLP weights loaded for layer=10
[Siglip2VisionModel::load_weights_to_graph] loading vision layer=11
[Siglip2VisionModel::load_weights_to_graph] attention weights loaded for layer=11
[Siglip2VisionModel::load_weights_to_graph] layer norms loaded for layer=11
[Siglip2VisionModel::load_weights_to_graph] MLP weights loaded for layer=11
[Model::init] load_weights_to_graph completed
[Lfm2VlModel::init] Vision tower initialized with folder=/home/karen/Documents/cactus/tests/build/../../weights/lfm2-vl-350m-fp16
[Lfm2VlModel::init] vision_weights_loaded_ set to true
[Model::init] invoking load_weights_to_graph for folder=/home/karen/Documents/cactus/tests/build/../../weights/lfm2-vl-350m-fp16 embedding_file_path=/home/karen/Documents/cactus/tests/build/../../weights/lfm2-vl-350m-fp16/token_embeddings.weights
[LFM2Model::load_weights_to_graph] begin
[LFM2Model::load_weights_to_graph] embedding_node_id_ mapped from /home/karen/Documents/cactus/tests/build/../../weights/lfm2-vl-350m-fp16/token_embeddings.weights
[LFM2Model::load_weights_to_graph] embedding_node_id_=203
[LFM2Model::load_weights_to_graph] output_norm_weight node=204
[LFM2Model::load_weights_to_graph] tie_word_embeddings true, output_weight_node_id_=203
[LFM2Model::load_weights_to_graph] loading layer=0
[LFM2Model::load_weights_to_graph] layer_type=conv is_conv_layer=1
[LFM2Model::load_weights_to_graph] conv weights nodes: in_proj=205 depthwise=207 out_proj=206
[LFM2Model::load_weights_to_graph] layer norms and ffn weights loaded for layer=0
[LFM2Model::load_weights_to_graph] loading layer=1
[LFM2Model::load_weights_to_graph] layer_type=conv is_conv_layer=1
[LFM2Model::load_weights_to_graph] conv weights nodes: in_proj=213 depthwise=215 out_proj=214
[LFM2Model::load_weights_to_graph] layer norms and ffn weights loaded for layer=1
[LFM2Model::load_weights_to_graph] loading layer=2
[LFM2Model::load_weights_to_graph] layer_type=full_attention is_conv_layer=0
[LFM2Model::load_weights_to_graph] attention weights nodes for layer=2
[LFM2Model::load_weights_to_graph] layer norms and ffn weights loaded for layer=2
[LFM2Model::load_weights_to_graph] loading layer=3
[LFM2Model::load_weights_to_graph] layer_type=conv is_conv_layer=1
[LFM2Model::load_weights_to_graph] conv weights nodes: in_proj=232 depthwise=234 out_proj=233
[LFM2Model::load_weights_to_graph] layer norms and ffn weights loaded for layer=3
[LFM2Model::load_weights_to_graph] loading layer=4
[LFM2Model::load_weights_to_graph] layer_type=conv is_conv_layer=1
[LFM2Model::load_weights_to_graph] conv weights nodes: in_proj=240 depthwise=242 out_proj=241
[LFM2Model::load_weights_to_graph] layer norms and ffn weights loaded for layer=4
[LFM2Model::load_weights_to_graph] loading layer=5
[LFM2Model::load_weights_to_graph] layer_type=full_attention is_conv_layer=0
[LFM2Model::load_weights_to_graph] attention weights nodes for layer=5
[LFM2Model::load_weights_to_graph] layer norms and ffn weights loaded for layer=5
[LFM2Model::load_weights_to_graph] loading layer=6
[LFM2Model::load_weights_to_graph] layer_type=conv is_conv_layer=1
[LFM2Model::load_weights_to_graph] conv weights nodes: in_proj=259 depthwise=261 out_proj=260
[LFM2Model::load_weights_to_graph] layer norms and ffn weights loaded for layer=6
[LFM2Model::load_weights_to_graph] loading layer=7
[LFM2Model::load_weights_to_graph] layer_type=conv is_conv_layer=1
[LFM2Model::load_weights_to_graph] conv weights nodes: in_proj=267 depthwise=269 out_proj=268
[LFM2Model::load_weights_to_graph] layer norms and ffn weights loaded for layer=7
[LFM2Model::load_weights_to_graph] loading layer=8
[LFM2Model::load_weights_to_graph] layer_type=full_attention is_conv_layer=0
[LFM2Model::load_weights_to_graph] attention weights nodes for layer=8
[LFM2Model::load_weights_to_graph] layer norms and ffn weights loaded for layer=8
[LFM2Model::load_weights_to_graph] loading layer=9
[LFM2Model::load_weights_to_graph] layer_type=conv is_conv_layer=1
[LFM2Model::load_weights_to_graph] conv weights nodes: in_proj=286 depthwise=288 out_proj=287
[LFM2Model::load_weights_to_graph] layer norms and ffn weights loaded for layer=9
[LFM2Model::load_weights_to_graph] loading layer=10
[LFM2Model::load_weights_to_graph] layer_type=full_attention is_conv_layer=0
[LFM2Model::load_weights_to_graph] attention weights nodes for layer=10
[LFM2Model::load_weights_to_graph] layer norms and ffn weights loaded for layer=10
[LFM2Model::load_weights_to_graph] loading layer=11
[LFM2Model::load_weights_to_graph] layer_type=conv is_conv_layer=1
[LFM2Model::load_weights_to_graph] conv weights nodes: in_proj=305 depthwise=307 out_proj=306
[LFM2Model::load_weights_to_graph] layer norms and ffn weights loaded for layer=11
[LFM2Model::load_weights_to_graph] loading layer=12
[LFM2Model::load_weights_to_graph] layer_type=full_attention is_conv_layer=0
[LFM2Model::load_weights_to_graph] attention weights nodes for layer=12
[LFM2Model::load_weights_to_graph] layer norms and ffn weights loaded for layer=12
[LFM2Model::load_weights_to_graph] loading layer=13
[LFM2Model::load_weights_to_graph] layer_type=conv is_conv_layer=1
[LFM2Model::load_weights_to_graph] conv weights nodes: in_proj=324 depthwise=326 out_proj=325
[LFM2Model::load_weights_to_graph] layer norms and ffn weights loaded for layer=13
[LFM2Model::load_weights_to_graph] loading layer=14
[LFM2Model::load_weights_to_graph] layer_type=full_attention is_conv_layer=0
[LFM2Model::load_weights_to_graph] attention weights nodes for layer=14
[LFM2Model::load_weights_to_graph] layer norms and ffn weights loaded for layer=14
[LFM2Model::load_weights_to_graph] loading layer=15
[LFM2Model::load_weights_to_graph] layer_type=conv is_conv_layer=1
[LFM2Model::load_weights_to_graph] conv weights nodes: in_proj=343 depthwise=345 out_proj=344
[LFM2Model::load_weights_to_graph] layer norms and ffn weights loaded for layer=15
[LFM2Model::load_weights_to_graph] finished
[Model::init] load_weights_to_graph completed
[LFM2Model::post_init] conv_cache_ initialized with conv_window=2
[LFM2Model::post_init] last_forward_used_cache_ reset
[LFM2Model::init] base Model::init succeeded using shared graph for folder=/home/karen/Documents/cactus/tests/build/../../weights/lfm2-vl-350m-fp16
[LFM2Model::init] conv_cache_bx_nodes_ assigned zeros size=16
[Lfm2VlModel::init] Language model initialized
[Lfm2VlModel::init] language_weights_loaded_ set to true
Model initialized successfully.

Formatting chat prompt using tokenizer template...
[Lfm2VlPreprocessor::round_by_factor] number=512 factor=32 result=512
[Lfm2VlPreprocessor::round_by_factor] number=512 factor=32 result=512
[Lfm2VlPreprocessor::smart_resize] initial h_bar=512 w_bar=512
[Lfm2VlPreprocessor::round_by_factor] number=512 factor=32 result=512
[Lfm2VlPreprocessor::round_by_factor] number=512 factor=32 result=512
[Lfm2VlPreprocessor::is_image_too_large] height=512 width=512 pixels=262144 limit=524288 result=0
[Lfm2VlPreprocessor::compute_spatial_shapes] height=512 width=512 resized_width=512 resized_height=512 should_split=0
[Lfm2VlPreprocessor::compute_spatial_shapes] single shape=32x32
Formatted prompt:
<|startoftext|><|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|image_start|><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><|image_end|>Describe this image<|im_end|>
<|im_start|>assistant


[Lfm2VlPreprocessor::round_by_factor] number=512 factor=32 result=512
[Lfm2VlPreprocessor::round_by_factor] number=512 factor=32 result=512
[Lfm2VlPreprocessor::smart_resize] initial h_bar=512 w_bar=512
[Lfm2VlPreprocessor::round_by_factor] number=512 factor=32 result=512
[Lfm2VlPreprocessor::round_by_factor] number=512 factor=32 result=512
[Lfm2VlPreprocessor::is_image_too_large] height=512 width=512 pixels=262144 limit=524288 result=0
[Lfm2VlPreprocessor::compute_spatial_shapes] height=512 width=512 resized_width=512 resized_height=512 should_split=0
[Lfm2VlPreprocessor::compute_spatial_shapes] single shape=32x32
Token count: 282
Tokens: 1, 6, 24131, 708, 4083, 938, 768, 11538, 16701, 523, 7, 708, 6, 6423, 708, 498, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 499, 11980, 9918, 1033, 4646, 7, 708, 6, 64015, 708

Starting multimodal generation (up to 64 tokens)...
[Lfm2VlModel::generate_with_images] called with tokens size=282 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlPreprocessor::round_by_factor] number=512 factor=32 result=512
[Lfm2VlPreprocessor::round_by_factor] number=512 factor=32 result=512
[Lfm2VlPreprocessor::smart_resize] initial h_bar=512 w_bar=512
[Lfm2VlPreprocessor::preprocess_from_memory] resized_width=512 resized_height=512
[Lfm2VlPreprocessor::round_by_factor] number=512 factor=32 result=512
[Lfm2VlPreprocessor::round_by_factor] number=512 factor=32 result=512
[Lfm2VlPreprocessor::is_image_too_large] height=512 width=512 pixels=262144 limit=524288 result=0
[Lfm2VlPreprocessor::preprocess_from_memory] allow_splitting=1 should_split=0
[Lfm2VlPreprocessor::normalize_image] normalized image with width=512 height=512
[Lfm2VlPreprocessor::preprocess_from_memory] using original dimensions
[Lfm2VlPreprocessor::pad_patches] num_tiles=1 max_patches_per_tile=1024 patch_dim=768
[Lfm2VlPreprocessor::pad_patches] copied tile_idx=0 actual_patches=1024
[Lfm2VlPreprocessor::pad_patches] pixel_values_shape set
[Lfm2VlPreprocessor::pad_patches] spatial_shapes_shape set
[Lfm2VlPreprocessor::preprocess_from_memory] pad_patches returned num_tiles=1
[Lfm2VlPreprocessor::preprocess_from_memory] tokens_per_tile=256
[Lfm2VlPreprocessor::preprocess_from_file] image_path=/home/karen/Documents/cactus/tests/monkey-nose-muzzle-wallpaper.png width=512 height=512 channels=3
[Siglip2VisionModel::build_vision_embeddings] Starting to build vision embeddings
[Siglip2VisionModel::build_vision_embeddings] num_tiles=1 max_patches=1024 patch_dim=768
[Siglip2VisionModel::build_vision_embeddings] reshaped_weight node=351
[Siglip2VisionModel::build_vision_embeddings] patch_embedding_weight buffer (raw) node=6 precision=FP16 shape=[768, 768] total_size=589824
[Siglip2VisionModel::build_vision_embeddings] patch_embedding_bias buffer (raw) node=7 precision=FP16 shape=[768] total_size=768
[Siglip2VisionModel::build_vision_embeddings] position_embedding buffer (raw) node=8 precision=FP16 shape=[256, 768] total_size=196608
[Siglip2VisionModel::build_vision_embeddings] reshaped_weight buffer node=351 precision=FP16 shape=[768, 768] total_size=589824
[Siglip2VisionModel::build_vision_embeddings] patch_embedding_bias buffer node=7 precision=FP16 shape=[768] total_size=768
[Siglip2VisionModel::build_vision_embeddings] tile_idx=0 tile_h=32 tile_w=32 actual_patches=1024
[Siglip2VisionModel::build_vision_embeddings] tile_input_fp32 node=352
[Siglip2VisionModel::build_vision_embeddings] tile_input_fp32 buffer node=352 precision=FP32 shape=[1024, 768] total_size=786432
[Siglip2VisionModel::build_vision_embeddings] tile_input node=353
[Siglip2VisionModel::build_vision_embeddings] tile_input_fp16 buffer node=353 precision=FP16 shape=[1024, 768] total_size=786432
[Siglip2VisionModel::build_vision_embeddings] reshaped_weight buffer node=351 precision=FP16 shape=[768, 768] total_size=589824
[Siglip2VisionModel::build_vision_embeddings] tile_patch buffer node=354 precision=FP16 shape=[1024, 768] total_size=786432
[Siglip2VisionModel::build_vision_embeddings] patch_embedding_bias_reshaped buffer node=7 precision=FP16 shape=[768] total_size=768
[Siglip2VisionModel::build_vision_embeddings] tile_bias buffer node=355 precision=FP16 shape=[1024, 768] total_size=786432
[Siglip2VisionModel::build_vision_embeddings] tile_patch node=354 tile_bias node=355
[Siglip2VisionModel::build_vision_embeddings] positional destination sizes32 x 32
[Siglip2VisionModel::build_vision_embeddings] tile_pos buffer node=356 precision=FP32 shape=[1024, 768] total_size=786432
[Siglip2VisionModel::build_vision_embeddings] tile_pos node=356
[Siglip2VisionModel::build_vision_embeddings] tile_pos_cast buffer node=357 precision=FP16 shape=[1024, 768] total_size=786432
[Siglip2VisionModel::build_vision_embeddings] tile_bias buffer before add_pos node=355 precision=FP16 shape=[1024, 768] total_size=786432
[Siglip2VisionModel::build_vision_embeddings] tile_embed buffer node=358 precision=FP16 shape=[1024, 768] total_size=786432
[Siglip2VisionModel::build_vision_embeddings] tile_embed node=358
[Siglip2VisionModel::build_vision_embeddings] tile_embeddings size=1
[Siglip2VisionModel::build_vision_embeddings] embeddings node=358
[Siglip2VisionModel::forward_vision external] embedding_result.combined=358 tile_embeddings size=1
[Siglip2VisionModel::forward_vision external] processing tile=0 node=358
[Siglip2VisionModel::build_vision_transformer_layer] normalized before attn node=359
[Siglip2VisionModel::build_vision_attention] q node=361
[Siglip2VisionModel::build_vision_attention] k node=363
[Siglip2VisionModel::build_vision_attention] v node=365
[Siglip2VisionModel::build_vision_attention] q_4d=366 k_4d=367 v_4d=368
[Siglip2VisionModel::build_vision_attention] attn_output node=369
[Siglip2VisionModel::build_vision_attention] attn_2d node=370
[Siglip2VisionModel::build_vision_attention] output node=372
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after attn node=373
[Siglip2VisionModel::build_vision_transformer_layer] normalized before mlp node=374
[Siglip2VisionModel::build_vision_mlp] fc1_output node=376
[Siglip2VisionModel::build_vision_mlp] activated node=377
[Siglip2VisionModel::build_vision_mlp] fc2_output node=379
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after mlp node=380
[Siglip2VisionModel::forward_vision external] tile=0 after layer=0 node=380
[Siglip2VisionModel::build_vision_transformer_layer] normalized before attn node=381
[Siglip2VisionModel::build_vision_attention] q node=383
[Siglip2VisionModel::build_vision_attention] k node=385
[Siglip2VisionModel::build_vision_attention] v node=387
[Siglip2VisionModel::build_vision_attention] q_4d=388 k_4d=389 v_4d=390
[Siglip2VisionModel::build_vision_attention] attn_output node=391
[Siglip2VisionModel::build_vision_attention] attn_2d node=392
[Siglip2VisionModel::build_vision_attention] output node=394
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after attn node=395
[Siglip2VisionModel::build_vision_transformer_layer] normalized before mlp node=396
[Siglip2VisionModel::build_vision_mlp] fc1_output node=398
[Siglip2VisionModel::build_vision_mlp] activated node=399
[Siglip2VisionModel::build_vision_mlp] fc2_output node=401
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after mlp node=402
[Siglip2VisionModel::forward_vision external] tile=0 after layer=1 node=402
[Siglip2VisionModel::build_vision_transformer_layer] normalized before attn node=403
[Siglip2VisionModel::build_vision_attention] q node=405
[Siglip2VisionModel::build_vision_attention] k node=407
[Siglip2VisionModel::build_vision_attention] v node=409
[Siglip2VisionModel::build_vision_attention] q_4d=410 k_4d=411 v_4d=412
[Siglip2VisionModel::build_vision_attention] attn_output node=413
[Siglip2VisionModel::build_vision_attention] attn_2d node=414
[Siglip2VisionModel::build_vision_attention] output node=416
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after attn node=417
[Siglip2VisionModel::build_vision_transformer_layer] normalized before mlp node=418
[Siglip2VisionModel::build_vision_mlp] fc1_output node=420
[Siglip2VisionModel::build_vision_mlp] activated node=421
[Siglip2VisionModel::build_vision_mlp] fc2_output node=423
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after mlp node=424
[Siglip2VisionModel::forward_vision external] tile=0 after layer=2 node=424
[Siglip2VisionModel::build_vision_transformer_layer] normalized before attn node=425
[Siglip2VisionModel::build_vision_attention] q node=427
[Siglip2VisionModel::build_vision_attention] k node=429
[Siglip2VisionModel::build_vision_attention] v node=431
[Siglip2VisionModel::build_vision_attention] q_4d=432 k_4d=433 v_4d=434
[Siglip2VisionModel::build_vision_attention] attn_output node=435
[Siglip2VisionModel::build_vision_attention] attn_2d node=436
[Siglip2VisionModel::build_vision_attention] output node=438
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after attn node=439
[Siglip2VisionModel::build_vision_transformer_layer] normalized before mlp node=440
[Siglip2VisionModel::build_vision_mlp] fc1_output node=442
[Siglip2VisionModel::build_vision_mlp] activated node=443
[Siglip2VisionModel::build_vision_mlp] fc2_output node=445
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after mlp node=446
[Siglip2VisionModel::forward_vision external] tile=0 after layer=3 node=446
[Siglip2VisionModel::build_vision_transformer_layer] normalized before attn node=447
[Siglip2VisionModel::build_vision_attention] q node=449
[Siglip2VisionModel::build_vision_attention] k node=451
[Siglip2VisionModel::build_vision_attention] v node=453
[Siglip2VisionModel::build_vision_attention] q_4d=454 k_4d=455 v_4d=456
[Siglip2VisionModel::build_vision_attention] attn_output node=457
[Siglip2VisionModel::build_vision_attention] attn_2d node=458
[Siglip2VisionModel::build_vision_attention] output node=460
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after attn node=461
[Siglip2VisionModel::build_vision_transformer_layer] normalized before mlp node=462
[Siglip2VisionModel::build_vision_mlp] fc1_output node=464
[Siglip2VisionModel::build_vision_mlp] activated node=465
[Siglip2VisionModel::build_vision_mlp] fc2_output node=467
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after mlp node=468
[Siglip2VisionModel::forward_vision external] tile=0 after layer=4 node=468
[Siglip2VisionModel::build_vision_transformer_layer] normalized before attn node=469
[Siglip2VisionModel::build_vision_attention] q node=471
[Siglip2VisionModel::build_vision_attention] k node=473
[Siglip2VisionModel::build_vision_attention] v node=475
[Siglip2VisionModel::build_vision_attention] q_4d=476 k_4d=477 v_4d=478
[Siglip2VisionModel::build_vision_attention] attn_output node=479
[Siglip2VisionModel::build_vision_attention] attn_2d node=480
[Siglip2VisionModel::build_vision_attention] output node=482
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after attn node=483
[Siglip2VisionModel::build_vision_transformer_layer] normalized before mlp node=484
[Siglip2VisionModel::build_vision_mlp] fc1_output node=486
[Siglip2VisionModel::build_vision_mlp] activated node=487
[Siglip2VisionModel::build_vision_mlp] fc2_output node=489
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after mlp node=490
[Siglip2VisionModel::forward_vision external] tile=0 after layer=5 node=490
[Siglip2VisionModel::build_vision_transformer_layer] normalized before attn node=491
[Siglip2VisionModel::build_vision_attention] q node=493
[Siglip2VisionModel::build_vision_attention] k node=495
[Siglip2VisionModel::build_vision_attention] v node=497
[Siglip2VisionModel::build_vision_attention] q_4d=498 k_4d=499 v_4d=500
[Siglip2VisionModel::build_vision_attention] attn_output node=501
[Siglip2VisionModel::build_vision_attention] attn_2d node=502
[Siglip2VisionModel::build_vision_attention] output node=504
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after attn node=505
[Siglip2VisionModel::build_vision_transformer_layer] normalized before mlp node=506
[Siglip2VisionModel::build_vision_mlp] fc1_output node=508
[Siglip2VisionModel::build_vision_mlp] activated node=509
[Siglip2VisionModel::build_vision_mlp] fc2_output node=511
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after mlp node=512
[Siglip2VisionModel::forward_vision external] tile=0 after layer=6 node=512
[Siglip2VisionModel::build_vision_transformer_layer] normalized before attn node=513
[Siglip2VisionModel::build_vision_attention] q node=515
[Siglip2VisionModel::build_vision_attention] k node=517
[Siglip2VisionModel::build_vision_attention] v node=519
[Siglip2VisionModel::build_vision_attention] q_4d=520 k_4d=521 v_4d=522
[Siglip2VisionModel::build_vision_attention] attn_output node=523
[Siglip2VisionModel::build_vision_attention] attn_2d node=524
[Siglip2VisionModel::build_vision_attention] output node=526
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after attn node=527
[Siglip2VisionModel::build_vision_transformer_layer] normalized before mlp node=528
[Siglip2VisionModel::build_vision_mlp] fc1_output node=530
[Siglip2VisionModel::build_vision_mlp] activated node=531
[Siglip2VisionModel::build_vision_mlp] fc2_output node=533
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after mlp node=534
[Siglip2VisionModel::forward_vision external] tile=0 after layer=7 node=534
[Siglip2VisionModel::build_vision_transformer_layer] normalized before attn node=535
[Siglip2VisionModel::build_vision_attention] q node=537
[Siglip2VisionModel::build_vision_attention] k node=539
[Siglip2VisionModel::build_vision_attention] v node=541
[Siglip2VisionModel::build_vision_attention] q_4d=542 k_4d=543 v_4d=544
[Siglip2VisionModel::build_vision_attention] attn_output node=545
[Siglip2VisionModel::build_vision_attention] attn_2d node=546
[Siglip2VisionModel::build_vision_attention] output node=548
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after attn node=549
[Siglip2VisionModel::build_vision_transformer_layer] normalized before mlp node=550
[Siglip2VisionModel::build_vision_mlp] fc1_output node=552
[Siglip2VisionModel::build_vision_mlp] activated node=553
[Siglip2VisionModel::build_vision_mlp] fc2_output node=555
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after mlp node=556
[Siglip2VisionModel::forward_vision external] tile=0 after layer=8 node=556
[Siglip2VisionModel::build_vision_transformer_layer] normalized before attn node=557
[Siglip2VisionModel::build_vision_attention] q node=559
[Siglip2VisionModel::build_vision_attention] k node=561
[Siglip2VisionModel::build_vision_attention] v node=563
[Siglip2VisionModel::build_vision_attention] q_4d=564 k_4d=565 v_4d=566
[Siglip2VisionModel::build_vision_attention] attn_output node=567
[Siglip2VisionModel::build_vision_attention] attn_2d node=568
[Siglip2VisionModel::build_vision_attention] output node=570
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after attn node=571
[Siglip2VisionModel::build_vision_transformer_layer] normalized before mlp node=572
[Siglip2VisionModel::build_vision_mlp] fc1_output node=574
[Siglip2VisionModel::build_vision_mlp] activated node=575
[Siglip2VisionModel::build_vision_mlp] fc2_output node=577
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after mlp node=578
[Siglip2VisionModel::forward_vision external] tile=0 after layer=9 node=578
[Siglip2VisionModel::build_vision_transformer_layer] normalized before attn node=579
[Siglip2VisionModel::build_vision_attention] q node=581
[Siglip2VisionModel::build_vision_attention] k node=583
[Siglip2VisionModel::build_vision_attention] v node=585
[Siglip2VisionModel::build_vision_attention] q_4d=586 k_4d=587 v_4d=588
[Siglip2VisionModel::build_vision_attention] attn_output node=589
[Siglip2VisionModel::build_vision_attention] attn_2d node=590
[Siglip2VisionModel::build_vision_attention] output node=592
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after attn node=593
[Siglip2VisionModel::build_vision_transformer_layer] normalized before mlp node=594
[Siglip2VisionModel::build_vision_mlp] fc1_output node=596
[Siglip2VisionModel::build_vision_mlp] activated node=597
[Siglip2VisionModel::build_vision_mlp] fc2_output node=599
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after mlp node=600
[Siglip2VisionModel::forward_vision external] tile=0 after layer=10 node=600
[Siglip2VisionModel::build_vision_transformer_layer] normalized before attn node=601
[Siglip2VisionModel::build_vision_attention] q node=603
[Siglip2VisionModel::build_vision_attention] k node=605
[Siglip2VisionModel::build_vision_attention] v node=607
[Siglip2VisionModel::build_vision_attention] q_4d=608 k_4d=609 v_4d=610
[Siglip2VisionModel::build_vision_attention] attn_output node=611
[Siglip2VisionModel::build_vision_attention] attn_2d node=612
[Siglip2VisionModel::build_vision_attention] output node=614
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after attn node=615
[Siglip2VisionModel::build_vision_transformer_layer] normalized before mlp node=616
[Siglip2VisionModel::build_vision_mlp] fc1_output node=618
[Siglip2VisionModel::build_vision_mlp] activated node=619
[Siglip2VisionModel::build_vision_mlp] fc2_output node=621
[Siglip2VisionModel::build_vision_transformer_layer] hidden_states after mlp node=622
[Siglip2VisionModel::forward_vision external] tile=0 after layer=11 node=622
[Siglip2VisionModel::forward_vision external] tile=0 post layer norm node=623
[Siglip2VisionModel::forward_vision external] tile_outputs size=1
[Siglip2VisionModel::forward_vision external] combined_output node=623
[Lfm2VlModel::get_image_features] vision_output node=623
[Lfm2VlModel::get_image_features] tile_idx=0 tile_h=32 tile_w=32 tile_tokens=1024 factor=2
[Lfm2VlModel::get_image_features] new_h=16 new_w=16 projected_tokens=256
[Lfm2VlModel::get_image_features] tile_features node=624 offset=0
[Lfm2VlModel::get_image_features] updated offset=1024
[Lfm2VlModel::get_image_features] reshaped node=625
[Lfm2VlModel::build_multimodal_projector] vision_hidden=768 tile_h=32 tile_w=32
[Lfm2VlModel::pixel_unshuffle] factor=2 new_height=16 new_width=16
[Lfm2VlModel::pixel_unshuffle] step1 node=627
[Lfm2VlModel::pixel_unshuffle] step1 transposed node=628
[Lfm2VlModel::pixel_unshuffle] step2 node=629
[Lfm2VlModel::pixel_unshuffle] result node=630
[Lfm2VlModel::build_multimodal_projector] unshuffled node=630
[Lfm2VlModel::build_multimodal_projector] factor=2 new_h=16 new_w=16 in_channels=3072 seq_len=256
[Lfm2VlModel::build_multimodal_projector] flattened node=631
[Lfm2VlModel::build_multimodal_projector] normalized node=632
[Lfm2VlModel::build_multimodal_projector] hidden node after linear1=633
[Lfm2VlModel::build_multimodal_projector] hidden node after bias1=634
[Lfm2VlModel::build_multimodal_projector] hidden node after GELU=635
[Lfm2VlModel::build_multimodal_projector] output node after linear2=636
[Lfm2VlModel::build_multimodal_projector] output node after bias2=637
[Lfm2VlModel::get_image_features] projected node=637
[Lfm2VlModel::get_image_features] pushed ProjectedTileFeature with node_id=637 token_count=256
[Lfm2VlModel::forward_images] text_embedding_inputs reserved size
[Lfm2VlModel::merge_image_text_embeddings_special_tokens] image_start_id=498 image_end_id=499 image_token_id=396
[Lfm2VlModel::merge_image_text_embeddings] starting merge with tokens size=282
[LFM2_DEBUG] merge_start tokens=282 image_embeddings=1
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=0 token_id=1
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=1
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 1
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=1 token_id=6
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=2
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 2
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=2 token_id=24131
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=3
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 3
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=3 token_id=708
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=4
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 4
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=4 token_id=4083
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=5
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 5
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=5 token_id=938
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=6
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 6
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=6 token_id=768
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=7
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 7
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=7 token_id=11538
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=8
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 8
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=8 token_id=16701
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=9
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 9
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=9 token_id=523
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=10
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 10
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=10 token_id=7
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=11
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 11
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=11 token_id=708
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=12
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 12
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=12 token_id=6
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=13
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 13
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=13 token_id=6423
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=14
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 14
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=14 token_id=708
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=15
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 15
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=15 token_id=498
[Lfm2VlModel::merge_image_text_embeddings] flush_segment segment_len=15
[Lfm2VlModel::merge_image_text_embeddings] created input node=638
[LFM2-VL] embedding tensor shape: [65536, 1024] segment_len=15
[Lfm2VlModel::merge_image_text_embeddings] embedding_node=639
[Lfm2VlModel::merge_image_text_embeddings] text_embedding_inputs size=1
[Lfm2VlModel::merge_image_text_embeddings] sequence_nodes size=1
[Lfm2VlModel::merge_image_text_embeddings] total_seq_len updated=15
[Lfm2VlModel::merge_image_text_embeddings] current_segment cleared
[Lfm2VlModel::merge_image_text_embeddings] advanced past image_start, token_index=16 image_index=0
[Lfm2VlModel::merge_image_text_embeddings] tiles size=1
[LFM2_DEBUG] image_block_start image_index=0 tiles=1
[Lfm2VlModel::merge_image_text_embeddings] inner_token at index=16 value=396
[Lfm2VlModel::merge_image_text_embeddings] flush_segment segment_len=1
[Lfm2VlModel::merge_image_text_embeddings] created input node=640
[LFM2-VL] embedding tensor shape: [65536, 1024] segment_len=1
[Lfm2VlModel::merge_image_text_embeddings] embedding_node=641
[Lfm2VlModel::merge_image_text_embeddings] text_embedding_inputs size=2
[Lfm2VlModel::merge_image_text_embeddings] sequence_nodes size=2
[Lfm2VlModel::merge_image_text_embeddings] total_seq_len updated=16
[Lfm2VlModel::merge_image_text_embeddings] current_segment cleared
[LFM2_DEBUG] image_placeholder token_index=16 tile_index=0 tiles=1
[Lfm2VlModel::merge_image_text_embeddings] inserting tile node=637 token_count=256
[LFM2_DEBUG] insert_tile image_index=0 tile_index=0 token_count=256
[Lfm2VlModel::merge_image_text_embeddings] sequence_nodes size after tile=3
[Lfm2VlModel::merge_image_text_embeddings] total_seq_len after tile=272
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=17
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=18
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=19
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=20
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=21
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=22
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=23
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=24
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=25
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=26
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=27
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=28
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=29
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=30
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=31
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=32
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=33
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=34
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=35
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=36
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=37
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=38
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=39
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=40
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=41
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=42
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=43
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=44
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=45
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=46
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=47
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=48
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=49
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=50
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=51
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=52
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=53
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=54
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=55
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=56
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=57
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=58
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=59
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=60
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=61
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=62
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=63
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=64
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=65
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=66
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=67
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=68
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=69
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=70
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=71
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=72
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=73
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=74
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=75
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=76
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=77
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=78
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=79
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=80
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=81
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=82
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=83
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=84
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=85
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=86
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=87
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=88
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=89
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=90
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=91
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=92
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=93
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=94
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=95
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=96
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=97
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=98
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=99
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=100
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=101
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=102
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=103
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=104
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=105
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=106
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=107
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=108
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=109
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=110
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=111
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=112
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=113
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=114
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=115
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=116
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=117
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=118
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=119
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=120
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=121
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=122
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=123
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=124
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=125
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=126
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=127
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=128
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=129
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=130
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=131
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=132
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=133
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=134
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=135
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=136
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=137
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=138
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=139
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=140
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=141
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=142
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=143
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=144
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=145
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=146
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=147
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=148
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=149
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=150
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=151
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=152
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=153
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=154
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=155
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=156
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=157
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=158
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=159
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=160
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=161
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=162
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=163
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=164
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=165
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=166
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=167
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=168
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=169
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=170
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=171
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=172
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=173
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=174
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=175
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=176
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=177
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=178
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=179
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=180
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=181
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=182
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=183
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=184
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=185
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=186
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=187
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=188
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=189
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=190
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=191
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=192
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=193
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=194
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=195
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=196
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=197
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=198
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=199
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=200
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=201
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=202
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=203
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=204
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=205
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=206
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=207
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=208
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=209
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=210
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=211
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=212
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=213
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=214
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=215
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=216
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=217
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=218
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=219
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=220
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=221
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=222
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=223
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=224
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=225
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=226
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=227
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=228
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=229
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=230
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=231
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=232
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=233
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=234
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=235
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=236
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=237
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=238
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=239
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=240
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=241
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=242
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=243
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=244
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=245
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=246
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=247
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=248
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=249
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=250
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=251
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=252
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=253
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=254
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=255
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=256
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=257
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=258
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=259
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=260
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=261
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=262
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=263
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=264
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=265
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=266
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=267
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=268
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=269
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=270
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=271
[Lfm2VlModel::merge_image_text_embeddings] consumed image token, token_index=272
[Lfm2VlModel::merge_image_text_embeddings] inner_token at index=272 value=499
[Lfm2VlModel::merge_image_text_embeddings] appended inner token to current_segment, size=1
[Lfm2VlModel::merge_image_text_embeddings] incremented token_index to 273
[Lfm2VlModel::merge_image_text_embeddings] flush_segment segment_len=1
[Lfm2VlModel::merge_image_text_embeddings] created input node=642
[LFM2-VL] embedding tensor shape: [65536, 1024] segment_len=1
[Lfm2VlModel::merge_image_text_embeddings] embedding_node=643
[Lfm2VlModel::merge_image_text_embeddings] text_embedding_inputs size=3
[Lfm2VlModel::merge_image_text_embeddings] sequence_nodes size=4
[Lfm2VlModel::merge_image_text_embeddings] total_seq_len updated=273
[Lfm2VlModel::merge_image_text_embeddings] current_segment cleared
[Lfm2VlModel::merge_image_text_embeddings] completed image block, image_index=1
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=273 token_id=11980
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=1
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 274
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=274 token_id=9918
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=2
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 275
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=275 token_id=1033
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=3
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 276
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=276 token_id=4646
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=4
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 277
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=277 token_id=7
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=5
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 278
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=278 token_id=708
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=6
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 279
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=279 token_id=6
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=7
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 280
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=280 token_id=64015
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=8
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 281
[Lfm2VlModel::merge_image_text_embeddings] processing token_index=281 token_id=708
[Lfm2VlModel::merge_image_text_embeddings] appended token to current_segment, size=9
[Lfm2VlModel::merge_image_text_embeddings] token_index incremented to 282
[Lfm2VlModel::merge_image_text_embeddings] flush_segment segment_len=9
[Lfm2VlModel::merge_image_text_embeddings] created input node=644
[LFM2-VL] embedding tensor shape: [65536, 1024] segment_len=9
[Lfm2VlModel::merge_image_text_embeddings] embedding_node=645
[Lfm2VlModel::merge_image_text_embeddings] text_embedding_inputs size=4
[Lfm2VlModel::merge_image_text_embeddings] sequence_nodes size=5
[Lfm2VlModel::merge_image_text_embeddings] total_seq_len updated=282
[Lfm2VlModel::merge_image_text_embeddings] current_segment cleared
[Lfm2VlModel::merge_image_text_embeddings] finished main loop, total_seq_len=282
[Lfm2VlModel::merge_image_text_embeddings] initial merged node=639
[Lfm2VlModel::merge_image_text_embeddings] concatenated node idx=1 merged node now=646
[Lfm2VlModel::merge_image_text_embeddings] concatenated node idx=2 merged node now=647
[Lfm2VlModel::merge_image_text_embeddings] concatenated node idx=3 merged node now=648
[Lfm2VlModel::merge_image_text_embeddings] concatenated node idx=4 merged node now=649
[Lfm2VlModel::forward_images] merged_embeddings node=649 seq_len=282
[LFM2Model::forward embeddings] seq_len=282 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=0
[LFM2Model::forward embeddings] starting hidden node=649
[LFM2Model::build_transformer_block] normalized_input node=650
[LFM2Model::build_conv1d] in_proj node=651
[LFM2Model::build_conv1d] in_proj shape dims=282 x 3072
[LFM2Model::build_conv1d] L=282 C=1024
[LFM2Model::build_conv1d] triplet node=652
[LFM2Model::build_conv1d] slice nodes B=653 Cg=654 X=655
[LFM2Model::build_conv1d] reshaped B=656 Cg=657 X=658
[LFM2Model::build_conv1d] Bx node=659
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=659
[LFM2Model::build_conv1d] cache window view len1=0 len2=0
[LFM2Model::build_conv1d] conv_input total_len=282
[LFM2Model::build_conv1d] x_nlc node=660
[LFM2Model::build_conv1d] y_nlc node=661
[LFM2Model::build_conv1d] y_slice start=0 node=662 y_lc node=663
[LFM2Model::build_conv1d] gated node=664
[LFM2Model::build_conv1d] projected node=665
[LFM2Model::build_transformer_block] block_output from conv layer=665
[LFM2Model::build_transformer_block] after_block node=666
[LFM2Model::build_transformer_block] normalized_after_block node=667
[LFM2Model::build_mlp] gate node=668
[LFM2Model::build_mlp] up node=669
[LFM2Model::build_mlp] activated node=671
[LFM2Model::build_mlp] down node=672
[LFM2Model::build_transformer_block] mlp_output node=672
[LFM2Model::build_transformer_block] block_result node=673
[LFM2Model::forward embeddings] after layer=0 hidden node=673
[LFM2Model::build_transformer_block] normalized_input node=674
[LFM2Model::build_conv1d] in_proj node=675
[LFM2Model::build_conv1d] in_proj shape dims=282 x 3072
[LFM2Model::build_conv1d] L=282 C=1024
[LFM2Model::build_conv1d] triplet node=676
[LFM2Model::build_conv1d] slice nodes B=677 Cg=678 X=679
[LFM2Model::build_conv1d] reshaped B=680 Cg=681 X=682
[LFM2Model::build_conv1d] Bx node=683
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=683
[LFM2Model::build_conv1d] cache window view len1=0 len2=0
[LFM2Model::build_conv1d] conv_input total_len=282
[LFM2Model::build_conv1d] x_nlc node=684
[LFM2Model::build_conv1d] y_nlc node=685
[LFM2Model::build_conv1d] y_slice start=0 node=686 y_lc node=687
[LFM2Model::build_conv1d] gated node=688
[LFM2Model::build_conv1d] projected node=689
[LFM2Model::build_transformer_block] block_output from conv layer=689
[LFM2Model::build_transformer_block] after_block node=690
[LFM2Model::build_transformer_block] normalized_after_block node=691
[LFM2Model::build_mlp] gate node=692
[LFM2Model::build_mlp] up node=693
[LFM2Model::build_mlp] activated node=695
[LFM2Model::build_mlp] down node=696
[LFM2Model::build_transformer_block] mlp_output node=696
[LFM2Model::build_transformer_block] block_result node=697
[LFM2Model::forward embeddings] after layer=1 hidden node=697
[LFM2Model::build_transformer_block] normalized_input node=698
[LFM2Model::build_attention] q_proj_linear node=699
[LFM2Model::build_attention] k_proj_linear node=700
[LFM2Model::build_attention] v_proj_linear node=701
[LFM2Model::build_attention] batch_seq=282 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=704
[LFM2Model::build_attention] k_proj node=707
[LFM2Model::build_attention] q_proj_4d=708 k_proj_4d=709 v_proj_4d=710
[LFM2Model::build_attention] applied ROPE with position_offset=0
[LFM2Model::build_attention] initial final_k=712 final_v=710
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=713
[LFM2Model::build_attention] attn_output node=714
[LFM2Model::build_attention] projected node=715
[LFM2Model::build_transformer_block] block_output from attention layer=715
[LFM2Model::build_transformer_block] after_block node=716
[LFM2Model::build_transformer_block] normalized_after_block node=717
[LFM2Model::build_mlp] gate node=718
[LFM2Model::build_mlp] up node=719
[LFM2Model::build_mlp] activated node=721
[LFM2Model::build_mlp] down node=722
[LFM2Model::build_transformer_block] mlp_output node=722
[LFM2Model::build_transformer_block] block_result node=723
[LFM2Model::forward embeddings] after layer=2 hidden node=723
[LFM2Model::build_transformer_block] normalized_input node=724
[LFM2Model::build_conv1d] in_proj node=725
[LFM2Model::build_conv1d] in_proj shape dims=282 x 3072
[LFM2Model::build_conv1d] L=282 C=1024
[LFM2Model::build_conv1d] triplet node=726
[LFM2Model::build_conv1d] slice nodes B=727 Cg=728 X=729
[LFM2Model::build_conv1d] reshaped B=730 Cg=731 X=732
[LFM2Model::build_conv1d] Bx node=733
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=733
[LFM2Model::build_conv1d] cache window view len1=0 len2=0
[LFM2Model::build_conv1d] conv_input total_len=282
[LFM2Model::build_conv1d] x_nlc node=734
[LFM2Model::build_conv1d] y_nlc node=735
[LFM2Model::build_conv1d] y_slice start=0 node=736 y_lc node=737
[LFM2Model::build_conv1d] gated node=738
[LFM2Model::build_conv1d] projected node=739
[LFM2Model::build_transformer_block] block_output from conv layer=739
[LFM2Model::build_transformer_block] after_block node=740
[LFM2Model::build_transformer_block] normalized_after_block node=741
[LFM2Model::build_mlp] gate node=742
[LFM2Model::build_mlp] up node=743
[LFM2Model::build_mlp] activated node=745
[LFM2Model::build_mlp] down node=746
[LFM2Model::build_transformer_block] mlp_output node=746
[LFM2Model::build_transformer_block] block_result node=747
[LFM2Model::forward embeddings] after layer=3 hidden node=747
[LFM2Model::build_transformer_block] normalized_input node=748
[LFM2Model::build_conv1d] in_proj node=749
[LFM2Model::build_conv1d] in_proj shape dims=282 x 3072
[LFM2Model::build_conv1d] L=282 C=1024
[LFM2Model::build_conv1d] triplet node=750
[LFM2Model::build_conv1d] slice nodes B=751 Cg=752 X=753
[LFM2Model::build_conv1d] reshaped B=754 Cg=755 X=756
[LFM2Model::build_conv1d] Bx node=757
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=757
[LFM2Model::build_conv1d] cache window view len1=0 len2=0
[LFM2Model::build_conv1d] conv_input total_len=282
[LFM2Model::build_conv1d] x_nlc node=758
[LFM2Model::build_conv1d] y_nlc node=759
[LFM2Model::build_conv1d] y_slice start=0 node=760 y_lc node=761
[LFM2Model::build_conv1d] gated node=762
[LFM2Model::build_conv1d] projected node=763
[LFM2Model::build_transformer_block] block_output from conv layer=763
[LFM2Model::build_transformer_block] after_block node=764
[LFM2Model::build_transformer_block] normalized_after_block node=765
[LFM2Model::build_mlp] gate node=766
[LFM2Model::build_mlp] up node=767
[LFM2Model::build_mlp] activated node=769
[LFM2Model::build_mlp] down node=770
[LFM2Model::build_transformer_block] mlp_output node=770
[LFM2Model::build_transformer_block] block_result node=771
[LFM2Model::forward embeddings] after layer=4 hidden node=771
[LFM2Model::build_transformer_block] normalized_input node=772
[LFM2Model::build_attention] q_proj_linear node=773
[LFM2Model::build_attention] k_proj_linear node=774
[LFM2Model::build_attention] v_proj_linear node=775
[LFM2Model::build_attention] batch_seq=282 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=778
[LFM2Model::build_attention] k_proj node=781
[LFM2Model::build_attention] q_proj_4d=782 k_proj_4d=783 v_proj_4d=784
[LFM2Model::build_attention] applied ROPE with position_offset=0
[LFM2Model::build_attention] initial final_k=786 final_v=784
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=787
[LFM2Model::build_attention] attn_output node=788
[LFM2Model::build_attention] projected node=789
[LFM2Model::build_transformer_block] block_output from attention layer=789
[LFM2Model::build_transformer_block] after_block node=790
[LFM2Model::build_transformer_block] normalized_after_block node=791
[LFM2Model::build_mlp] gate node=792
[LFM2Model::build_mlp] up node=793
[LFM2Model::build_mlp] activated node=795
[LFM2Model::build_mlp] down node=796
[LFM2Model::build_transformer_block] mlp_output node=796
[LFM2Model::build_transformer_block] block_result node=797
[LFM2Model::forward embeddings] after layer=5 hidden node=797
[LFM2Model::build_transformer_block] normalized_input node=798
[LFM2Model::build_conv1d] in_proj node=799
[LFM2Model::build_conv1d] in_proj shape dims=282 x 3072
[LFM2Model::build_conv1d] L=282 C=1024
[LFM2Model::build_conv1d] triplet node=800
[LFM2Model::build_conv1d] slice nodes B=801 Cg=802 X=803
[LFM2Model::build_conv1d] reshaped B=804 Cg=805 X=806
[LFM2Model::build_conv1d] Bx node=807
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=807
[LFM2Model::build_conv1d] cache window view len1=0 len2=0
[LFM2Model::build_conv1d] conv_input total_len=282
[LFM2Model::build_conv1d] x_nlc node=808
[LFM2Model::build_conv1d] y_nlc node=809
[LFM2Model::build_conv1d] y_slice start=0 node=810 y_lc node=811
[LFM2Model::build_conv1d] gated node=812
[LFM2Model::build_conv1d] projected node=813
[LFM2Model::build_transformer_block] block_output from conv layer=813
[LFM2Model::build_transformer_block] after_block node=814
[LFM2Model::build_transformer_block] normalized_after_block node=815
[LFM2Model::build_mlp] gate node=816
[LFM2Model::build_mlp] up node=817
[LFM2Model::build_mlp] activated node=819
[LFM2Model::build_mlp] down node=820
[LFM2Model::build_transformer_block] mlp_output node=820
[LFM2Model::build_transformer_block] block_result node=821
[LFM2Model::forward embeddings] after layer=6 hidden node=821
[LFM2Model::build_transformer_block] normalized_input node=822
[LFM2Model::build_conv1d] in_proj node=823
[LFM2Model::build_conv1d] in_proj shape dims=282 x 3072
[LFM2Model::build_conv1d] L=282 C=1024
[LFM2Model::build_conv1d] triplet node=824
[LFM2Model::build_conv1d] slice nodes B=825 Cg=826 X=827
[LFM2Model::build_conv1d] reshaped B=828 Cg=829 X=830
[LFM2Model::build_conv1d] Bx node=831
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=831
[LFM2Model::build_conv1d] cache window view len1=0 len2=0
[LFM2Model::build_conv1d] conv_input total_len=282
[LFM2Model::build_conv1d] x_nlc node=832
[LFM2Model::build_conv1d] y_nlc node=833
[LFM2Model::build_conv1d] y_slice start=0 node=834 y_lc node=835
[LFM2Model::build_conv1d] gated node=836
[LFM2Model::build_conv1d] projected node=837
[LFM2Model::build_transformer_block] block_output from conv layer=837
[LFM2Model::build_transformer_block] after_block node=838
[LFM2Model::build_transformer_block] normalized_after_block node=839
[LFM2Model::build_mlp] gate node=840
[LFM2Model::build_mlp] up node=841
[LFM2Model::build_mlp] activated node=843
[LFM2Model::build_mlp] down node=844
[LFM2Model::build_transformer_block] mlp_output node=844
[LFM2Model::build_transformer_block] block_result node=845
[LFM2Model::forward embeddings] after layer=7 hidden node=845
[LFM2Model::build_transformer_block] normalized_input node=846
[LFM2Model::build_attention] q_proj_linear node=847
[LFM2Model::build_attention] k_proj_linear node=848
[LFM2Model::build_attention] v_proj_linear node=849
[LFM2Model::build_attention] batch_seq=282 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=852
[LFM2Model::build_attention] k_proj node=855
[LFM2Model::build_attention] q_proj_4d=856 k_proj_4d=857 v_proj_4d=858
[LFM2Model::build_attention] applied ROPE with position_offset=0
[LFM2Model::build_attention] initial final_k=860 final_v=858
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=861
[LFM2Model::build_attention] attn_output node=862
[LFM2Model::build_attention] projected node=863
[LFM2Model::build_transformer_block] block_output from attention layer=863
[LFM2Model::build_transformer_block] after_block node=864
[LFM2Model::build_transformer_block] normalized_after_block node=865
[LFM2Model::build_mlp] gate node=866
[LFM2Model::build_mlp] up node=867
[LFM2Model::build_mlp] activated node=869
[LFM2Model::build_mlp] down node=870
[LFM2Model::build_transformer_block] mlp_output node=870
[LFM2Model::build_transformer_block] block_result node=871
[LFM2Model::forward embeddings] after layer=8 hidden node=871
[LFM2Model::build_transformer_block] normalized_input node=872
[LFM2Model::build_conv1d] in_proj node=873
[LFM2Model::build_conv1d] in_proj shape dims=282 x 3072
[LFM2Model::build_conv1d] L=282 C=1024
[LFM2Model::build_conv1d] triplet node=874
[LFM2Model::build_conv1d] slice nodes B=875 Cg=876 X=877
[LFM2Model::build_conv1d] reshaped B=878 Cg=879 X=880
[LFM2Model::build_conv1d] Bx node=881
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=881
[LFM2Model::build_conv1d] cache window view len1=0 len2=0
[LFM2Model::build_conv1d] conv_input total_len=282
[LFM2Model::build_conv1d] x_nlc node=882
[LFM2Model::build_conv1d] y_nlc node=883
[LFM2Model::build_conv1d] y_slice start=0 node=884 y_lc node=885
[LFM2Model::build_conv1d] gated node=886
[LFM2Model::build_conv1d] projected node=887
[LFM2Model::build_transformer_block] block_output from conv layer=887
[LFM2Model::build_transformer_block] after_block node=888
[LFM2Model::build_transformer_block] normalized_after_block node=889
[LFM2Model::build_mlp] gate node=890
[LFM2Model::build_mlp] up node=891
[LFM2Model::build_mlp] activated node=893
[LFM2Model::build_mlp] down node=894
[LFM2Model::build_transformer_block] mlp_output node=894
[LFM2Model::build_transformer_block] block_result node=895
[LFM2Model::forward embeddings] after layer=9 hidden node=895
[LFM2Model::build_transformer_block] normalized_input node=896
[LFM2Model::build_attention] q_proj_linear node=897
[LFM2Model::build_attention] k_proj_linear node=898
[LFM2Model::build_attention] v_proj_linear node=899
[LFM2Model::build_attention] batch_seq=282 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=902
[LFM2Model::build_attention] k_proj node=905
[LFM2Model::build_attention] q_proj_4d=906 k_proj_4d=907 v_proj_4d=908
[LFM2Model::build_attention] applied ROPE with position_offset=0
[LFM2Model::build_attention] initial final_k=910 final_v=908
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=911
[LFM2Model::build_attention] attn_output node=912
[LFM2Model::build_attention] projected node=913
[LFM2Model::build_transformer_block] block_output from attention layer=913
[LFM2Model::build_transformer_block] after_block node=914
[LFM2Model::build_transformer_block] normalized_after_block node=915
[LFM2Model::build_mlp] gate node=916
[LFM2Model::build_mlp] up node=917
[LFM2Model::build_mlp] activated node=919
[LFM2Model::build_mlp] down node=920
[LFM2Model::build_transformer_block] mlp_output node=920
[LFM2Model::build_transformer_block] block_result node=921
[LFM2Model::forward embeddings] after layer=10 hidden node=921
[LFM2Model::build_transformer_block] normalized_input node=922
[LFM2Model::build_conv1d] in_proj node=923
[LFM2Model::build_conv1d] in_proj shape dims=282 x 3072
[LFM2Model::build_conv1d] L=282 C=1024
[LFM2Model::build_conv1d] triplet node=924
[LFM2Model::build_conv1d] slice nodes B=925 Cg=926 X=927
[LFM2Model::build_conv1d] reshaped B=928 Cg=929 X=930
[LFM2Model::build_conv1d] Bx node=931
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=931
[LFM2Model::build_conv1d] cache window view len1=0 len2=0
[LFM2Model::build_conv1d] conv_input total_len=282
[LFM2Model::build_conv1d] x_nlc node=932
[LFM2Model::build_conv1d] y_nlc node=933
[LFM2Model::build_conv1d] y_slice start=0 node=934 y_lc node=935
[LFM2Model::build_conv1d] gated node=936
[LFM2Model::build_conv1d] projected node=937
[LFM2Model::build_transformer_block] block_output from conv layer=937
[LFM2Model::build_transformer_block] after_block node=938
[LFM2Model::build_transformer_block] normalized_after_block node=939
[LFM2Model::build_mlp] gate node=940
[LFM2Model::build_mlp] up node=941
[LFM2Model::build_mlp] activated node=943
[LFM2Model::build_mlp] down node=944
[LFM2Model::build_transformer_block] mlp_output node=944
[LFM2Model::build_transformer_block] block_result node=945
[LFM2Model::forward embeddings] after layer=11 hidden node=945
[LFM2Model::build_transformer_block] normalized_input node=946
[LFM2Model::build_attention] q_proj_linear node=947
[LFM2Model::build_attention] k_proj_linear node=948
[LFM2Model::build_attention] v_proj_linear node=949
[LFM2Model::build_attention] batch_seq=282 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=952
[LFM2Model::build_attention] k_proj node=955
[LFM2Model::build_attention] q_proj_4d=956 k_proj_4d=957 v_proj_4d=958
[LFM2Model::build_attention] applied ROPE with position_offset=0
[LFM2Model::build_attention] initial final_k=960 final_v=958
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=961
[LFM2Model::build_attention] attn_output node=962
[LFM2Model::build_attention] projected node=963
[LFM2Model::build_transformer_block] block_output from attention layer=963
[LFM2Model::build_transformer_block] after_block node=964
[LFM2Model::build_transformer_block] normalized_after_block node=965
[LFM2Model::build_mlp] gate node=966
[LFM2Model::build_mlp] up node=967
[LFM2Model::build_mlp] activated node=969
[LFM2Model::build_mlp] down node=970
[LFM2Model::build_transformer_block] mlp_output node=970
[LFM2Model::build_transformer_block] block_result node=971
[LFM2Model::forward embeddings] after layer=12 hidden node=971
[LFM2Model::build_transformer_block] normalized_input node=972
[LFM2Model::build_conv1d] in_proj node=973
[LFM2Model::build_conv1d] in_proj shape dims=282 x 3072
[LFM2Model::build_conv1d] L=282 C=1024
[LFM2Model::build_conv1d] triplet node=974
[LFM2Model::build_conv1d] slice nodes B=975 Cg=976 X=977
[LFM2Model::build_conv1d] reshaped B=978 Cg=979 X=980
[LFM2Model::build_conv1d] Bx node=981
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=981
[LFM2Model::build_conv1d] cache window view len1=0 len2=0
[LFM2Model::build_conv1d] conv_input total_len=282
[LFM2Model::build_conv1d] x_nlc node=982
[LFM2Model::build_conv1d] y_nlc node=983
[LFM2Model::build_conv1d] y_slice start=0 node=984 y_lc node=985
[LFM2Model::build_conv1d] gated node=986
[LFM2Model::build_conv1d] projected node=987
[LFM2Model::build_transformer_block] block_output from conv layer=987
[LFM2Model::build_transformer_block] after_block node=988
[LFM2Model::build_transformer_block] normalized_after_block node=989
[LFM2Model::build_mlp] gate node=990
[LFM2Model::build_mlp] up node=991
[LFM2Model::build_mlp] activated node=993
[LFM2Model::build_mlp] down node=994
[LFM2Model::build_transformer_block] mlp_output node=994
[LFM2Model::build_transformer_block] block_result node=995
[LFM2Model::forward embeddings] after layer=13 hidden node=995
[LFM2Model::build_transformer_block] normalized_input node=996
[LFM2Model::build_attention] q_proj_linear node=997
[LFM2Model::build_attention] k_proj_linear node=998
[LFM2Model::build_attention] v_proj_linear node=999
[LFM2Model::build_attention] batch_seq=282 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=1002
[LFM2Model::build_attention] k_proj node=1005
[LFM2Model::build_attention] q_proj_4d=1006 k_proj_4d=1007 v_proj_4d=1008
[LFM2Model::build_attention] applied ROPE with position_offset=0
[LFM2Model::build_attention] initial final_k=1010 final_v=1008
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=1011
[LFM2Model::build_attention] attn_output node=1012
[LFM2Model::build_attention] projected node=1013
[LFM2Model::build_transformer_block] block_output from attention layer=1013
[LFM2Model::build_transformer_block] after_block node=1014
[LFM2Model::build_transformer_block] normalized_after_block node=1015
[LFM2Model::build_mlp] gate node=1016
[LFM2Model::build_mlp] up node=1017
[LFM2Model::build_mlp] activated node=1019
[LFM2Model::build_mlp] down node=1020
[LFM2Model::build_transformer_block] mlp_output node=1020
[LFM2Model::build_transformer_block] block_result node=1021
[LFM2Model::forward embeddings] after layer=14 hidden node=1021
[LFM2Model::build_transformer_block] normalized_input node=1022
[LFM2Model::build_conv1d] in_proj node=1023
[LFM2Model::build_conv1d] in_proj shape dims=282 x 3072
[LFM2Model::build_conv1d] L=282 C=1024
[LFM2Model::build_conv1d] triplet node=1024
[LFM2Model::build_conv1d] slice nodes B=1025 Cg=1026 X=1027
[LFM2Model::build_conv1d] reshaped B=1028 Cg=1029 X=1030
[LFM2Model::build_conv1d] Bx node=1031
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=1031
[LFM2Model::build_conv1d] cache window view len1=0 len2=0
[LFM2Model::build_conv1d] conv_input total_len=282
[LFM2Model::build_conv1d] x_nlc node=1032
[LFM2Model::build_conv1d] y_nlc node=1033
[LFM2Model::build_conv1d] y_slice start=0 node=1034 y_lc node=1035
[LFM2Model::build_conv1d] gated node=1036
[LFM2Model::build_conv1d] projected node=1037
[LFM2Model::build_transformer_block] block_output from conv layer=1037
[LFM2Model::build_transformer_block] after_block node=1038
[LFM2Model::build_transformer_block] normalized_after_block node=1039
[LFM2Model::build_mlp] gate node=1040
[LFM2Model::build_mlp] up node=1041
[LFM2Model::build_mlp] activated node=1043
[LFM2Model::build_mlp] down node=1044
[LFM2Model::build_transformer_block] mlp_output node=1044
[LFM2Model::build_transformer_block] block_result node=1045
[LFM2Model::forward embeddings] after layer=15 hidden node=1045
[LFM2Model::forward embeddings] final_hidden node=1046
[Lfm2VlModel::forward_images] final_hidden node=1046
[Lfm2VlModel::generate_with_images] performed image prefill final_hidden=1046 seq_len=282
[Lfm2VlModel::generate_with_images] logits_node_id=1047
[Lfm2VlModel::generate_with_images] sampled_token_id node=1048
Bilinear Interpolation from (16, 16) to (32, 32), scale_h: 0.483871, scale_w: 0.483871
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=282
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [ 1] token_id=0 text="<|pad|>"
[Lfm2VlModel::generate_with_images] called with tokens size=283 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=282
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=282
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=282
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=282
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=282
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=282
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=282
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=282
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=282
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=282
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=282
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=282
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=282
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [ 2] token_id=0 text="<|pad|>"
[Lfm2VlModel::generate_with_images] called with tokens size=284 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=283
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=283
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=283
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=283
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=283
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=283
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=283
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=283
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=283
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=283
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=283
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=283
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=283
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [ 3] token_id=0 text="<|pad|>"
[Lfm2VlModel::generate_with_images] called with tokens size=285 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=284
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=284
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=284
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=284
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=284
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=284
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=284
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=284
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=284
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=284
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=284
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=284
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=284
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [ 4] token_id=0 text="<|pad|>"
[Lfm2VlModel::generate_with_images] called with tokens size=286 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=285
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=285
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=285
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=285
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=285
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=285
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=285
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=285
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=285
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=285
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=285
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=285
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=285
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [ 5] token_id=0 text="<|pad|>"
[Lfm2VlModel::generate_with_images] called with tokens size=287 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=286
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=286
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=286
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=286
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=286
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=286
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=286
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=286
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=286
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=286
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=286
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=286
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=286
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [ 6] token_id=0 text="<|pad|>"
[Lfm2VlModel::generate_with_images] called with tokens size=288 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=287
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=287
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=287
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=287
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=287
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=287
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=287
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=287
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=287
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=287
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=287
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=287
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=287
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [ 7] token_id=0 text="<|pad|>"
[Lfm2VlModel::generate_with_images] called with tokens size=289 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=288
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=288
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=288
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=288
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=288
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=288
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=288
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=288
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=288
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=288
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=288
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=288
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=288
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [ 8] token_id=0 text="<|pad|>"
[Lfm2VlModel::generate_with_images] called with tokens size=290 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=289
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=289
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=289
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=289
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=289
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=289
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=289
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=289
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=289
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=289
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=289
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=289
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=289
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [ 9] token_id=0 text="<|pad|>"
[Lfm2VlModel::generate_with_images] called with tokens size=291 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=290
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=290
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=290
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=290
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=290
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=290
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=290
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=290
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=290
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=290
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=290
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=290
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=290
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [10] token_id=0 text="<|pad|>"
[Lfm2VlModel::generate_with_images] called with tokens size=292 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=291
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=291
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=291
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=291
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=291
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=291
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=291
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=291
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=291
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=291
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=291
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=291
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=291
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [11] token_id=0 text="<|pad|>"
[Lfm2VlModel::generate_with_images] called with tokens size=293 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=292
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=292
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=292
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=292
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=292
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=292
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=292
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=292
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=292
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=292
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=292
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=292
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=292
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [12] token_id=8076 text=" **"
[Lfm2VlModel::generate_with_images] called with tokens size=294 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=293
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=293
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=293
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=293
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=293
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=293
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=293
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=293
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=293
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=293
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=293
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=293
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=293
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [13] token_id=554 text="M"
[Lfm2VlModel::generate_with_images] called with tokens size=295 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=294
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=294
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=294
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=294
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=294
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=294
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=294
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=294
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=294
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=294
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=294
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=294
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=294
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [14] token_id=552 text="K"
[Lfm2VlModel::generate_with_images] called with tokens size=296 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=295
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=295
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=295
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=295
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=295
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=295
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=295
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=295
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=295
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=295
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=295
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=295
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=295
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [15] token_id=516 text="'"
[Lfm2VlModel::generate_with_images] called with tokens size=297 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=296
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=296
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=296
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=296
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=296
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=296
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=296
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=296
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=296
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=296
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=296
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=296
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=296
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [16] token_id=28559 text="Ve"
[Lfm2VlModel::generate_with_images] called with tokens size=298 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=297
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=297
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=297
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=297
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=297
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=297
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=297
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=297
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=297
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=297
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=297
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=297
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=297
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [17] token_id=557 text="P"
[Lfm2VlModel::generate_with_images] called with tokens size=299 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=298
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=298
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=298
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=298
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=298
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=298
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=298
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=298
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=298
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=298
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=298
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=298
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=298
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [18] token_id=958 text="ain"
[Lfm2VlModel::generate_with_images] called with tokens size=300 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=299
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=299
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=299
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=299
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=299
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=299
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=299
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=299
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=299
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=299
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=299
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=299
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=299
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [19] token_id=1145 text="ier"
[Lfm2VlModel::generate_with_images] called with tokens size=301 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=300
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=300
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=300
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=300
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=300
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=300
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=300
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=300
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=300
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=300
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=300
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=300
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=300
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [20] token_id=522 text="-"
[Lfm2VlModel::generate_with_images] called with tokens size=302 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=301
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=301
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=301
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=301
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=301
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=301
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=301
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=301
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=301
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=301
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=301
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=301
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=301
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [21] token_id=574 text="a"
[Lfm2VlModel::generate_with_images] called with tokens size=303 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=302
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=302
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=302
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=302
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=302
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=302
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=302
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=302
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=302
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=302
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=302
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=302
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=302
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [22] token_id=1168 text="ung"
[Lfm2VlModel::generate_with_images] called with tokens size=304 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=303
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=303
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=303
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=303
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=303
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=303
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=303
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=303
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=303
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=303
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=303
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=303
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=303
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [23] token_id=1480 text="sp"
[Lfm2VlModel::generate_with_images] called with tokens size=305 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=304
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=304
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=304
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=304
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=304
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=304
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=304
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=304
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=304
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=304
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=304
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=304
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=304
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [24] token_id=827 text="ur"
[Lfm2VlModel::generate_with_images] called with tokens size=306 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=305
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=305
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=305
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=305
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=305
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=305
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=305
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=305
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=305
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=305
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=305
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=305
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=305
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [25] token_id=518 text=")"
[Lfm2VlModel::generate_with_images] called with tokens size=307 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=306
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=306
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=306
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=306
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=306
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=306
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=306
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=306
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=306
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=306
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=306
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=306
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=306
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [26] token_id=730 text=" "
[Lfm2VlModel::generate_with_images] called with tokens size=308 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=307
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=307
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=307
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=307
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=307
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=307
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=307
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=307
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=307
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=307
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=307
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=307
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=307
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [27] token_id=785 text="it"
[Lfm2VlModel::generate_with_images] called with tokens size=309 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=308
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=308
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=308
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=308
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=308
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=308
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=308
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=308
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=308
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=308
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=308
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=308
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=308
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [28] token_id=1090 text="'s"
[Lfm2VlModel::generate_with_images] called with tokens size=310 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=309
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=309
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=309
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=309
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=309
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=309
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=309
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=309
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=309
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=309
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=309
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=309
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=309
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [29] token_id=776 text="at"
[Lfm2VlModel::generate_with_images] called with tokens size=311 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=310
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=310
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=310
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=310
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=310
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=310
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=310
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=310
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=310
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=310
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=310
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=310
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=310
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [30] token_id=769 text="in"
[Lfm2VlModel::generate_with_images] called with tokens size=312 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=311
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=311
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=311
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=311
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=311
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=311
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=311
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=311
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=311
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=311
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=311
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=311
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=311
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [31] token_id=9111 text=" faith"
[Lfm2VlModel::generate_with_images] called with tokens size=313 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=312
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=312
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=312
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=312
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=312
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=312
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=312
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=312
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=312
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=312
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=312
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=312
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=312
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [32] token_id=4877 text="fully"
[Lfm2VlModel::generate_with_images] called with tokens size=314 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=313
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=313
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=313
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=313
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=313
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=313
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=313
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=313
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=313
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=313
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=313
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=313
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=313
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [33] token_id=2743 text=" come"
[Lfm2VlModel::generate_with_images] called with tokens size=315 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=314
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=314
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=314
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=314
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=314
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=314
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=314
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=314
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=314
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=314
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=314
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=314
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=314
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [34] token_id=1456 text="ced"
[Lfm2VlModel::generate_with_images] called with tokens size=316 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=315
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=315
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=315
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=315
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=315
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=315
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=315
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=315
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=315
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=315
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=315
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=315
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=315
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [35] token_id=1230 text=" out"
[Lfm2VlModel::generate_with_images] called with tokens size=317 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=316
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=316
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=316
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=316
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=316
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=316
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=316
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=316
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=316
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=316
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=316
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=316
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=316
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [36] token_id=803 text=" of"
[Lfm2VlModel::generate_with_images] called with tokens size=318 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=317
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=317
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=317
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=317
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=317
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=317
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=317
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=317
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=317
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=317
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=317
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=317
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=317
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [37] token_id=1973 text=" every"
[Lfm2VlModel::generate_with_images] called with tokens size=319 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=318
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=318
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=318
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=318
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=318
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=318
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=318
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=318
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=318
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=318
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=318
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=318
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=318
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [38] token_id=779 text=" the"
[Lfm2VlModel::generate_with_images] called with tokens size=320 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=319
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=319
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=319
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=319
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=319
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=319
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=319
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=319
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=319
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=319
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=319
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=319
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=319
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [39] token_id=7578 text=" seven"
[Lfm2VlModel::generate_with_images] called with tokens size=321 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=320
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=320
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=320
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=320
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=320
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=320
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=320
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=320
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=320
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=320
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=320
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=320
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=320
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [40] token_id=24146 text=" crystal"
[Lfm2VlModel::generate_with_images] called with tokens size=322 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=321
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=321
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=321
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=321
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=321
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=321
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=321
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=321
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=321
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=321
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=321
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=321
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=321
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [41] token_id=6913 text=" ball"
[Lfm2VlModel::generate_with_images] called with tokens size=323 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=322
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=322
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=322
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=322
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=322
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=322
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=322
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=322
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=322
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=322
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=322
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=322
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=322
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [42] token_id=588 text="o"
[Lfm2VlModel::generate_with_images] called with tokens size=324 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=323
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=323
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=323
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=323
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=323
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=323
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=323
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=323
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=323
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=323
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=323
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=323
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=323
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [43] token_id=1580 text="ars"
[Lfm2VlModel::generate_with_images] called with tokens size=325 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=324
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=324
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=324
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=324
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=324
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=324
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=324
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=324
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=324
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=324
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=324
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=324
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=324
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [44] token_id=871 text="ers"
[Lfm2VlModel::generate_with_images] called with tokens size=326 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=325
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=325
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=325
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=325
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=325
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=325
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=325
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=325
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=325
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=325
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=325
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=325
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=325
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [45] token_id=2217 text="men"
[Lfm2VlModel::generate_with_images] called with tokens size=327 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=326
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=326
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=326
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=326
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=326
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=326
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=326
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=326
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=326
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=326
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=326
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=326
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=326
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [46] token_id=4581 text=""
[Lfm2VlModel::generate_with_images] called with tokens size=328 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=327
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=327
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=327
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=327
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=327
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=327
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=327
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=327
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=327
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=327
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=327
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=327
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=327
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [47] token_id=11203 text=""
[Lfm2VlModel::generate_with_images] called with tokens size=329 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=328
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=328
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=328
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=328
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=328
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=328
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=328
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=328
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=328
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=328
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=328
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=328
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=328
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [48] token_id=2712 text=" here"
[Lfm2VlModel::generate_with_images] called with tokens size=330 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=329
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=329
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=329
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=329
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=329
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=329
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=329
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=329
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=329
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=329
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=329
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=329
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=329
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [49] token_id=1095 text="to"
[Lfm2VlModel::generate_with_images] called with tokens size=331 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=330
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=330
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=330
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=330
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=330
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=330
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=330
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=330
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=330
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=330
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=330
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=330
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=330
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [50] token_id=6358 text="ying"
[Lfm2VlModel::generate_with_images] called with tokens size=332 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=331
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=331
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=331
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=331
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=331
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=331
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=331
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=331
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=331
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=331
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=331
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=331
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=331
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [51] token_id=933 text=" or"
[Lfm2VlModel::generate_with_images] called with tokens size=333 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=332
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=332
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=332
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=332
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=332
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=332
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=332
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=332
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=332
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=332
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=332
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=332
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=332
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [52] token_id=19332 text=" minds"
[Lfm2VlModel::generate_with_images] called with tokens size=334 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=333
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=333
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=333
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=333
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=333
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=333
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=333
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=333
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=333
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=333
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=333
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=333
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=333
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [53] token_id=791 text="ed"
[Lfm2VlModel::generate_with_images] called with tokens size=335 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=334
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=334
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=334
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=334
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=334
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=334
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=334
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=334
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=334
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=334
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=334
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=334
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=334
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [54] token_id=10504 text="s"
[Lfm2VlModel::generate_with_images] called with tokens size=336 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=335
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=335
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=335
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=335
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=335
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=335
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=335
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=335
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=335
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=335
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=335
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=335
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=335
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [55] token_id=791 text="ed"
[Lfm2VlModel::generate_with_images] called with tokens size=337 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=336
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=336
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=336
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=336
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=336
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=336
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=336
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=336
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=336
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=336
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=336
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=336
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=336
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [56] token_id=1264 text="ys"
[Lfm2VlModel::generate_with_images] called with tokens size=338 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=337
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=337
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=337
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=337
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=337
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=337
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=337
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=337
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=337
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=337
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=337
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=337
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=337
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [57] token_id=947 text="ort"
[Lfm2VlModel::generate_with_images] called with tokens size=339 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=338
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=338
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=338
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=338
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=338
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=338
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=338
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=338
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=338
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=338
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=338
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=338
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=338
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [58] token_id=582 text="i"
[Lfm2VlModel::generate_with_images] called with tokens size=340 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=339
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=339
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=339
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=339
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=339
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=339
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=339
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=339
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=339
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=339
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=339
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=339
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=339
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [59] token_id=982 text="pl"
[Lfm2VlModel::generate_with_images] called with tokens size=341 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=340
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=340
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=340
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=340
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=340
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=340
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=340
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=340
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=340
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=340
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=340
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=340
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=340
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [60] token_id=771 text="en"
[Lfm2VlModel::generate_with_images] called with tokens size=342 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=341
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=341
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=341
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=341
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=341
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=341
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=341
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=341
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=341
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=341
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=341
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=341
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=341
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [61] token_id=3660 text="ius"
[Lfm2VlModel::generate_with_images] called with tokens size=343 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=342
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=342
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=342
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=342
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=342
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=342
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=342
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=342
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=342
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=342
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=342
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=342
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=342
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [62] token_id=4119 text=".org"
[Lfm2VlModel::generate_with_images] called with tokens size=344 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=343
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] appended right cache segment node=364
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=365
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=366
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=367
[LFM2Model::build_conv1d] y_nlc node=368
[LFM2Model::build_conv1d] y_slice start=2 node=369 y_lc node=370
[LFM2Model::build_conv1d] gated node=371
[LFM2Model::build_conv1d] projected node=372
[LFM2Model::build_transformer_block] block_output from conv layer=372
[LFM2Model::build_transformer_block] after_block node=373
[LFM2Model::build_transformer_block] normalized_after_block node=374
[LFM2Model::build_mlp] gate node=375
[LFM2Model::build_mlp] up node=376
[LFM2Model::build_mlp] activated node=378
[LFM2Model::build_mlp] down node=379
[LFM2Model::build_transformer_block] mlp_output node=379
[LFM2Model::build_transformer_block] block_result node=380
[LFM2Model::forward embeddings] after layer=0 hidden node=380
[LFM2Model::build_transformer_block] normalized_input node=381
[LFM2Model::build_conv1d] in_proj node=382
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=383
[LFM2Model::build_conv1d] slice nodes B=384 Cg=385 X=386
[LFM2Model::build_conv1d] reshaped B=387 Cg=388 X=389
[LFM2Model::build_conv1d] Bx node=390
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=390
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=391
[LFM2Model::build_conv1d] appended right cache segment node=392
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=393
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=394
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=395
[LFM2Model::build_conv1d] y_nlc node=396
[LFM2Model::build_conv1d] y_slice start=2 node=397 y_lc node=398
[LFM2Model::build_conv1d] gated node=399
[LFM2Model::build_conv1d] projected node=400
[LFM2Model::build_transformer_block] block_output from conv layer=400
[LFM2Model::build_transformer_block] after_block node=401
[LFM2Model::build_transformer_block] normalized_after_block node=402
[LFM2Model::build_mlp] gate node=403
[LFM2Model::build_mlp] up node=404
[LFM2Model::build_mlp] activated node=406
[LFM2Model::build_mlp] down node=407
[LFM2Model::build_transformer_block] mlp_output node=407
[LFM2Model::build_transformer_block] block_result node=408
[LFM2Model::forward embeddings] after layer=1 hidden node=408
[LFM2Model::build_transformer_block] normalized_input node=409
[LFM2Model::build_attention] q_proj_linear node=410
[LFM2Model::build_attention] k_proj_linear node=411
[LFM2Model::build_attention] v_proj_linear node=412
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=415
[LFM2Model::build_attention] k_proj node=418
[LFM2Model::build_attention] q_proj_4d=419 k_proj_4d=420 v_proj_4d=421
[LFM2Model::build_attention] applied ROPE with position_offset=343
[LFM2Model::build_attention] initial final_k=423 final_v=421
[LFM2Model::build_attention] using cache current_seq_len=343
[LFM2Model::build_attention] concatenated cache nodes final_k=426 final_v=427
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=428
[LFM2Model::build_attention] attn_output node=429
[LFM2Model::build_attention] projected node=430
[LFM2Model::build_transformer_block] block_output from attention layer=430
[LFM2Model::build_transformer_block] after_block node=431
[LFM2Model::build_transformer_block] normalized_after_block node=432
[LFM2Model::build_mlp] gate node=433
[LFM2Model::build_mlp] up node=434
[LFM2Model::build_mlp] activated node=436
[LFM2Model::build_mlp] down node=437
[LFM2Model::build_transformer_block] mlp_output node=437
[LFM2Model::build_transformer_block] block_result node=438
[LFM2Model::forward embeddings] after layer=2 hidden node=438
[LFM2Model::build_transformer_block] normalized_input node=439
[LFM2Model::build_conv1d] in_proj node=440
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=441
[LFM2Model::build_conv1d] slice nodes B=442 Cg=443 X=444
[LFM2Model::build_conv1d] reshaped B=445 Cg=446 X=447
[LFM2Model::build_conv1d] Bx node=448
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=448
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=449
[LFM2Model::build_conv1d] appended right cache segment node=450
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=451
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=452
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=453
[LFM2Model::build_conv1d] y_nlc node=454
[LFM2Model::build_conv1d] y_slice start=2 node=455 y_lc node=456
[LFM2Model::build_conv1d] gated node=457
[LFM2Model::build_conv1d] projected node=458
[LFM2Model::build_transformer_block] block_output from conv layer=458
[LFM2Model::build_transformer_block] after_block node=459
[LFM2Model::build_transformer_block] normalized_after_block node=460
[LFM2Model::build_mlp] gate node=461
[LFM2Model::build_mlp] up node=462
[LFM2Model::build_mlp] activated node=464
[LFM2Model::build_mlp] down node=465
[LFM2Model::build_transformer_block] mlp_output node=465
[LFM2Model::build_transformer_block] block_result node=466
[LFM2Model::forward embeddings] after layer=3 hidden node=466
[LFM2Model::build_transformer_block] normalized_input node=467
[LFM2Model::build_conv1d] in_proj node=468
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=469
[LFM2Model::build_conv1d] slice nodes B=470 Cg=471 X=472
[LFM2Model::build_conv1d] reshaped B=473 Cg=474 X=475
[LFM2Model::build_conv1d] Bx node=476
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=476
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=477
[LFM2Model::build_conv1d] appended right cache segment node=478
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=479
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=480
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=481
[LFM2Model::build_conv1d] y_nlc node=482
[LFM2Model::build_conv1d] y_slice start=2 node=483 y_lc node=484
[LFM2Model::build_conv1d] gated node=485
[LFM2Model::build_conv1d] projected node=486
[LFM2Model::build_transformer_block] block_output from conv layer=486
[LFM2Model::build_transformer_block] after_block node=487
[LFM2Model::build_transformer_block] normalized_after_block node=488
[LFM2Model::build_mlp] gate node=489
[LFM2Model::build_mlp] up node=490
[LFM2Model::build_mlp] activated node=492
[LFM2Model::build_mlp] down node=493
[LFM2Model::build_transformer_block] mlp_output node=493
[LFM2Model::build_transformer_block] block_result node=494
[LFM2Model::forward embeddings] after layer=4 hidden node=494
[LFM2Model::build_transformer_block] normalized_input node=495
[LFM2Model::build_attention] q_proj_linear node=496
[LFM2Model::build_attention] k_proj_linear node=497
[LFM2Model::build_attention] v_proj_linear node=498
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=501
[LFM2Model::build_attention] k_proj node=504
[LFM2Model::build_attention] q_proj_4d=505 k_proj_4d=506 v_proj_4d=507
[LFM2Model::build_attention] applied ROPE with position_offset=343
[LFM2Model::build_attention] initial final_k=509 final_v=507
[LFM2Model::build_attention] using cache current_seq_len=343
[LFM2Model::build_attention] concatenated cache nodes final_k=512 final_v=513
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=514
[LFM2Model::build_attention] attn_output node=515
[LFM2Model::build_attention] projected node=516
[LFM2Model::build_transformer_block] block_output from attention layer=516
[LFM2Model::build_transformer_block] after_block node=517
[LFM2Model::build_transformer_block] normalized_after_block node=518
[LFM2Model::build_mlp] gate node=519
[LFM2Model::build_mlp] up node=520
[LFM2Model::build_mlp] activated node=522
[LFM2Model::build_mlp] down node=523
[LFM2Model::build_transformer_block] mlp_output node=523
[LFM2Model::build_transformer_block] block_result node=524
[LFM2Model::forward embeddings] after layer=5 hidden node=524
[LFM2Model::build_transformer_block] normalized_input node=525
[LFM2Model::build_conv1d] in_proj node=526
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=527
[LFM2Model::build_conv1d] slice nodes B=528 Cg=529 X=530
[LFM2Model::build_conv1d] reshaped B=531 Cg=532 X=533
[LFM2Model::build_conv1d] Bx node=534
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=534
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=535
[LFM2Model::build_conv1d] appended right cache segment node=536
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=537
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=538
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=539
[LFM2Model::build_conv1d] y_nlc node=540
[LFM2Model::build_conv1d] y_slice start=2 node=541 y_lc node=542
[LFM2Model::build_conv1d] gated node=543
[LFM2Model::build_conv1d] projected node=544
[LFM2Model::build_transformer_block] block_output from conv layer=544
[LFM2Model::build_transformer_block] after_block node=545
[LFM2Model::build_transformer_block] normalized_after_block node=546
[LFM2Model::build_mlp] gate node=547
[LFM2Model::build_mlp] up node=548
[LFM2Model::build_mlp] activated node=550
[LFM2Model::build_mlp] down node=551
[LFM2Model::build_transformer_block] mlp_output node=551
[LFM2Model::build_transformer_block] block_result node=552
[LFM2Model::forward embeddings] after layer=6 hidden node=552
[LFM2Model::build_transformer_block] normalized_input node=553
[LFM2Model::build_conv1d] in_proj node=554
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=555
[LFM2Model::build_conv1d] slice nodes B=556 Cg=557 X=558
[LFM2Model::build_conv1d] reshaped B=559 Cg=560 X=561
[LFM2Model::build_conv1d] Bx node=562
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=562
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=563
[LFM2Model::build_conv1d] appended right cache segment node=564
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=565
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=566
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=567
[LFM2Model::build_conv1d] y_nlc node=568
[LFM2Model::build_conv1d] y_slice start=2 node=569 y_lc node=570
[LFM2Model::build_conv1d] gated node=571
[LFM2Model::build_conv1d] projected node=572
[LFM2Model::build_transformer_block] block_output from conv layer=572
[LFM2Model::build_transformer_block] after_block node=573
[LFM2Model::build_transformer_block] normalized_after_block node=574
[LFM2Model::build_mlp] gate node=575
[LFM2Model::build_mlp] up node=576
[LFM2Model::build_mlp] activated node=578
[LFM2Model::build_mlp] down node=579
[LFM2Model::build_transformer_block] mlp_output node=579
[LFM2Model::build_transformer_block] block_result node=580
[LFM2Model::forward embeddings] after layer=7 hidden node=580
[LFM2Model::build_transformer_block] normalized_input node=581
[LFM2Model::build_attention] q_proj_linear node=582
[LFM2Model::build_attention] k_proj_linear node=583
[LFM2Model::build_attention] v_proj_linear node=584
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=587
[LFM2Model::build_attention] k_proj node=590
[LFM2Model::build_attention] q_proj_4d=591 k_proj_4d=592 v_proj_4d=593
[LFM2Model::build_attention] applied ROPE with position_offset=343
[LFM2Model::build_attention] initial final_k=595 final_v=593
[LFM2Model::build_attention] using cache current_seq_len=343
[LFM2Model::build_attention] concatenated cache nodes final_k=598 final_v=599
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=600
[LFM2Model::build_attention] attn_output node=601
[LFM2Model::build_attention] projected node=602
[LFM2Model::build_transformer_block] block_output from attention layer=602
[LFM2Model::build_transformer_block] after_block node=603
[LFM2Model::build_transformer_block] normalized_after_block node=604
[LFM2Model::build_mlp] gate node=605
[LFM2Model::build_mlp] up node=606
[LFM2Model::build_mlp] activated node=608
[LFM2Model::build_mlp] down node=609
[LFM2Model::build_transformer_block] mlp_output node=609
[LFM2Model::build_transformer_block] block_result node=610
[LFM2Model::forward embeddings] after layer=8 hidden node=610
[LFM2Model::build_transformer_block] normalized_input node=611
[LFM2Model::build_conv1d] in_proj node=612
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=613
[LFM2Model::build_conv1d] slice nodes B=614 Cg=615 X=616
[LFM2Model::build_conv1d] reshaped B=617 Cg=618 X=619
[LFM2Model::build_conv1d] Bx node=620
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=620
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=621
[LFM2Model::build_conv1d] appended right cache segment node=622
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=623
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=624
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=625
[LFM2Model::build_conv1d] y_nlc node=626
[LFM2Model::build_conv1d] y_slice start=2 node=627 y_lc node=628
[LFM2Model::build_conv1d] gated node=629
[LFM2Model::build_conv1d] projected node=630
[LFM2Model::build_transformer_block] block_output from conv layer=630
[LFM2Model::build_transformer_block] after_block node=631
[LFM2Model::build_transformer_block] normalized_after_block node=632
[LFM2Model::build_mlp] gate node=633
[LFM2Model::build_mlp] up node=634
[LFM2Model::build_mlp] activated node=636
[LFM2Model::build_mlp] down node=637
[LFM2Model::build_transformer_block] mlp_output node=637
[LFM2Model::build_transformer_block] block_result node=638
[LFM2Model::forward embeddings] after layer=9 hidden node=638
[LFM2Model::build_transformer_block] normalized_input node=639
[LFM2Model::build_attention] q_proj_linear node=640
[LFM2Model::build_attention] k_proj_linear node=641
[LFM2Model::build_attention] v_proj_linear node=642
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=645
[LFM2Model::build_attention] k_proj node=648
[LFM2Model::build_attention] q_proj_4d=649 k_proj_4d=650 v_proj_4d=651
[LFM2Model::build_attention] applied ROPE with position_offset=343
[LFM2Model::build_attention] initial final_k=653 final_v=651
[LFM2Model::build_attention] using cache current_seq_len=343
[LFM2Model::build_attention] concatenated cache nodes final_k=656 final_v=657
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=658
[LFM2Model::build_attention] attn_output node=659
[LFM2Model::build_attention] projected node=660
[LFM2Model::build_transformer_block] block_output from attention layer=660
[LFM2Model::build_transformer_block] after_block node=661
[LFM2Model::build_transformer_block] normalized_after_block node=662
[LFM2Model::build_mlp] gate node=663
[LFM2Model::build_mlp] up node=664
[LFM2Model::build_mlp] activated node=666
[LFM2Model::build_mlp] down node=667
[LFM2Model::build_transformer_block] mlp_output node=667
[LFM2Model::build_transformer_block] block_result node=668
[LFM2Model::forward embeddings] after layer=10 hidden node=668
[LFM2Model::build_transformer_block] normalized_input node=669
[LFM2Model::build_conv1d] in_proj node=670
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=671
[LFM2Model::build_conv1d] slice nodes B=672 Cg=673 X=674
[LFM2Model::build_conv1d] reshaped B=675 Cg=676 X=677
[LFM2Model::build_conv1d] Bx node=678
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=678
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=679
[LFM2Model::build_conv1d] appended right cache segment node=680
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=681
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=682
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=683
[LFM2Model::build_conv1d] y_nlc node=684
[LFM2Model::build_conv1d] y_slice start=2 node=685 y_lc node=686
[LFM2Model::build_conv1d] gated node=687
[LFM2Model::build_conv1d] projected node=688
[LFM2Model::build_transformer_block] block_output from conv layer=688
[LFM2Model::build_transformer_block] after_block node=689
[LFM2Model::build_transformer_block] normalized_after_block node=690
[LFM2Model::build_mlp] gate node=691
[LFM2Model::build_mlp] up node=692
[LFM2Model::build_mlp] activated node=694
[LFM2Model::build_mlp] down node=695
[LFM2Model::build_transformer_block] mlp_output node=695
[LFM2Model::build_transformer_block] block_result node=696
[LFM2Model::forward embeddings] after layer=11 hidden node=696
[LFM2Model::build_transformer_block] normalized_input node=697
[LFM2Model::build_attention] q_proj_linear node=698
[LFM2Model::build_attention] k_proj_linear node=699
[LFM2Model::build_attention] v_proj_linear node=700
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=703
[LFM2Model::build_attention] k_proj node=706
[LFM2Model::build_attention] q_proj_4d=707 k_proj_4d=708 v_proj_4d=709
[LFM2Model::build_attention] applied ROPE with position_offset=343
[LFM2Model::build_attention] initial final_k=711 final_v=709
[LFM2Model::build_attention] using cache current_seq_len=343
[LFM2Model::build_attention] concatenated cache nodes final_k=714 final_v=715
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=716
[LFM2Model::build_attention] attn_output node=717
[LFM2Model::build_attention] projected node=718
[LFM2Model::build_transformer_block] block_output from attention layer=718
[LFM2Model::build_transformer_block] after_block node=719
[LFM2Model::build_transformer_block] normalized_after_block node=720
[LFM2Model::build_mlp] gate node=721
[LFM2Model::build_mlp] up node=722
[LFM2Model::build_mlp] activated node=724
[LFM2Model::build_mlp] down node=725
[LFM2Model::build_transformer_block] mlp_output node=725
[LFM2Model::build_transformer_block] block_result node=726
[LFM2Model::forward embeddings] after layer=12 hidden node=726
[LFM2Model::build_transformer_block] normalized_input node=727
[LFM2Model::build_conv1d] in_proj node=728
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=729
[LFM2Model::build_conv1d] slice nodes B=730 Cg=731 X=732
[LFM2Model::build_conv1d] reshaped B=733 Cg=734 X=735
[LFM2Model::build_conv1d] Bx node=736
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=736
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=737
[LFM2Model::build_conv1d] appended right cache segment node=738
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=739
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=740
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=741
[LFM2Model::build_conv1d] y_nlc node=742
[LFM2Model::build_conv1d] y_slice start=2 node=743 y_lc node=744
[LFM2Model::build_conv1d] gated node=745
[LFM2Model::build_conv1d] projected node=746
[LFM2Model::build_transformer_block] block_output from conv layer=746
[LFM2Model::build_transformer_block] after_block node=747
[LFM2Model::build_transformer_block] normalized_after_block node=748
[LFM2Model::build_mlp] gate node=749
[LFM2Model::build_mlp] up node=750
[LFM2Model::build_mlp] activated node=752
[LFM2Model::build_mlp] down node=753
[LFM2Model::build_transformer_block] mlp_output node=753
[LFM2Model::build_transformer_block] block_result node=754
[LFM2Model::forward embeddings] after layer=13 hidden node=754
[LFM2Model::build_transformer_block] normalized_input node=755
[LFM2Model::build_attention] q_proj_linear node=756
[LFM2Model::build_attention] k_proj_linear node=757
[LFM2Model::build_attention] v_proj_linear node=758
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=761
[LFM2Model::build_attention] k_proj node=764
[LFM2Model::build_attention] q_proj_4d=765 k_proj_4d=766 v_proj_4d=767
[LFM2Model::build_attention] applied ROPE with position_offset=343
[LFM2Model::build_attention] initial final_k=769 final_v=767
[LFM2Model::build_attention] using cache current_seq_len=343
[LFM2Model::build_attention] concatenated cache nodes final_k=772 final_v=773
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=774
[LFM2Model::build_attention] attn_output node=775
[LFM2Model::build_attention] projected node=776
[LFM2Model::build_transformer_block] block_output from attention layer=776
[LFM2Model::build_transformer_block] after_block node=777
[LFM2Model::build_transformer_block] normalized_after_block node=778
[LFM2Model::build_mlp] gate node=779
[LFM2Model::build_mlp] up node=780
[LFM2Model::build_mlp] activated node=782
[LFM2Model::build_mlp] down node=783
[LFM2Model::build_transformer_block] mlp_output node=783
[LFM2Model::build_transformer_block] block_result node=784
[LFM2Model::forward embeddings] after layer=14 hidden node=784
[LFM2Model::build_transformer_block] normalized_input node=785
[LFM2Model::build_conv1d] in_proj node=786
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=787
[LFM2Model::build_conv1d] slice nodes B=788 Cg=789 X=790
[LFM2Model::build_conv1d] reshaped B=791 Cg=792 X=793
[LFM2Model::build_conv1d] Bx node=794
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=794
[LFM2Model::build_conv1d] cache window view len1=1 len2=1
[LFM2Model::build_conv1d] appended left cache segment node=795
[LFM2Model::build_conv1d] appended right cache segment node=796
[LFM2Model::build_conv1d] concatenated cache segment idx=1 node=797
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=798
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=799
[LFM2Model::build_conv1d] y_nlc node=800
[LFM2Model::build_conv1d] y_slice start=2 node=801 y_lc node=802
[LFM2Model::build_conv1d] gated node=803
[LFM2Model::build_conv1d] projected node=804
[LFM2Model::build_transformer_block] block_output from conv layer=804
[LFM2Model::build_transformer_block] after_block node=805
[LFM2Model::build_transformer_block] normalized_after_block node=806
[LFM2Model::build_mlp] gate node=807
[LFM2Model::build_mlp] up node=808
[LFM2Model::build_mlp] activated node=810
[LFM2Model::build_mlp] down node=811
[LFM2Model::build_transformer_block] mlp_output node=811
[LFM2Model::build_transformer_block] block_result node=812
[LFM2Model::forward embeddings] after layer=15 hidden node=812
[LFM2Model::forward embeddings] final_hidden node=813
[LFM2Model::forward tokens] final_hidden node=813
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=814
[Lfm2VlModel::generate_with_images] sampled_token_id node=815
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [63] token_id=26151 text=" logo"
[Lfm2VlModel::generate_with_images] called with tokens size=345 image_paths size=1
[Lfm2VlModel::generate_with_images] backend=0
[Lfm2VlModel::generate_with_images] incremental decode tokens=1
[LFM2Model] embedding tensor shape: [65536, 1024] seq_len=1
[LFM2Model::forward tokens] hidden node after embedding=352
[LFM2Model::forward embeddings] seq_len=1 use_cache=1
[LFM2Model::forward embeddings] conv_cache_bx_nodes_ cleared
[LFM2Model::forward embeddings] last_forward_used_cache_=1
[LFM2Model::forward embeddings] position_offset=344
[LFM2Model::forward embeddings] starting hidden node=352
[LFM2Model::build_transformer_block] normalized_input node=353
[LFM2Model::build_conv1d] in_proj node=354
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=355
[LFM2Model::build_conv1d] slice nodes B=356 Cg=357 X=358
[LFM2Model::build_conv1d] reshaped B=359 Cg=360 X=361
[LFM2Model::build_conv1d] Bx node=362
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=0
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=362
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=363
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=364
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=365
[LFM2Model::build_conv1d] y_nlc node=366
[LFM2Model::build_conv1d] y_slice start=2 node=367 y_lc node=368
[LFM2Model::build_conv1d] gated node=369
[LFM2Model::build_conv1d] projected node=370
[LFM2Model::build_transformer_block] block_output from conv layer=370
[LFM2Model::build_transformer_block] after_block node=371
[LFM2Model::build_transformer_block] normalized_after_block node=372
[LFM2Model::build_mlp] gate node=373
[LFM2Model::build_mlp] up node=374
[LFM2Model::build_mlp] activated node=376
[LFM2Model::build_mlp] down node=377
[LFM2Model::build_transformer_block] mlp_output node=377
[LFM2Model::build_transformer_block] block_result node=378
[LFM2Model::forward embeddings] after layer=0 hidden node=378
[LFM2Model::build_transformer_block] normalized_input node=379
[LFM2Model::build_conv1d] in_proj node=380
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=381
[LFM2Model::build_conv1d] slice nodes B=382 Cg=383 X=384
[LFM2Model::build_conv1d] reshaped B=385 Cg=386 X=387
[LFM2Model::build_conv1d] Bx node=388
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=1
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=388
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=389
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=390
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=391
[LFM2Model::build_conv1d] y_nlc node=392
[LFM2Model::build_conv1d] y_slice start=2 node=393 y_lc node=394
[LFM2Model::build_conv1d] gated node=395
[LFM2Model::build_conv1d] projected node=396
[LFM2Model::build_transformer_block] block_output from conv layer=396
[LFM2Model::build_transformer_block] after_block node=397
[LFM2Model::build_transformer_block] normalized_after_block node=398
[LFM2Model::build_mlp] gate node=399
[LFM2Model::build_mlp] up node=400
[LFM2Model::build_mlp] activated node=402
[LFM2Model::build_mlp] down node=403
[LFM2Model::build_transformer_block] mlp_output node=403
[LFM2Model::build_transformer_block] block_result node=404
[LFM2Model::forward embeddings] after layer=1 hidden node=404
[LFM2Model::build_transformer_block] normalized_input node=405
[LFM2Model::build_attention] q_proj_linear node=406
[LFM2Model::build_attention] k_proj_linear node=407
[LFM2Model::build_attention] v_proj_linear node=408
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=411
[LFM2Model::build_attention] k_proj node=414
[LFM2Model::build_attention] q_proj_4d=415 k_proj_4d=416 v_proj_4d=417
[LFM2Model::build_attention] applied ROPE with position_offset=344
[LFM2Model::build_attention] initial final_k=419 final_v=417
[LFM2Model::build_attention] using cache current_seq_len=344
[LFM2Model::build_attention] concatenated cache nodes final_k=422 final_v=423
[LFM2Model::build_attention] stored cache outputs for layer=2
[LFM2Model::build_attention] attn_output_4d node=424
[LFM2Model::build_attention] attn_output node=425
[LFM2Model::build_attention] projected node=426
[LFM2Model::build_transformer_block] block_output from attention layer=426
[LFM2Model::build_transformer_block] after_block node=427
[LFM2Model::build_transformer_block] normalized_after_block node=428
[LFM2Model::build_mlp] gate node=429
[LFM2Model::build_mlp] up node=430
[LFM2Model::build_mlp] activated node=432
[LFM2Model::build_mlp] down node=433
[LFM2Model::build_transformer_block] mlp_output node=433
[LFM2Model::build_transformer_block] block_result node=434
[LFM2Model::forward embeddings] after layer=2 hidden node=434
[LFM2Model::build_transformer_block] normalized_input node=435
[LFM2Model::build_conv1d] in_proj node=436
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=437
[LFM2Model::build_conv1d] slice nodes B=438 Cg=439 X=440
[LFM2Model::build_conv1d] reshaped B=441 Cg=442 X=443
[LFM2Model::build_conv1d] Bx node=444
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=3
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=444
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=445
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=446
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=447
[LFM2Model::build_conv1d] y_nlc node=448
[LFM2Model::build_conv1d] y_slice start=2 node=449 y_lc node=450
[LFM2Model::build_conv1d] gated node=451
[LFM2Model::build_conv1d] projected node=452
[LFM2Model::build_transformer_block] block_output from conv layer=452
[LFM2Model::build_transformer_block] after_block node=453
[LFM2Model::build_transformer_block] normalized_after_block node=454
[LFM2Model::build_mlp] gate node=455
[LFM2Model::build_mlp] up node=456
[LFM2Model::build_mlp] activated node=458
[LFM2Model::build_mlp] down node=459
[LFM2Model::build_transformer_block] mlp_output node=459
[LFM2Model::build_transformer_block] block_result node=460
[LFM2Model::forward embeddings] after layer=3 hidden node=460
[LFM2Model::build_transformer_block] normalized_input node=461
[LFM2Model::build_conv1d] in_proj node=462
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=463
[LFM2Model::build_conv1d] slice nodes B=464 Cg=465 X=466
[LFM2Model::build_conv1d] reshaped B=467 Cg=468 X=469
[LFM2Model::build_conv1d] Bx node=470
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=4
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=470
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=471
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=472
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=473
[LFM2Model::build_conv1d] y_nlc node=474
[LFM2Model::build_conv1d] y_slice start=2 node=475 y_lc node=476
[LFM2Model::build_conv1d] gated node=477
[LFM2Model::build_conv1d] projected node=478
[LFM2Model::build_transformer_block] block_output from conv layer=478
[LFM2Model::build_transformer_block] after_block node=479
[LFM2Model::build_transformer_block] normalized_after_block node=480
[LFM2Model::build_mlp] gate node=481
[LFM2Model::build_mlp] up node=482
[LFM2Model::build_mlp] activated node=484
[LFM2Model::build_mlp] down node=485
[LFM2Model::build_transformer_block] mlp_output node=485
[LFM2Model::build_transformer_block] block_result node=486
[LFM2Model::forward embeddings] after layer=4 hidden node=486
[LFM2Model::build_transformer_block] normalized_input node=487
[LFM2Model::build_attention] q_proj_linear node=488
[LFM2Model::build_attention] k_proj_linear node=489
[LFM2Model::build_attention] v_proj_linear node=490
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=493
[LFM2Model::build_attention] k_proj node=496
[LFM2Model::build_attention] q_proj_4d=497 k_proj_4d=498 v_proj_4d=499
[LFM2Model::build_attention] applied ROPE with position_offset=344
[LFM2Model::build_attention] initial final_k=501 final_v=499
[LFM2Model::build_attention] using cache current_seq_len=344
[LFM2Model::build_attention] concatenated cache nodes final_k=504 final_v=505
[LFM2Model::build_attention] stored cache outputs for layer=5
[LFM2Model::build_attention] attn_output_4d node=506
[LFM2Model::build_attention] attn_output node=507
[LFM2Model::build_attention] projected node=508
[LFM2Model::build_transformer_block] block_output from attention layer=508
[LFM2Model::build_transformer_block] after_block node=509
[LFM2Model::build_transformer_block] normalized_after_block node=510
[LFM2Model::build_mlp] gate node=511
[LFM2Model::build_mlp] up node=512
[LFM2Model::build_mlp] activated node=514
[LFM2Model::build_mlp] down node=515
[LFM2Model::build_transformer_block] mlp_output node=515
[LFM2Model::build_transformer_block] block_result node=516
[LFM2Model::forward embeddings] after layer=5 hidden node=516
[LFM2Model::build_transformer_block] normalized_input node=517
[LFM2Model::build_conv1d] in_proj node=518
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=519
[LFM2Model::build_conv1d] slice nodes B=520 Cg=521 X=522
[LFM2Model::build_conv1d] reshaped B=523 Cg=524 X=525
[LFM2Model::build_conv1d] Bx node=526
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=6
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=526
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=527
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=528
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=529
[LFM2Model::build_conv1d] y_nlc node=530
[LFM2Model::build_conv1d] y_slice start=2 node=531 y_lc node=532
[LFM2Model::build_conv1d] gated node=533
[LFM2Model::build_conv1d] projected node=534
[LFM2Model::build_transformer_block] block_output from conv layer=534
[LFM2Model::build_transformer_block] after_block node=535
[LFM2Model::build_transformer_block] normalized_after_block node=536
[LFM2Model::build_mlp] gate node=537
[LFM2Model::build_mlp] up node=538
[LFM2Model::build_mlp] activated node=540
[LFM2Model::build_mlp] down node=541
[LFM2Model::build_transformer_block] mlp_output node=541
[LFM2Model::build_transformer_block] block_result node=542
[LFM2Model::forward embeddings] after layer=6 hidden node=542
[LFM2Model::build_transformer_block] normalized_input node=543
[LFM2Model::build_conv1d] in_proj node=544
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=545
[LFM2Model::build_conv1d] slice nodes B=546 Cg=547 X=548
[LFM2Model::build_conv1d] reshaped B=549 Cg=550 X=551
[LFM2Model::build_conv1d] Bx node=552
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=7
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=552
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=553
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=554
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=555
[LFM2Model::build_conv1d] y_nlc node=556
[LFM2Model::build_conv1d] y_slice start=2 node=557 y_lc node=558
[LFM2Model::build_conv1d] gated node=559
[LFM2Model::build_conv1d] projected node=560
[LFM2Model::build_transformer_block] block_output from conv layer=560
[LFM2Model::build_transformer_block] after_block node=561
[LFM2Model::build_transformer_block] normalized_after_block node=562
[LFM2Model::build_mlp] gate node=563
[LFM2Model::build_mlp] up node=564
[LFM2Model::build_mlp] activated node=566
[LFM2Model::build_mlp] down node=567
[LFM2Model::build_transformer_block] mlp_output node=567
[LFM2Model::build_transformer_block] block_result node=568
[LFM2Model::forward embeddings] after layer=7 hidden node=568
[LFM2Model::build_transformer_block] normalized_input node=569
[LFM2Model::build_attention] q_proj_linear node=570
[LFM2Model::build_attention] k_proj_linear node=571
[LFM2Model::build_attention] v_proj_linear node=572
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=575
[LFM2Model::build_attention] k_proj node=578
[LFM2Model::build_attention] q_proj_4d=579 k_proj_4d=580 v_proj_4d=581
[LFM2Model::build_attention] applied ROPE with position_offset=344
[LFM2Model::build_attention] initial final_k=583 final_v=581
[LFM2Model::build_attention] using cache current_seq_len=344
[LFM2Model::build_attention] concatenated cache nodes final_k=586 final_v=587
[LFM2Model::build_attention] stored cache outputs for layer=8
[LFM2Model::build_attention] attn_output_4d node=588
[LFM2Model::build_attention] attn_output node=589
[LFM2Model::build_attention] projected node=590
[LFM2Model::build_transformer_block] block_output from attention layer=590
[LFM2Model::build_transformer_block] after_block node=591
[LFM2Model::build_transformer_block] normalized_after_block node=592
[LFM2Model::build_mlp] gate node=593
[LFM2Model::build_mlp] up node=594
[LFM2Model::build_mlp] activated node=596
[LFM2Model::build_mlp] down node=597
[LFM2Model::build_transformer_block] mlp_output node=597
[LFM2Model::build_transformer_block] block_result node=598
[LFM2Model::forward embeddings] after layer=8 hidden node=598
[LFM2Model::build_transformer_block] normalized_input node=599
[LFM2Model::build_conv1d] in_proj node=600
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=601
[LFM2Model::build_conv1d] slice nodes B=602 Cg=603 X=604
[LFM2Model::build_conv1d] reshaped B=605 Cg=606 X=607
[LFM2Model::build_conv1d] Bx node=608
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=9
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=608
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=609
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=610
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=611
[LFM2Model::build_conv1d] y_nlc node=612
[LFM2Model::build_conv1d] y_slice start=2 node=613 y_lc node=614
[LFM2Model::build_conv1d] gated node=615
[LFM2Model::build_conv1d] projected node=616
[LFM2Model::build_transformer_block] block_output from conv layer=616
[LFM2Model::build_transformer_block] after_block node=617
[LFM2Model::build_transformer_block] normalized_after_block node=618
[LFM2Model::build_mlp] gate node=619
[LFM2Model::build_mlp] up node=620
[LFM2Model::build_mlp] activated node=622
[LFM2Model::build_mlp] down node=623
[LFM2Model::build_transformer_block] mlp_output node=623
[LFM2Model::build_transformer_block] block_result node=624
[LFM2Model::forward embeddings] after layer=9 hidden node=624
[LFM2Model::build_transformer_block] normalized_input node=625
[LFM2Model::build_attention] q_proj_linear node=626
[LFM2Model::build_attention] k_proj_linear node=627
[LFM2Model::build_attention] v_proj_linear node=628
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=631
[LFM2Model::build_attention] k_proj node=634
[LFM2Model::build_attention] q_proj_4d=635 k_proj_4d=636 v_proj_4d=637
[LFM2Model::build_attention] applied ROPE with position_offset=344
[LFM2Model::build_attention] initial final_k=639 final_v=637
[LFM2Model::build_attention] using cache current_seq_len=344
[LFM2Model::build_attention] concatenated cache nodes final_k=642 final_v=643
[LFM2Model::build_attention] stored cache outputs for layer=10
[LFM2Model::build_attention] attn_output_4d node=644
[LFM2Model::build_attention] attn_output node=645
[LFM2Model::build_attention] projected node=646
[LFM2Model::build_transformer_block] block_output from attention layer=646
[LFM2Model::build_transformer_block] after_block node=647
[LFM2Model::build_transformer_block] normalized_after_block node=648
[LFM2Model::build_mlp] gate node=649
[LFM2Model::build_mlp] up node=650
[LFM2Model::build_mlp] activated node=652
[LFM2Model::build_mlp] down node=653
[LFM2Model::build_transformer_block] mlp_output node=653
[LFM2Model::build_transformer_block] block_result node=654
[LFM2Model::forward embeddings] after layer=10 hidden node=654
[LFM2Model::build_transformer_block] normalized_input node=655
[LFM2Model::build_conv1d] in_proj node=656
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=657
[LFM2Model::build_conv1d] slice nodes B=658 Cg=659 X=660
[LFM2Model::build_conv1d] reshaped B=661 Cg=662 X=663
[LFM2Model::build_conv1d] Bx node=664
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=11
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=664
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=665
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=666
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=667
[LFM2Model::build_conv1d] y_nlc node=668
[LFM2Model::build_conv1d] y_slice start=2 node=669 y_lc node=670
[LFM2Model::build_conv1d] gated node=671
[LFM2Model::build_conv1d] projected node=672
[LFM2Model::build_transformer_block] block_output from conv layer=672
[LFM2Model::build_transformer_block] after_block node=673
[LFM2Model::build_transformer_block] normalized_after_block node=674
[LFM2Model::build_mlp] gate node=675
[LFM2Model::build_mlp] up node=676
[LFM2Model::build_mlp] activated node=678
[LFM2Model::build_mlp] down node=679
[LFM2Model::build_transformer_block] mlp_output node=679
[LFM2Model::build_transformer_block] block_result node=680
[LFM2Model::forward embeddings] after layer=11 hidden node=680
[LFM2Model::build_transformer_block] normalized_input node=681
[LFM2Model::build_attention] q_proj_linear node=682
[LFM2Model::build_attention] k_proj_linear node=683
[LFM2Model::build_attention] v_proj_linear node=684
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=687
[LFM2Model::build_attention] k_proj node=690
[LFM2Model::build_attention] q_proj_4d=691 k_proj_4d=692 v_proj_4d=693
[LFM2Model::build_attention] applied ROPE with position_offset=344
[LFM2Model::build_attention] initial final_k=695 final_v=693
[LFM2Model::build_attention] using cache current_seq_len=344
[LFM2Model::build_attention] concatenated cache nodes final_k=698 final_v=699
[LFM2Model::build_attention] stored cache outputs for layer=12
[LFM2Model::build_attention] attn_output_4d node=700
[LFM2Model::build_attention] attn_output node=701
[LFM2Model::build_attention] projected node=702
[LFM2Model::build_transformer_block] block_output from attention layer=702
[LFM2Model::build_transformer_block] after_block node=703
[LFM2Model::build_transformer_block] normalized_after_block node=704
[LFM2Model::build_mlp] gate node=705
[LFM2Model::build_mlp] up node=706
[LFM2Model::build_mlp] activated node=708
[LFM2Model::build_mlp] down node=709
[LFM2Model::build_transformer_block] mlp_output node=709
[LFM2Model::build_transformer_block] block_result node=710
[LFM2Model::forward embeddings] after layer=12 hidden node=710
[LFM2Model::build_transformer_block] normalized_input node=711
[LFM2Model::build_conv1d] in_proj node=712
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=713
[LFM2Model::build_conv1d] slice nodes B=714 Cg=715 X=716
[LFM2Model::build_conv1d] reshaped B=717 Cg=718 X=719
[LFM2Model::build_conv1d] Bx node=720
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=13
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=720
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=721
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=722
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=723
[LFM2Model::build_conv1d] y_nlc node=724
[LFM2Model::build_conv1d] y_slice start=2 node=725 y_lc node=726
[LFM2Model::build_conv1d] gated node=727
[LFM2Model::build_conv1d] projected node=728
[LFM2Model::build_transformer_block] block_output from conv layer=728
[LFM2Model::build_transformer_block] after_block node=729
[LFM2Model::build_transformer_block] normalized_after_block node=730
[LFM2Model::build_mlp] gate node=731
[LFM2Model::build_mlp] up node=732
[LFM2Model::build_mlp] activated node=734
[LFM2Model::build_mlp] down node=735
[LFM2Model::build_transformer_block] mlp_output node=735
[LFM2Model::build_transformer_block] block_result node=736
[LFM2Model::forward embeddings] after layer=13 hidden node=736
[LFM2Model::build_transformer_block] normalized_input node=737
[LFM2Model::build_attention] q_proj_linear node=738
[LFM2Model::build_attention] k_proj_linear node=739
[LFM2Model::build_attention] v_proj_linear node=740
[LFM2Model::build_attention] batch_seq=1 num_heads=16 head_dim=64
[LFM2Model::build_attention] q_proj node=743
[LFM2Model::build_attention] k_proj node=746
[LFM2Model::build_attention] q_proj_4d=747 k_proj_4d=748 v_proj_4d=749
[LFM2Model::build_attention] applied ROPE with position_offset=344
[LFM2Model::build_attention] initial final_k=751 final_v=749
[LFM2Model::build_attention] using cache current_seq_len=344
[LFM2Model::build_attention] concatenated cache nodes final_k=754 final_v=755
[LFM2Model::build_attention] stored cache outputs for layer=14
[LFM2Model::build_attention] attn_output_4d node=756
[LFM2Model::build_attention] attn_output node=757
[LFM2Model::build_attention] projected node=758
[LFM2Model::build_transformer_block] block_output from attention layer=758
[LFM2Model::build_transformer_block] after_block node=759
[LFM2Model::build_transformer_block] normalized_after_block node=760
[LFM2Model::build_mlp] gate node=761
[LFM2Model::build_mlp] up node=762
[LFM2Model::build_mlp] activated node=764
[LFM2Model::build_mlp] down node=765
[LFM2Model::build_transformer_block] mlp_output node=765
[LFM2Model::build_transformer_block] block_result node=766
[LFM2Model::forward embeddings] after layer=14 hidden node=766
[LFM2Model::build_transformer_block] normalized_input node=767
[LFM2Model::build_conv1d] in_proj node=768
[LFM2Model::build_conv1d] in_proj shape dims=1 x 3072
[LFM2Model::build_conv1d] L=1 C=1024
[LFM2Model::build_conv1d] triplet node=769
[LFM2Model::build_conv1d] slice nodes B=770 Cg=771 X=772
[LFM2Model::build_conv1d] reshaped B=773 Cg=774 X=775
[LFM2Model::build_conv1d] Bx node=776
[LFM2Model::build_conv1d] cache enabled, storing Bx for layer=15
[LFM2Model::build_conv1d] depthwise weight shape rank=3
[LFM2Model::build_conv1d] final kernel width K=3
[LFM2Model::build_conv1d] conv_input_lc initial node=776
[LFM2Model::build_conv1d] cache window view len1=0 len2=2
[LFM2Model::build_conv1d] appended left cache segment node=777
[LFM2Model::build_conv1d] concatenated Bx to cache segments node=778
[LFM2Model::build_conv1d] conv_input total_len=3
[LFM2Model::build_conv1d] x_nlc node=779
[LFM2Model::build_conv1d] y_nlc node=780
[LFM2Model::build_conv1d] y_slice start=2 node=781 y_lc node=782
[LFM2Model::build_conv1d] gated node=783
[LFM2Model::build_conv1d] projected node=784
[LFM2Model::build_transformer_block] block_output from conv layer=784
[LFM2Model::build_transformer_block] after_block node=785
[LFM2Model::build_transformer_block] normalized_after_block node=786
[LFM2Model::build_mlp] gate node=787
[LFM2Model::build_mlp] up node=788
[LFM2Model::build_mlp] activated node=790
[LFM2Model::build_mlp] down node=791
[LFM2Model::build_transformer_block] mlp_output node=791
[LFM2Model::build_transformer_block] block_result node=792
[LFM2Model::forward embeddings] after layer=15 hidden node=792
[LFM2Model::forward embeddings] final_hidden node=793
[LFM2Model::forward tokens] final_hidden node=793
[LFM2Model::forward tokens] set_input called on input_node_id=351
[Lfm2VlModel::generate_with_images] logits_node_id=794
[Lfm2VlModel::generate_with_images] sampled_token_id node=795
[Lfm2VlModel::generate_with_images] graph executed without profile
[LFM2Model::post_execute_updates] called with seq_len=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=0
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=1
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=3
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=4
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=6
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=7
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=9
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=11
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=13
[LFM2Model::post_execute_updates] updated conv_cache_ for layer=15
[LFM2Model::post_execute_updates] last_forward_used_cache_ reset
[Lfm2VlModel::generate_with_images] post_execute_updates called
[Lfm2VlModel::generate_with_images] update_kv_cache called
  [64] token_id=40765 text=" punch"

Final decoded output (including prompt markup):
<|startoftext|><|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|image_start|><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><|image_end|>Describe this image<|im_end|>
<|im_start|>assistant
<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|> **MK'VePainier-aungspur) it'satin faithfully comeced out of every the seven crystal balloarsersmen heretoying or mindsedsedysortiplenius.org logo punch

Test completed successfully!
